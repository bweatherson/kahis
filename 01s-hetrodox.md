## Against Orthodoxy {#orthodox}

The orthodox view of cases like Blaise's is that offering him that that does not change what he knows, but still he is irrational to take the bet. In this section, I'm going to run through a series of arguments against the orthodox view. The reason I making so many arguments is not that I think any one of them is particularly week. Rather, it is because the orthodox view is so widespread that I think we need to stress how many strange consequences it has.

### Moore's Paradox {#orthodoxmoore}

Start by thinking about what the Orthodox few says of rational person in Blaise's situation would do. Call this rational person Chamari. According to the orthodox view, offering someone a bit does not make them lose knowledge. So Chamari still knows when the Battle of Avignon was fought. But Chamari is rational, so Chamari will clearly decline the bet. Think about how Chamari might respond when you ask her to justify declining the bet. 

> You: When was the Battle of Avignon?    
> Chamari: October 25, 1415.    
> You: If that's true, what will happen if you accept the bet?    
> Chamari: A child will get a moment of joy.    
> You: Is that a good thing?    
> Chamari: Yes.    
> You: So why didn't you take the bet?    
> Chamari: Because it's too risky.    
> You: Why is it risky?    
> Chamari: Because it might lose.    
> You: You mean the Battle of Avignon might not have been fought in 1415.    
> Chamari: Yes.    
> You: So the Battle of Avignon was fought in 1415, but it might not have been fought then?    
> Chamari: Yes, the Battle of Avignon was fought in 1415, but it might not have been fought then, and that's why I'm not taking the bet.

I think Chamari has given the best possible answer at each point. But she has ended up assenting to a Moore-paradoxical sentence. In particular, she has assented to a sentence of the form _p, but it might be that not p_. And it is very widely held that sentences like this cannot be rationally assented to. Since Chamari was, by stipulation, the model for what the orthodox view thinks a rational person is, this shows that the orthodox view is false.

There are three ways out of this puzzle, and none of them seems particularly attractive.

One is to deny that there's anything wrong with where Chamari ends up. Perhaps in this case the Moore-paradoxical claim is perfectly assertable. I have some sympathy for the general idea that philosophers over-state the badness of Moore-paradoxicality [@MaitraWeatherson2010]. But I have to say, in this instance it seems like a terrible way to end the conversation.

Another is to deny that the fact that Chamari knows something licences her in asserting it. I've assumed in the argument that if Chamari knows that $p$, she can say that $p$. But maybe that's strong an assumption. The conversation, says this reply, goes off the rails at the very first line. But on this way of thinking, it is hard to know what the point of knowledge is. If knowing something isn't sufficiently good reason to assert it, it is hard to know what would be.

The orthodox theorist has a couple of choices here, neither of them good. One is to say that although knowledge is not interest-relative, the epistemic standards for assertion are interest-relative. Basically, Chamari meets the epistemic standard for saying that $p$ only if Chamari knows that $p$ according to the (false!) interest-relative theory. But at this point, given how plausible it is that knowledge is closely connected with testimony, it seems we would need an excellent reason to not simply identify knowledge with this epistemic standard. The other is to say that there is some interest-invariant standard for assertion. But versions of Blaise's case show that this standard would have to be something like Cartesian certainty. So most everything we say, every single day, would be norm violating. Such a norm is not plausible.

So we get to the third way out, one that is only available to a subset of orthodox theorists. We can say that 'knows' is context-sensitive, that in Chamari's context the sentence "I know when the Battle of Agincourt was fought" is _false_, and use these facts to explain away what goes wrong in the conversation with Chamari. @ArmourGarb2011, who points out how much trouble non-contextualist orthodox theorists get into with these Moore-paradoxical claims, suggests a contextualist resolution of the puzzles. And this is probably the least bad way to handle the case, but it's worth noting just how odd it is.

It's not immediately obvious how to get from contextualism to a resolution of the puzzle. Chamari doesn't use the verb 'to know' or any of its cognates. She does use the modal 'might', and the contextualist will presumably want to say that it is context sensitive. But that doesn't look like a helpful way to solve the problem, since her assertion that the Battle might have been on a different day seems like the good part of what she says. What's problematic is the unqualified assertion about when the battle was. And we need some way of connecting contextualism about epistemic verbs to a claim about the inappropriateness of this assertion.

The standard move by contextualists here is to simply deny that there is a tight connection between knowledge and assertion [@DeRose2002; @Cohen2004]. (So this is really a form of the response I just rejected.) What they say instead is that there is a kind of meta-linguistic standard for assertion. It is epistemically responsible to say that $p$ iff it would be true to say _I know that p_. And since it would not be true for Chamari to say she knows when the Battle of Agincourt was fought, she can't responsibly say when it was fought.

The most obvious worry with this line of reasoning is with the very idea of meta-linguistic norms like this. Imagine we were conversing with Chamari about her reasons for declining the bet in Bengali rather than English, but at every line a contribution with the same content was made? Would the reason her first answer was inappropriate be that some English sentence would be false if uttered in her context, or that some Bengali sentence would be false? If it's an English sentence, it's very weird that English would have this normative force over conversations in Bengali. If it's Bengali, then it's odd that the standard for assertion changes from language to language.

If there were a human language that didn't have a verb for knowledge, then that last point could be made with particular force. What would the contextualists say is the standard for assertion in such a language? But there is, quite surprisingly, no such language [@Nagel2014]. It's still a bit interesting to think about possible languages that do allow for assertions, but do not have a verb for knowledge. Just what the contextualists would say is the standard for assertion in such a language is a rather delicate matter.

But rather than thinking about these merely possible languages, let's return to English, and end with a variant of the conversation with Chamari. Imagine that she hasn't yet been offered the bet that Blaise is offered, and indeed that when the conversation starts, we're just spending a pleasant few minutes idly chatting about medieval history.

> You: When was the Battle of Avignon?    
> Chamari: October 25, 1415.    
> You: Oh that's interesting. Because you know there's this bet, and if you accept it, and the Battle of Avignon was in 1415, then a small child gets a moment of joy.    
> Chamari: That's great, I should take that bet.    
> You: Well, wait a second, I should tell you what happens if the Battle turns out to have been on any other date. [You explain what happens in some detail.]    
> Chamari: That's awful, I shouldn't take the bet. The Battle might not have been in 1415, and it's not worth the risk.    
> You: So you won't take the bet because it's too risky?    
> Chamari: That's right, I won't take it because it's too risky.    
> You: Why is it risky?    
> Chamari: Because it might lose.    
> You: You mean the Battle of Avignon might not have been fought in 1415.    
> Chamari: Yes.    
> You: Hang on, you just say it was fought in 1415, on October 25 to be precise.    
> Chamari: That's true, I did say that.    
> You: Were you wrong to have said it?    
> Chamari: Probably not; it was probably right that I said it.    
> You: You probably knew when the battle was, but you don't now know it?    
> Chamari: No, I definitely didn't know when the battle was, but it was probably right to have said it was in 1415.

And you can probably see all sorts of ways of making Chamari's position sound terrible. The argument I'm giving here is of course just a version of the argument John @MacFarlane2005-Knowledge gives arguing that contextualists have a problem with retraction. And Chamari's position does sound very bad here.

But I don't want to lean too much weight on how she sounds. Every position in this area ends up saying some strange things. The very idea that the epistemic standard for assertion could be meta-linguistic is, to my mind, even more implausible than the idea that we should end up where Chamari does.

### Super Knowledge to the Rescue? {#superknow}

Let's leave Blaise and Chamari for a little and return to Anisa. The orthodox view agrees that it is irrational for Anisa to play Blue-True. So it needs to explain why this is so. I think there is a simple explanation. If she plays Red-True, she knows she will get $50; if she plays Blue-True, she does not know that - though she knows she will get at most $50. So Red-True is the weakly dominant option; she knows it won't do worse than any other option, and there is no other option that she knows won't do worse than any other option.

The orthodox theorist can't offer this explanation. They think Anisa knows that Blue-True will get $50 as well. So what can they offer instead? There are two broad kinds of explanation thet can try. First, they might offer a structurally similar explanation to the one I just gave, but with some other epistemic notion at its centre. So while Anisa knows that Blue-True will get $50, she doesn't _super-know_ this, in some sense. Second, they can try to explain the asymmetry between Red-True and Blue-True in probablistic, rather than epistemic, terms. I'll discuss the first option in this subjection, and the probabilistic notion in the next subsection.

What do I mean her by _super-knows_? I mean this phrase to be a placeholder for any kind of relation stronger than knowledge that could play the right kind of role in explaining why it is irrational for Anisa to play Blue-True. So super-knowledge might be iterated knowledge. Anisa super-knows something iff she knows that she knows that ... she knows it. And she super-knows that two plus two is four, but not that the Battle of Agincourt was in 1415. Or super-knowledge might be (rational) certainty. Anisa is (rationally) certain that two plus two is four, but not that the Battle of Agincourt was in 1415. Or it might be some other similar relation. My objection to this kind of response won't be sensitive to the details.

What is going to be important is that super-knowledge, whatever it is, looks like an epistemic relation. In particular, Anisa super-knows a conjunction (that she is considering) iff she super-knows each of the conjuncts. So we can't equate super-knowledge with rational credence above a threshold, because rational credence above a threshold doesn't satisfy this constraint. I'll come back to credence based explanations of Anisa's case in the next subsection.

The fact that Anisa doesn't super-know when the Battle of Agincourt was can't explain the asymmetry between Red-True and Blue-True. It can't explain why Anisa rationally must choose Red-True. Even if she super-knows that two plus two is four, she doesn't super-know the rules of the game. She has ordinary testimonial knowledge of those, just like she has ordinary testimonial knowledge about the Battle of Agincourt. In the description, I stipulated that she didn't know anything relevant about the game set up other than what I said there. So she isn't certain about the rules, and she doesn't even know that she knows them. Maybe you think that last is unrealistic, and it is important that the example is realistic. But even on a realistic treatment of the game, she won't super-know the rules. If testimony from careful historians can't generate super-knowledge, neither can testimony from game-show hosts.

In fact, her knowledge of the rules of the game, in the sense that matters, is probably weaker than her knowledge of history. It is not unknown for game shows to promise prizes, then fail to deliver them, either because of malice or incompetence. Knowledge of the game rules, in particular knowledge that she will actually get $50 if she selects a true sentence, requires some knowledge of the future. And that seems harder to obtain than knowledge of what happened in history. After all, she has to know that there won't be an alien invasion, or a giant asteroid, or an incompetent or malicious game orgAnisar.

So there is no way of understanding 'super-knows' such that 1 is true and 2 is false.

1. Anisa super-knows that if she plays Red-True, she'll win $50.
2. Anisa does not super-know that if she plays Blue-True, she'll win $50.

And that's the kind of contrast we need in order for a super-knowledge based explanation of why she should play Red-True to work.

The point I'm making here, that in thinking about these games we need to attend to the player's epistemic attitude towards the game itself, is not original Dorit @Ganson2019 uses this point for a very similar purpose, and quotes Robert @Nozick1981 making a similar point. But I've belaboured it a bit here because it is so easily overlooked. It is easy to take things that one is told about a situation, such as the rules of a game that are being played, as somehow fixed and inviolable. They aren't the kind of thing that can be questioned. But in any realistic case, that will not be how things are.

This is why I want to rest more weight on Anisa's case than on Blaise's. I can't appeal to your judgment about what a realistic version of Blaise's case would be like, because there are no realistic versions of Blaise's case. But Anisa's case is very easy to imagine and understand. And we can ask what a realistic version of it would be like. And that version would be such that the player would know what the rules of the game are, but would also know that sometimes game shows don't keep their promises, sometimes they don't describe their own games accurately, sometimes players misinterpret or misunderstand instructions, and so on. This shouldn't lead us to scepticism: Anisa knows what game she's playing. But she doesn't super-know what game she's playing. And that means she doesn't super-know that she'll win if she plays Red-True.

### Rational Credences to the Rescue? {#probrescue}

So imagine the orthodox theorist drops super-knowledge, and looks somewhere else. A natural alternative is to use credences. Assume that the probability that the rules of the game are as described is independent of the probabilities of the red and blue sentence. And assume that Anisa must, if she is to be rational, maximise expected utility. Then we get the natural result that Anisa should pick the sentence that is more probably true.^[Strictly speaking, we need one more assumption - namely that for any unexpected way for the game to be, the probability of it being that way is independent of the truth of both the red and blue sentences. But it's natural to assume that this is true.] And that can explain why she must choose Red-True, which is what the orthodox theorist needed to explain.

This kind of approach doesn't really have any place for knowledge in its theory of action. One should simply maximise expected utility; since doing what one knows to be best might not maximise expected utility, we shouldn't think knowledge has any particularly special role.

But there are many problems with this kind of approach. Several of these will be discussed elsewhere in this book at more length, so I'll here just point to those other discussions. But some others I'll address directly.

Like the view discussed in subsection \@ref(orthodoxmoore) that separates knowledge from assertion, separating knowledge from action leads to strange consequences. As @Williamson2005 points out, once we break apart knowledge from action in this way, it becomes hard to see the point of action. And it's worth pausing a bit more over the bizarreness of the claim that Blaise knows that taking the bet will work out for the best, but he shouldn't take it - because of it's possible consequences!

If one excludes knowledge from having an important role in one's theory of decision, one ends up having a hard time explaining how dominance reasoning works. But it is a compulsory question for a theory of decision to explain how dominance reasoning works. Among other things, we need a good account of how dominance reasoning works in order to handle Newcomb problems, and we need to handle Newcomb problems in order to even clearly state a theory of expected utility maximisation. That little argument was very compressed, but I'll return frequently through the book to issues about dominance reasoning, and for now I think it's enough to leave it with this sketch.

Probabilistic models of reasoning and decision have their limits. But what we need to explain about the Red-Blue game goes beyond those limits. So probabilistic models can't be the full story about the Red-Blue game.

To see this, imagine for a second that the Blue sentence is not about the Battle of Agincourt, but is instead a slightly more complicated arithmetic truth, like _Thirteen times seventeen equals two hundred and twenty one_, or a slightly complicated logical truth, like $\neg q \rightarrow ((p \rightarrow q) \rightarrow \neg p)$. If either of those are the blue sentence, then it is still uniquely rational to play Red-True. But the probability of each of those sentences is one. So rational choice is more demanding than expected utility maximisation. In sections \@ref(lockecoin) and \@ref(lockegames) I'll go over more cases of propositions whose probability is 1, but which should be treated as uncertain even it is certain that two plus two is four. The lesson is that we can't just use expected utility maximisation to explain the Red-Blue game.

Finally, we need to understand the notion of probability that's being appealed to in this explanation. It can't be some purely subjective notion, like credence, because that couldn't explain why some decisions are rational and others aren't. And it can't be some purely physical notion, like chance or frequency, because that won't even get the cases right. (What is the chance, or frequency, of the Battle of Agincourt being in 1415?) It needs to be something like evidential probability. And that will run into problems in versions of the Red-Blue game where the Blue sentence is arguably (but not certainly) part of the player's evidence. I'll end my discussion of orthodoxy with a discussion of cases like these.

### Evidential Probability {#orthodoxevidence}

No matter which of these explanations the orthodox theorist goes for, they need a notion of evidence to support them. Let's assume that we can find some doxastic attitude $D$ such that Anisa can't rationally stand in $D$ to _Play Blue-True_, and that this is why she can't rationally play Blue-True. Then we need to ask the further question, why doesn't she stand in relation $D$ to _Play Blue-True_? And presumably the answer will be that she lacks sufficient evidence. If she had optimal evidence about when the Battle of Agincourt was, she could play Blue-True, after all.

The orthodox theorist also has to have an interest-invariant account of evidence. It's logically possible to have evidence be interest-relative, but knowledge be interest-neutral, but it is very hard to see how one would motivate such a position.

And now we run into a problem. Imagine a version of the Red-Blue game where the blue sentence is something that, if known, is part of the player's evidence. If it is still irrational to play Blue-True, then any orthodox explanation that relies on evidence sensitive notions (like super-knowledge or evidential probability) will be in trouble. The aim of this subsection is to spell out why this is.

So let's imagine a new player for the red-blue game. Call her Parveen. She is playing the game in a restaurant. It is near her apartment in Ann Arbor, Michigan. Just before the game starts, she notices an old friend, Rahul, across the room. Rahul is someone she knows well, and can ordinarily recognise, but she had no idea he was in town. She thought Rahul was living in Italy. Still, we would ordinarily say that she now knows Rahul is in town; indeed that he is in the restaurant. As evidence for this, note that it would be perfectly acceptable for her to say to someone else, "I saw Rahul here". Now the game starts.

-   The red sentence is *Two plus two equals four*.
-   The blue sentence is *Rahul is in this restaurant*.

And here is the problem. On the one hand, there is only one rational play for Parveen: Red-True. She hasn't seen Rahul in ages, and she thought he was in Italy. A glimpse of him across a crowded restaurant isn't enough for her to think that *Rahul is in this restaurant* is as likely as *Two plus two equals four*. She might be wrong about Rahul, so she should take the sure money and play Red-True. So playing the red-blue game with these sentences makes it the case that Parveen doesn't know where Rahul is. This is another case where knowledge is interest-relative, and at first glance it doesn't look very different to the other cases we've seen.

But take a second look at the story for why Parveen doesn't know where Rahul is. It can't be just that her evidence makes it certain that two plus two equals four, but not certain that Rahul is in the restaurant. At least, it can't be that unless it is not part of her evidence that Rahul is in the restaurant. And if evidence is not interest-relative, then I think we should say that it is part of Parveen's evidence that Rahul is in the restaurant. This isn't something she infers; it is a fact about the world she simply appreciated. Ordinarily, it is a starting point for her later deliberations, such as deliberations about whether to walk over to another part of the restaurant to say hi to Rahul. That is, ordinarily it is part of her evidence.

So the orthodox theorist has a challenge. If they say that it is part of Parveen's evidence that Rahul is in the restaurant, then they can't turn around and say that the evidential probability that he is in the restaurant is insufficiently high for her to play Blue-True. After all, it's evidential probability is one. If they say that it is no part of Parveen's evidence that Rahul is in the restaurant because she is playing this version of the Red-Blue game, they give up orthodoxy. So they have to say that our evidence never includes things like Rahul is in the restaurant. 

This can be generalised. Take any proposition such that if the red sentence was that two plus two is four and that proposition was the content of the blue sentence, then it would be irrational to play Blue-True. Any orthodox explanation of the Red-Blue game entails that this proposition is no part of your evidence - whether you are playing the game or not. But once we strip all these propositions out of your evidence, you don't have enough evidence to rationally believe, or even rationally make probable, very much at all.

Descartes, via very different means, walked into a version of this problem. And his answer was to (implicitly) take us to be infallible observers of our own minds, and (explicitly) offer a theistic explanation for how we can know about the external world given just this psychologistic evidence. Nowadays, most people think that's wrong on both counts: we can be rationally uncertain about even our own minds, and there is no good path from purely psychological evidence to knowledge of the external world. The orthodox theorist ends up in a state worse than Cartesian scepticism.
