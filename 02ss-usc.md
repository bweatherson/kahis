### Ross and Schroeder's Theory {#usc}

Jacob Ross and Mark Schroeder [-@RossSchroeder2014] have what looks like, on the surface, a rather different view to mine.^[This is based on material in my -@Weatherson2016[sect. 3].] They say that to believe *p* is to have a **default reasoning disposition** to use *p* in reasoning. Here's how they describe their view.

> What we should expect, therefore, is that for some propositions we  would have a *defeasible* or *default* disposition to treat them as true in our reasoning--a disposition that can be overridden under circumstances where the cost of mistakenly acting as if these propositions are true is particularly salient. And this expectation is confirmed by our experience. We do indeed seem to treat some uncertain propositions as true in our reasoning; we do indeed seem to treat them as true automatically, without first weighing the costs and benefits of so treating them; and yet in contexts such as High where the costs of mistakenly treating them as true is salient, our natural tendency to treat these propositions as true often seems to be overridden, and instead we treat them as merely probable.
>
> But if we concede that we have such defeasible dispositions to treat particular propositions as true in our reasoning, then a hypothesis naturally arises, namely, that beliefs consist in or involve such dispositions. More precisely, at least part of the functional role of belief is that believing that *p* defeasibly disposes the believer to treat *p* as true in her reasoning. Let us call this hypothesis the *reasoning disposition account* of belief. [@RossSchroeder2014 9-10]

There are, relative to what I'm interested in, three striking characteristics of Ross and Schroeder's view.

1. Whether you believe *p* is sensitive to how you reason; that is, your theoretical interests matter.
2. How you would reason about some questions that are not live is relevant to whether you believe *p*.
3. Dispositions can be masked, so you can believe *p* even though you don't actually use *p* in reasoning now.

The view I'm defending here agrees with them about 1 and 2, though my theory manifests those characteristics in a quite different way. But point 3 is a cost of their theory, not a benefit, so it's good that my theory doesn't accommodate it. (For the record, the theory I put forward in my [-@Weatherson2005-WEACWD] did not agree with them on point 2, and I changed my view because of their arguments.)

I agree with 1 because, as I've noted a few times above, I think theoretical interests as well as pragmatic interests matter for the relationship between credence and belief. And I agree with 2 because I think that whether someone is disposed to use *p* as a premise matters to whether they believe *p*. Let *p* be some ordinary proposition about the world that a person believes, such as that the Florida Marlins won the 2003 World Series. And let *q* be a lottery proposition that is just as probable as *p*. (That is, let *q* be a lottery proposition such that if the person were to play the Red-Blue game with *p* as red and *q* as blue, they would be rationally indifferent between the choices.) Then on my theory the person believes *p* but not *q*, and this isn't due to any features of their credal states. Rather, it is due to their dispositions to use *p* as a premise in reasoning. (For example, they might use it in figuring out how many World Series were won by National League teams in the 2000s.) 

Ross and Schroeder argue, and I basically agree, that interest-relative theories of belief that only focus on practical interests have trouble with folks who use odd techniques in reasoning. This is the lesson of their example of _Renzi_. I'll run through a somewhat more abstract version of that case, because the details are not particularly important. Start with a standard decision problem. The agent knows that X is better to do if *p*, and Y is better to do if $\neg p$. The agent should then go through calculating the relative gains to doing X or Y in the situations they are better, and the probability of *p*. But the agent imagined doesn't do that. Rather, the agent divides the possibility space in four, taking the salient possibilities to be
$p \wedge q, p \wedge \neg q, \neg p \wedge q$ and $\neg p \wedge \neg q$, and then calculates the expected utility of X and Y accordingly. This is a bad bit of reasoning on the agent's part. In the cases we are interested in, *q* is exceedingly likely. Moreover, the expected utility of each act doesn't change a lot depending on *q*'s truth value. So it is fairly obvious that we'll end up making the same decision whether we take the 'small worlds' in our decision model to be just the world where *p*, and the world where $\neg p$, or the four worlds this agent uses. But the agent does use these four, and the question is what to say about them.

Ross and Schroeder say that such an agent should not be counted as believing that *q*. If they are consciously calculating the probability that *q*, and taking $\neg q$ possibilities into account when calculating expected utilities, they regard *q* as an open question. And regarding *q* as open in this way is incompatible with believing it. 

I agree. The agent was trying to work out the expected utility of X and Y by working out the utility of each action in each of four 'small worlds', then working out the probability of each of these. Conditional on *q*, the probability of two of them ($p \wedge \neg q, \neg p \wedge \neg q$), will be 0. Unconditionally, this probability won't be 0. So the agent has a different view on some question they have taken an interest in unconditionally to their view conditional on *q*. So they don't believe *q*. The agent shouldn't care about that question, and conditional on each question they should care about, they have the same attitude unconditionally and conditional on *q*. But they do care about these probabilistic questions, so they don't believe *q*. (And again for the record, the theory I defended at the time Ross and Schroeder wrote their paper did not have the resources to make this reply; I've changed my views in light of their arguments.)

So far I've been agreeing with Ross and Schroeder. But there is one big point of disagreement. They think it is very important that a theory of belief vindicate a principle they call **Stability**.

> **Stability**: A fully rational agent does not change her beliefs purely in virtue of an evidentially irrelevant change in her credences or preferences. [-@RossSchroeder2014, 20]

Here's the kind of case that is meant to motivate Stability, and show that views like mine are in tension with it.

> Suppose Stella is extremely confident that steel is stronger than Styrofoam, but she's not so confident that she'd bet her life on this proposition for the prospect of winning a penny. PCR [their name for my old view] implies, implausibly, that if Stella were offered such a bet, she'd cease to believe that steel is stronger than Styrofoam, since her credence would cease to rationalize acting as if this proposition is true. [-@RossSchroeder2014, 20]

Ross and Schroeder's own view is that if Stella has a defeasible disposition to treat as true the proposition that steel is stronger than Styrofoam, that's enough for her to believe it. And that can be true if the disposition is not only defeasible, but actually defeated in the circumstances Stella is in. This all strikes me as just as implausible as the failure of Stability. Let's go over its costs.

The following propositions are clearly not mutually consistent, so one of them must be given up. We're assuming that Stella is facing, and knows she is facing, a bet that pays a penny if steel is stronger than Styrofoam, and costs her life if steel is not stronger than Styrofoam.

1. Stella believes that steel is stronger than Styrofoam.
2. Stella believes that if steel is stronger than Styrofoam, she'll win a penny and lose nothing by taking the bet.
3. If 1 and 2 are true, and Stella considers the question of whether she'll win a penny and lose nothing by taking the bet, she'll believe that she'll win a penny and lose nothing by taking the bet.
4. Stella prefers winning a penny and losing nothing to getting nothing.
5. If Stella believes that she'll win a penny and lose nothing by taking the bet, and prefers winning a penny and losing nothing to getting nothing, she'll take the bet.
6. Stella won't take the bet.

It's part of the setup of the problem that 2 and 4 are true. And it's common ground that 6 is true, at least assuming that Stella is rational. So we're left with 1, 3 and 5 as the possible candidates for falsehood.

Ross and Schroeder say that it's implausible to reject 1. After all, Stella believed it a few minutes ago, and hasn't received any evidence to the contrary. And I guess rejecting 1 isn't the most intuitive philosophical conclusion I've ever drawn. But compare the alternatives!

If we reject 3, we must say that Stella will simply refuse to infer *r* from *p*, *q* and $(p \wedge q) \rightarrow r$. Now it is notoriously hard to come up with a general principle for closure of beliefs. But it is hard to see why this particular instance would fail. And in any case, it's hard to see why Stella wouldn't have a general, defeasible, disposition to conclude *r* in this case, so by Ross and Schroeder's own lights, it seems 3 should be acceptable.

That leaves 5. It seems on Ross and Schroeder's view, Stella simply must violate a very basic principle of means-end reasoning. She desires something, she believes that taking the bet will get that thing, and come with no added costs. Yet, she refuses to take the bet. And she's rational to do so! At this stage, I think I've lost what's meant to be belief-like about their notion of belief. I certainly think attributing this kind of practical incoherence to Stella is much less plausible than attributing a failure of Stability to her.

Put another way, I don't think presenting Stability on its own as a desideratum of a theory is exactly playing fair. The salient question isn't whether we should accept or reject Stability. The salient question is whether giving up Stability is a fair price to pay for saving basic tenets of means-end rationality. And I think that it is. Perhaps there will be some way of understanding cases like Stella's so that we don't have to choose between theories of belief that violate Stability constraints, and theories of belief that violate coherence constraints. But I don't see one on offer, and I'm not sure what such a theory could look like.

I have one more argument against Stability, but it does rest on somewhat contentious premises. There's often a difference between the best methodology in an area, and the correct epistemology of that area. When that happens, it's possible that there is a good methodological rule saying that if such-and-such happens, re-open a certain inquiry. But that rule need not be epistemologically significant. That is, it need not be the case that the happening of such-and-such provides evidence against the conclusion of the inquiry. It just provides a reason that a good researcher will re-open the inquiry. And, as I've argued above, an open inquiry is incompatible with belief.

Here's one way that might happen. Like other non-conciliationists about disagreement, e.g., @Kelly2010-KELPDA, I hold that disagreement by peers with the same evidence as you doesn't provide *evidence* that you are wrong. But it might provide an excellent reason to re-open an inquiry. We shouldn't draw conclusions about the methodological significance of disagreement from the epistemology of disagreement. So learning that your peers all disagree with a conclusion might be a reason to re-open inquiry into that conclusion, and hence lose belief in the conclusion, without providing evidence that the conclusion is false. This example rests on a very contentious claim about the epistemology of disagreement. But any gap that opens up between methodology and epistemology will allow such an example to be constructed, and hence provide an independent reason to reject Stability.
