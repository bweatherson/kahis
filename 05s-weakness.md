## Epistemic Weakness {#weakness}

The cases where cut-elimination fails raise a problem for the way that Jeremy Fantl and Matthew McGrath spell out their version of IRT. Here is a principle they rely on in motivating IRT.

> When you know a proposition $p$, no weaknesses in your epistemic position with respect to $p$—no weaknesses, that is, in your standing on any truth-relevant dimension with respect to $p$—stand in the way of $p$ justifying you in having further beliefs. [@FantlMcGrath2009, 64]

And a few pages later they offer the following gloss on this principle.

> We offer no analysis of the intuitive notion of 'standing in the way'. But we do think that, when Y does not obtain, the following counterfactual condition is sufficient for a subject's position on some dimension d to be something that stands in the way of Y obtaining: whether Y obtains can vary with variations in the subject's position on d, holding fixed all other factors relevant to whether Y obtains. [@FantlMcGrath2009, 67]

This gloss suggests that the difference between knowledge and evidence is something that stands in the way of an inference. The inquirer who knows that nearby $F$s are $G$s, but does not know that somewhat distant $F$s are $G$s, has many things standing in the way of this knowledge. One of them is, according to this test, that her evidence does not include that all nearby $F$s are $G$s. Yet this is something she knows. So a weakness in her epistemic position with respect to the nature of nearby $F$s, that it is merely evidence and not knowledge, stands in the way of it justifying further beliefs.

The same thing will be true in the monontonic cases of cut-elimination failure. The thinker whose evidence includes $\Gamma \cup \Delta$, and whose inferential knowledge includes $A$, cannot infer $B$. But if they had $A$ as evidence, and not merely as knowledge, then they could infer $B$. So the weakness in their epistemic position, the gap between evidence and knowledge, stands in the way of something.

I didn't endorse this principle, but I did endorse very similar principles, and one might wonder whether they are subject to the same criticism. The main principle I endorsed was that if one knows that $p$, one is immune from criticism for using $p$ on the grounds that $p$ might be false, or is too risky to use. Equivalently, if the use of $p$ in an inference is defective, but $p$ is known, the explanation of why it is defective cannot be that $p$ is too risky. But now won't the same problem arise? Our inquirer in the monotonic cut-elimination example can't use $A$ in reasoning to $B$. If $A$ was part of their evidence, then it wouldn't be risky, and they would be able to use it. So the risk is part of what makes the use of it mistaken.

I reject the very last step in that criticism. The fact that something is wrong, and that it wouldn't have been wrong if X, does not mean the non-obtaining of X is part of the ground, or explanation, for why it is wrong. If I break a law, then what I do is illegal. Had the law in question been struck down by a constitutional court, then my action wouldn't have been illegal. Similarly, if the law had been repealed, my action would not have been illegal. But that doesn't imply that the ground or explanation of the illegality of my action is the court's not striking the law down, or the later legislature not repealing the law. That is to put too much into the notion of ground or explanation. No, what makes the act illegal is that a particular piece of legislation was passed, and this act violates it. This explanation is defeasible - it would be defeated if a court or later legislature had stepped in - but it is nonetheless complete.

The same thing is true in the case of knowledge and evidence. Imagine an inquirer who observes all the $F$s within 3 miles being $G$, and infers both that all the $F$s within 4 miles are $G$, and, therefore, that all the $F$s within 5 miles are $G$. The intermediate step is, in a sense, risky. And the later step is bad. And the later step wouldn't have been bad if the intermediate step hadn't been risky. But it's not the riskiness that makes the second inference bad. No, what makes the second inference bad is that it violates Weisberg's No Feedback principle. That's what the reasoner can be criticised, not for taking an epistemic risk.

There are two differences then between the core principle I rely on - using reasons that are known provides immunity to criticism for taking epistemic risks - and the principle Fantl and McGrath rely on. I use a concept of epistemic risk where they use a concept of strength of epistemic position. I don't think these are quite the same thing, but they are clearly similar. But the bigger difference is that they endorse a counterfactual gloss of their principle, and I reject any such counterfactual gloss. I don't say that the person who uses known $p$ is immune to all criticisms that would have been vitiated had $p$ been less risky. I just say that the risk can't be the ground of the criticism; something else must be. In some cases, including this one, that 'something else' might be correlated with risk. But it must be the explanation.

Of course, this difference between my version of IRT and Fantl and McGrath's is tiny compared to how much our theories have in common. And indeed, it's tiny compared to how much my theory simply borrows from theirs. But it's helpful I think to highlight the differences to understand the choice points within versions of IRT.
