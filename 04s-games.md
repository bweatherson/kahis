## Playing Games {#lockegames}

Some people might be nervous about resting too much weight on infinitary exaples like the coin sequence. So I'll show how the same puzzle arises in a simple, and finite, game.^[This is based on material in my -@Weatherson2014[sect. 1].] The game itself is a nice illustration of how a number of distinct solution concepts in game theory come apart. (Indeed, the use I'll make of it isn't a million miles from the use that @KohlbergMertens1986 make of it.) To set the problem up, I need to say a few words about how I think of game theory. This won't be at all original - most of what I say is taken from important works by Robert Stalnaker [-@Stalnaker1994; -@Stalnaker1996; -@Stalnaker1998; -@Stalnaker1999]. But the underlying philosophical points are important, and it is easy to get confused about them. (At least, I used to get these points all wrong, and that's got to be evidence they are easy to get confused about, right?) So I'll set the basic points slowly, and then circle back to the puzzle for the Lockeans.^[I'm grateful to the participants in a game theory seminar at ArchÃ© in 2011, especially Josh Dever and Levi Spectre, for very helpful discussions that helped me see through my previous confusions.]

Start with a simple decision problem, where the agent has a choice between two acts $A_1$ and $A_2$, and there are two possible states of the world, $S_1$ and $S_2$, and the agent knows the payouts for each act-state pair are given by the following table.

  ------- ------- -------
           $S_1$   $S_2$
    $A_1$    4       0
    $A_2$    1       1
  ------- ------- -------

What to do? I hope you share the intuition that it is radically underdetermined by the information I've given you so far. If $S_2$ is much more probable than $S_1$, then $A_2$ should be chosen; otherwise $A_1$ should be chosen. But I haven't said anything about the relative probability of those two states. Now compare that to a simple game. Row has two choices, which I'll call $A_1$ and $A_2$. Column also has two choices, which I'll call $S_1$ and $S_2$. It is common knowledge that each player is rational, and that the payouts for the pairs of choices are given in the following table. (As always, Row's payouts are given first.)

  ------- ------- -------
           $S_1$   $S_2$
    $A_1$  4, 0    0, 1
    $A_2$  1, 0    1, 1
  ------- ------- -------

What should Row do? This one is easy. Column gets 1 for sure if she plays $S_2$, and 0 for sure if she plays $S_1$. So she'll play $S_2$. And given that she's playing $S_2$, it is best for Row to play $A_2$.

You probably noticed that the game is just a version of the decision problem from a couple of paragraphs ago. The relevant states of the world are choices of Column. But that's fine; the layout of that decision problem was neutral on what constituted the states $S_1$ and $S_2$. Note that the game can be solved without explicitly saying anything about probabilities. What is added to the (unsolvable) decision-theoretic problem is not information about probabilities, but information about Column's payouts, and the fact that Column is rational. Those facts imply something about Column's play, namely that she would play  $S_2$. And that settles what Row should do.

There's something quite general about this example. What's distinctive about game theory isn't that it involves any special kinds of decision making. Once we get the probabilities of each move by the other player, what's left is (mostly) expected utility maximisation.^[The qualification is because weak dominance reasoning cannot be construed as orthodox expected utility maximisation. We saw that in the coins case, and it will become important again here. It is possible to model weak dominance reasoning using non-standard probabilities, as in @Brandenburger2008, but that introduces new complications.] The distinctive thing about game theory is that the probabilities aren't specified in the setup of the game; rather, they are solved for. Apart from special cases, such as where one option strictly dominates another, not much can be said about a decision problem with unspecified probabilities. But a lot can be said about games where the setup of the game doesn't specify the probabilities, because it is possible to solve for the probabilities given the information that is provided.

This way of thinking about games makes the description of game theory as 'interactive epistemology' [@Aumann1999] rather apt. The theorist's work is to solve for what a rational agent should think other rational agents in the game should do. From this perspective, it isn't surprising that game theory will make heavy use of equilibrium concepts. In solving a game, we must deploy a theory of rationality, and attribute that theory to rational actors in the game itself. In effect, we are treating rationality as something of an unknown, but one that occurs in every equation we have to work with. Not surprisingly, there are going to be multiple solutions to the puzzles we face.

This way of thinking lends itself to an epistemological interpretation of one of the most puzzling concepts in game theory, the mixed strategy. The most important solution concept in modern game theory is the Nash equilibrium. A set of moves is a Nash equilibrium if no player can improve their outcome by deviating from the equilibrium, conditional on no other player deviating. In many simple games, the only Nash equilibria involve mixed strategies. Here's one simple example.

  ------- ------- -------
           $S_1$   $S_2$
    $A_1$  0, 1    10, 0
    $A_2$  9, 0    -1, 1
  ------- ------- -------

This game is reminiscent of some puzzles that have been much discussed in the decision theory literature, namely asymmetric Death in Damascus puzzles [@Richter1984;] . Here Column wants herself and Row to make the 'same' choice, i.e., $A_1$ and $S_1$ or $A_2$ and $S_2$. She gets 1 if they do, 0 otherwise. And Row wants them to make different choices, and gets 10 if they do. Row also dislikes playing $A_2$, and this costs her 1 whatever else happens. It isn't too hard to prove that the only Nash equilibrium for this game is that Row plays a mixed strategy playing both $A_1$ and $A_2$ with probability $\frac{1}{2}$, while Column plays the mixed strategy that gives $S_1$ probability $\frac{11}{20}$, and $S_2$ with probability $\frac{9}{20}$.

Now what is a mixed strategy? It is easy enough to take away form the standard game theory textbooks a **metaphysical** interpretation of what a mixed strategy is. Here, for instance, is the paragraph introducing mixed strategies in Dixit and Skeath's *Games of Strategy*.

> When players choose to act unsystematically, they pick from among their pure strategies in some random way ...We call a random mixture between these two pure strategies a mixed strategy. [@DixitSkeath2004 186]

Dixit and Skeath are saying that it is definitive of a mixed strategy that players use some kind of randomisation device to pick their plays on any particular run of a game. That is, the probabilities in a mixed strategy must be in the world; they must go into the players' choice of play. That's one way, the paradigm way really, that we can think of mixed strategies metaphysically.

But the understanding of game theory as interactive epistemology naturally suggests an **epistemological** interpretation of mixed strategies.

> One could easily ... [model players] ... turning the choice over to a randomizing device, but while it might be harmless to permit this, players satisfying the cognitive idealizations that game theory and decision theory make could have no motive for playing a mixed strategy. So how are we to understand Nash equilibrium in model theoretic terms as a solution concept? We should follow the suggestion of Bayesian game theorists, interpreting mixed strategy profiles as representations, not of players' choices, but of their beliefs. [@Stalnaker1994 57-8]

One nice advantage of the epistemological interpretation, as noted by Binmore [-@Binmore2007 185] is that we don't require players to have $n$-sided dice in their satchels, for every $n$, every time they play a game.^[It is worse than if some games have the only equilibria involving mixed strategies with irrational probabilities. And it might be noted that Binmore's introduction of mixed strategies, on page 44 of his [-@Binmore2007], sounds much more like the metaphysical interpretation. But I think the later discussion is meant to indicate that this is just a heuristic introduction; the epistemological interpretation is the correct one.] But another advantage is that it lets us make sense of the difference between playing a pure strategy and playing a mixed strategy where one of the 'parts' of the mixture is played with probability one.

With that in mind, consider the below game, which I'll call Up-Down.^[In earlier work I'd called it Red-Green, but this is too easily confused with the Red-Blue game that plays such an important role in chapter \@ref(interests).] Informally, in this game $A$ and $B$ must each play a card with an arrow pointing up, or a card with an arrow pointing down. I will capitalise $A$'s moves, i.e., $A$ can play UP or DOWN, and italicise $B$'s moves, i.e., $B$ can play *up* or *down*. If at least one player plays a card with an arrow facing up, each player gets \$1. If two cards with arrows facing down are played, each gets nothing. Each cares just about their own wealth, so getting \$1 is worth 1 util. All of this is common knowledge. More formally, here is the game table, with $A$ on the row and $B$ on the column.

  ------- -------- --------
            *up*    *down*
       UP   1, 1     1, 1
     DOWN   1, 1     0, 0
  ------- -------- --------

When I write game tables like this, I mean that the players know that these are the payouts, that the players know the other players to be rational, and these pieces of knowledge are common knowledge to at least as many iterations as needed to solve the game. (I assume here that in solving the game, it is legitimate to assume that if a player knows that one option will do better than another, they have conclusive reason to reject the latter option. This is completely standard in game theory, though somewhat controversial in philosophy.) With that in mind, let's think about how the agents should approach this game.

I'm going to make one big simplifying assumption at first. I'll relax this later, but it will help the discussion to start with this assumption. This assumption is that the doctrine of Uniqueness applies here; there is precisely one rational credence to have in any salient proposition about how the game will play. Some philosophers think that Uniqueness always holds [@White2005-WHIEP]. I join with those such as @North2010 and @Schoenfield2013 who don't. But it does seem like Uniqueness might often hold; there might often be a right answer to a particular problem. Anyway, I'm going to start by assuming that it does hold here.

The first thing to note about the game is that it is symmetric. So the probability of $A$ playing UP should be the same as the probability of $B$ playing *up*, since $A$ and $B$ face exactly the same problem. Call this common probability $x$. If $x < 1$, we get a quick contradiction. The expected value, to Row, of UP, is 1. Indeed, the known value of UP is 1. If the probability of *up* is $x$, then the expected value of UP is $x$. So if $x < 1$, and Row is rational, she'll definitely play UP. But that's inconsistent with the claim that $x < 1$, since that means that it isn't definite that Row will play UP.

So we can conclude that $x = 1$. Does that mean we can know that Row will play UP? No. Assume we could conclude that. Whatever reason we would have for concluding that would be a reason for any rational person to conclude that Column will play *up*. Since any rational person can conclude this, Row can conclude it. So Row knows that she'll get 1 whether she plays UP or DOWN. But then she should be indifferent between playing UP and DOWN. And if we know she's indifferent between playing UP and DOWN, and our only evidence for what she'll play is that she's a rational player who'll maximise her returns, then we can't be in a position to know she'll play UP.

For the rest of this ssection I want to reply to one objection, and weaken an assumption I made earlier. The objection is that I'm wrong to assume that agents will only maximise expected utility. They may have tie-breaker rules, and those rules might undermine the arguments I gave above. The assumption is that there's a uniquely rational credence to have in any given situation.

I argued that if we knew that $A$ would play UP, we could show that $A$ had no reason to play UP. But actually what we showed was that the expected utility of playing UP would be the same as playing DOWN. Perhaps $A$ has a reason to play UP, namely that UP weakly dominates DOWN. After all, there's one possibility on the table where UP does better than DOWN, and none where RED does better. And perhaps that's a reason, even if it isn't a reason that expected utility considerations are sensitive to.

Now I don't want to insist on expected utility maximisation as the only rule for rational decision making. Sometimes, I think some kind of tie-breaker procedure is part of rationality. In the papers by Stalnaker I mentioned above, he often appeals to this kind of weak dominance reasoning to resolve various hard cases. But I don't think weak dominance provides a reason to play UP in this particular case. When Stalnaker says that agents should use weak dominance reasoning, it is always in the context of games where the agents' attitude towards the game matrix is different to their attitude towards each other. One case that Stalnaker discusses in detail is where the game table is common knowledge, but there is merely common (justified, true) belief in common rationality. Given such a difference in attitudes, it does seem there's a good sense in which the most salient departure from equilibrium will be one in which the players end up somewhere else on the table. And given that, weak dominance reasoning seems appropriate.

But that's not what we've got here. Assuming that rationality requires playing UP/*up*, the players know we'll end up in the top left corner of the table. There's no chance that we'll end up elsewhere. Or, perhaps better, there is just as much chance we'll end up 'off the table', as that we'll end up in a non-equilibrium point on the table. To make this more vivid, consider the 'possibility' that $B$ will play *across*, and if $B$ plays *across*, $A$ will receive 2 if she plays DOWN, and -1 if she plays UP. Well hold on, you might think, didn't I say that *up* and *down* were the only options, and this was common knowledge? Well, yes, I did, but if the exercise is to consider what would happen if something the agent knows to be true doesn't obtain, then the possibility that one agent will play blue certainly seems like one worth considering. It is, after all, a metaphysical possibility. And if we take it seriously, then it isn't true that under any possible play of the game, UP does better than DOWN.

We can put this as a dilemma. Assume, for reductio, that UP/*up* is the only rational play. Then if we restrict our attention to possibilities that are epistemically open to $A$, then UP does just as well as DOWN; they both get 1 in every possibility. If we allow possibilities that are epistemically closed to $A$, then the possibility where $B$ plays *blue* is just as relevant as the possibility that $B$ is irrational. After all, we stipulated that this is a case where rationality is common knowledge. In neither case does the weak dominance reasoning get any purchase.

With that in mind, we can see why we don't need the assumption of Uniqueness. Let's play through how a failure of Uniqueness could undermine the argument. Assume, again for reductio, that we have credence $\varepsilon > 0$ that $A$ will play DOWN. Since $A$ maximises expected utility, that means $A$ must have credence 1 that $B$ will play *up*. But this is already odd. Even if you think people can have different reactions to the same evidence, it is odd to think that one rational agent could regard a possibility as infinitely less likely than another, given isomorphic evidence. And that's not all of the problems. Even if $A$ has credence 1 that $B$ will play *up*, it isn't obvious that playing UP is rational. After all, relative to the space of epistemic possibilities, UP weakly dominates DOWN. Remember that we're no longer assuming that it can be known what $A$ or $B$ will play. So even without Uniqueness, there are two reasons to think that it is wrong to have credence $\varepsilon > 0$ that $A$ will play DOWN. So we've still shown that credence 1 doesn't imply knowledge, and since the proof is known to us, and full belief is incompatible with knowing that you can't know, this is a case where credence 1 doesn't imply full belief. So whether $A$ plays UP, like whether the coin will ever land tails, is a case where belief comes apart from high credence, even if by high credence we literally mean credence one. This is a problem for the Lockean, and, like Williamson's coin, it is also a problem for the view that belief is credence one.
