<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 2 Interests in Epistemology | Knowledge</title>
  <meta name="description" content="A defence of an interest-relative theory of knowledge">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 2 Interests in Epistemology | Knowledge" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://brian.weatherson.org/kahis/" />
  <meta property="og:image" content="https://brian.weatherson.org/kahis/tree.jpg" />
  <meta property="og:description" content="A defence of an interest-relative theory of knowledge" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Interests in Epistemology | Knowledge" />
  
  <meta name="twitter:description" content="A defence of an interest-relative theory of knowledge" />
  <meta name="twitter:image" content="https://brian.weatherson.org/kahis/tree.jpg" />

<meta name="author" content="Brian Weatherson">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="prologue.html">
<link rel="next" href="belief.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="kahis.html">Knowledge: A Human Interest Story</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Front Matter</a></li>
<li class="chapter" data-level="1" data-path="prologue.html"><a href="prologue.html"><i class="fa fa-check"></i><b>1</b> Prologue</a></li>
<li class="chapter" data-level="2" data-path="interests.html"><a href="interests.html"><i class="fa fa-check"></i><b>2</b> Interests in Epistemology</a><ul>
<li class="chapter" data-level="2.1" data-path="interests.html"><a href="interests.html#redblue"><i class="fa fa-check"></i><b>2.1</b> Red or Blue?</a></li>
<li class="chapter" data-level="2.2" data-path="interests.html"><a href="interests.html#fourfamilies"><i class="fa fa-check"></i><b>2.2</b> Four Families</a></li>
<li class="chapter" data-level="2.3" data-path="interests.html"><a href="interests.html#orthodox"><i class="fa fa-check"></i><b>2.3</b> Against Orthodoxy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="interests.html"><a href="interests.html#orthodoxmoore"><i class="fa fa-check"></i><b>2.3.1</b> Moore’s Paradox</a></li>
<li class="chapter" data-level="2.3.2" data-path="interests.html"><a href="interests.html#superknow"><i class="fa fa-check"></i><b>2.3.2</b> Super Knowledge to the Rescue?</a></li>
<li class="chapter" data-level="2.3.3" data-path="interests.html"><a href="interests.html#probrescue"><i class="fa fa-check"></i><b>2.3.3</b> Rational Credences to the Rescue?</a></li>
<li class="chapter" data-level="2.3.4" data-path="interests.html"><a href="interests.html#orthodoxevidence"><i class="fa fa-check"></i><b>2.3.4</b> Evidential Probability</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="interests.html"><a href="interests.html#oddsandstakes"><i class="fa fa-check"></i><b>2.4</b> Odds and Stakes</a></li>
<li class="chapter" data-level="2.5" data-path="interests.html"><a href="interests.html#whatinterests"><i class="fa fa-check"></i><b>2.5</b> Theoretical Interests Matter</a></li>
<li class="chapter" data-level="2.6" data-path="interests.html"><a href="interests.html#global"><i class="fa fa-check"></i><b>2.6</b> Global Interest Relativity</a></li>
<li class="chapter" data-level="2.7" data-path="interests.html"><a href="interests.html#neutrality"><i class="fa fa-check"></i><b>2.7</b> Neutrality</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="belief.html"><a href="belief.html"><i class="fa fa-check"></i><b>3</b> Belief</a><ul>
<li class="chapter" data-level="3.1" data-path="belief.html"><a href="belief.html#beliefsinterests"><i class="fa fa-check"></i><b>3.1</b> Beliefs and Interests</a></li>
<li class="chapter" data-level="3.2" data-path="belief.html"><a href="belief.html#mapslegends"><i class="fa fa-check"></i><b>3.2</b> Maps and Legends</a></li>
<li class="chapter" data-level="3.3" data-path="belief.html"><a href="belief.html#given"><i class="fa fa-check"></i><b>3.3</b> Taking As Given</a></li>
<li class="chapter" data-level="3.4" data-path="belief.html"><a href="belief.html#block"><i class="fa fa-check"></i><b>3.4</b> Blocking Belief</a></li>
<li class="chapter" data-level="3.5" data-path="belief.html"><a href="belief.html#questions"><i class="fa fa-check"></i><b>3.5</b> Questions and Conditional Questions</a></li>
<li class="chapter" data-level="3.6" data-path="belief.html"><a href="belief.html#changes"><i class="fa fa-check"></i><b>3.6</b> A Million Dead End Streets</a><ul>
<li class="chapter" data-level="3.6.1" data-path="belief.html"><a href="belief.html#mecorrect"><i class="fa fa-check"></i><b>3.6.1</b> Correctness</a></li>
<li class="chapter" data-level="3.6.2" data-path="belief.html"><a href="belief.html#meimpractical"><i class="fa fa-check"></i><b>3.6.2</b> Impractical Propositions</a></li>
<li class="chapter" data-level="3.6.3" data-path="belief.html"><a href="belief.html#threeway"><i class="fa fa-check"></i><b>3.6.3</b> Choices with More Than Two Options</a></li>
<li class="chapter" data-level="3.6.4" data-path="belief.html"><a href="belief.html#meties"><i class="fa fa-check"></i><b>3.6.4</b> Hard Times and Close Calls</a></li>
<li class="chapter" data-level="3.6.5" data-path="belief.html"><a href="belief.html#modalupdate"><i class="fa fa-check"></i><b>3.6.5</b> Updates and Modals</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="belief.html"><a href="belief.html#nearby-views"><i class="fa fa-check"></i><b>3.7</b> Nearby Views</a><ul>
<li class="chapter" data-level="3.7.1" data-path="belief.html"><a href="belief.html#ganson"><i class="fa fa-check"></i><b>3.7.1</b> Ganson’s Theory</a></li>
<li class="chapter" data-level="3.7.2" data-path="belief.html"><a href="belief.html#usc"><i class="fa fa-check"></i><b>3.7.2</b> Ross and Schroeder’s Theory</a></li>
<li class="chapter" data-level="3.7.3" data-path="belief.html"><a href="belief.html#leitgeb"><i class="fa fa-check"></i><b>3.7.3</b> Leitgeb’s Stability Theory</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="belief.html"><a href="belief.html#weakbelief"><i class="fa fa-check"></i><b>3.8</b> Weak Belief</a></li>
<li class="chapter" data-level="3.9" data-path="belief.html"><a href="belief.html#probone"><i class="fa fa-check"></i><b>3.9</b> Belief as Probability One</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="knowledge.html"><a href="knowledge.html"><i class="fa fa-check"></i><b>4</b> Knowledge</a><ul>
<li class="chapter" data-level="4.1" data-path="knowledge.html"><a href="knowledge.html#structure"><i class="fa fa-check"></i><b>4.1</b> Knowledge and Practical Interests</a></li>
<li class="chapter" data-level="4.2" data-path="knowledge.html"><a href="knowledge.html#theoreticalknowledge"><i class="fa fa-check"></i><b>4.2</b> Theoretical Knowledge</a></li>
<li class="chapter" data-level="4.3" data-path="knowledge.html"><a href="knowledge.html#knowledge-and-closure"><i class="fa fa-check"></i><b>4.3</b> Knowledge and Closure</a><ul>
<li class="chapter" data-level="4.3.1" data-path="knowledge.html"><a href="knowledge.html#andelim"><i class="fa fa-check"></i><b>4.3.1</b> Single Premise Closure</a></li>
<li class="chapter" data-level="4.3.2" data-path="knowledge.html"><a href="knowledge.html#andintro"><i class="fa fa-check"></i><b>4.3.2</b> Multiple Premise Closure</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="knowledge.html"><a href="knowledge.html#closuresummary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="evidence.html"><a href="evidence.html"><i class="fa fa-check"></i><b>5</b> Evidence</a><ul>
<li class="chapter" data-level="5.1" data-path="evidence.html"><a href="evidence.html#evpuzzle"><i class="fa fa-check"></i><b>5.1</b> A Puzzle About Evidence</a></li>
<li class="chapter" data-level="5.2" data-path="evidence.html"><a href="evidence.html#simplesolution"><i class="fa fa-check"></i><b>5.2</b> A Simple, but Incomplete, Solution</a></li>
<li class="chapter" data-level="5.3" data-path="evidence.html"><a href="evidence.html#radicalinterpretation"><i class="fa fa-check"></i><b>5.3</b> The Radical Interpreter</a></li>
<li class="chapter" data-level="5.4" data-path="evidence.html"><a href="evidence.html#globalgame"><i class="fa fa-check"></i><b>5.4</b> Motivating Risk-Dominant Equilibria</a><ul>
<li class="chapter" data-level="5.4.1" data-path="evidence.html"><a href="evidence.html#cvdproof"><i class="fa fa-check"></i><b>5.4.1</b> The Dominance Argument for Risk-Dominant Equilibria</a></li>
<li class="chapter" data-level="5.4.2" data-path="evidence.html"><a href="evidence.html#perfectri"><i class="fa fa-check"></i><b>5.4.2</b> Making One Signal Precise</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="evidence.html"><a href="evidence.html#evsolution"><i class="fa fa-check"></i><b>5.5</b> Objections and Replies</a></li>
<li class="chapter" data-level="5.6" data-path="evidence.html"><a href="evidence.html#cutelim"><i class="fa fa-check"></i><b>5.6</b> Evidence, Knowledge and Cut-Elimination</a></li>
<li class="chapter" data-level="5.7" data-path="evidence.html"><a href="evidence.html#basic"><i class="fa fa-check"></i><b>5.7</b> Basic Knowledge and Non-Inferential Knowledge</a></li>
<li class="chapter" data-level="5.8" data-path="evidence.html"><a href="evidence.html#weakness"><i class="fa fa-check"></i><b>5.8</b> Epistemic Weakness</a></li>
<li class="chapter" data-level="5.9" data-path="evidence.html"><a href="evidence.html#neta"><i class="fa fa-check"></i><b>5.9</b> Holism and Defeaters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ratbel.html"><a href="ratbel.html"><i class="fa fa-check"></i><b>6</b> Rational Belief</a><ul>
<li class="chapter" data-level="6.1" data-path="ratbel.html"><a href="ratbel.html#atomism"><i class="fa fa-check"></i><b>6.1</b> Atomism about Rational Belief</a></li>
<li class="chapter" data-level="6.2" data-path="ratbel.html"><a href="ratbel.html#lockecoin"><i class="fa fa-check"></i><b>6.2</b> Coin Puzzles</a></li>
<li class="chapter" data-level="6.3" data-path="ratbel.html"><a href="ratbel.html#lockegames"><i class="fa fa-check"></i><b>6.3</b> Playing Games</a></li>
<li class="chapter" data-level="6.4" data-path="ratbel.html"><a href="ratbel.html#lockepuzzles"><i class="fa fa-check"></i><b>6.4</b> Puzzles for Lockeans</a><ul>
<li class="chapter" data-level="6.4.1" data-path="ratbel.html"><a href="ratbel.html#lockearb"><i class="fa fa-check"></i><b>6.4.1</b> Arbitrariness</a></li>
<li class="chapter" data-level="6.4.2" data-path="ratbel.html"><a href="ratbel.html#lockecorrect"><i class="fa fa-check"></i><b>6.4.2</b> Correctness</a></li>
<li class="chapter" data-level="6.4.3" data-path="ratbel.html"><a href="ratbel.html#lockemoore"><i class="fa fa-check"></i><b>6.4.3</b> Moorean Paradoxes</a></li>
<li class="chapter" data-level="6.4.4" data-path="ratbel.html"><a href="ratbel.html#closure"><i class="fa fa-check"></i><b>6.4.4</b> Closure and the Lockean Theory</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ratbel.html"><a href="ratbel.html#solving"><i class="fa fa-check"></i><b>6.5</b> Solving the Challenges</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ratbel.html"><a href="ratbel.html#coins"><i class="fa fa-check"></i><b>6.5.1</b> Coins</a></li>
<li class="chapter" data-level="6.5.2" data-path="ratbel.html"><a href="ratbel.html#games"><i class="fa fa-check"></i><b>6.5.2</b> Games</a></li>
<li class="chapter" data-level="6.5.3" data-path="ratbel.html"><a href="ratbel.html#arbitrariness"><i class="fa fa-check"></i><b>6.5.3</b> Arbitrariness</a></li>
<li class="chapter" data-level="6.5.4" data-path="ratbel.html"><a href="ratbel.html#moore"><i class="fa fa-check"></i><b>6.5.4</b> Moore</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ties.html"><a href="ties.html"><i class="fa fa-check"></i><b>7</b> Hard Choices</a><ul>
<li class="chapter" data-level="7.1" data-path="ties.html"><a href="ties.html#frankielee"><i class="fa fa-check"></i><b>7.1</b> An Example</a></li>
<li class="chapter" data-level="7.2" data-path="ties.html"><a href="ties.html#tiesresponse"><i class="fa fa-check"></i><b>7.2</b> Responding to the Challenge, Quickly</a></li>
<li class="chapter" data-level="7.3" data-path="ties.html"><a href="ties.html#backearth"><i class="fa fa-check"></i><b>7.3</b> Back to Earth</a></li>
<li class="chapter" data-level="7.4" data-path="ties.html"><a href="ties.html#supermarketquestions"><i class="fa fa-check"></i><b>7.4</b> I Have Questions</a></li>
<li class="chapter" data-level="7.5" data-path="ties.html"><a href="ties.html#satisfied"><i class="fa fa-check"></i><b>7.5</b> You’ll Never Be Satisfied (If You Try to Maximise)</a></li>
<li class="chapter" data-level="7.6" data-path="ties.html"><a href="ties.html#deliberationcosts"><i class="fa fa-check"></i><b>7.6</b> Deliberation Costs and Infinite Regresses</a></li>
<li class="chapter" data-level="7.7" data-path="ties.html"><a href="ties.html#ignorancebliss"><i class="fa fa-check"></i><b>7.7</b> Ignorance is Bliss</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>8</b> To Be Written</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Knowledge</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interests" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Interests in Epistemology</h1>
<div id="redblue" class="section level2">
<h2><span class="header-section-number">2.1</span> Red or Blue?</h2>
<p>The key argument that knowledge is interest-relative starts with a puzzle about a game. Here are the rules of the game, which I’ll call the Red-Blue game.</p>
<ol style="list-style-type: decimal">
<li>Two sentences will be written on the board, one in red, one in blue.</li>
<li>The player will make two choices.</li>
<li>First, they will pick a colour, red or blue.</li>
<li>Second, they say whether the sentence in that colour is true or
false.</li>
<li>If they are right, they win. If not, they lose.</li>
<li>If they win, they get $50, and if they lose, they get nothing.</li>
</ol>
<p>Our player is Anisa. She has been reading some medieval history, and last night was reading about the Battle of Agincourt. She was amused to see that it too place on her birthday, October 25, and in 1415, precisely 595 years before her own birthday. The book says all these things about the Battle of Agincourt because they are actually true, and when she read the book, Anisa believed them. She believed them because she had lots of independent evidence that the book was reliable (it came from a respected author and publisher, it didn’t contradict her well-grounded background beliefs), and she was sensitive to that evidence of its reliability. And, indeed, the book was generally reliable, as well as accurate on this point.</p>
<p>Anisa comes to know that she is playing the Red-Blue game, and that these are its rules. She does not come to know any other relevant fact about the game. When the game starts, the following two sentences are written on the board, the first in red, the second in blue.</p>
<ul>
<li>Two plus two equals four.</li>
<li>The Battle of Agincourt took place in 1415.</li>
</ul>
<p>Anisa looks at this, thinks to herself, “Oh, my book said that the Battle of Agincourt was in 1415, so (given the rules of the game) playing Blue-True will be as good as any other play, so I’m playing Blue-True. Playing Red-True would get the same amount, since obviously two plus two is four, but I’m going to play Blue-True instead”. And that’s what she does, and she wins the $50.</p>
<p>Intuitively, Anisa’s move here is irrational. It doesn’t cost her anything - she gets the $50. And it’s not that irrational as these things go - she costs herself $50 in the somewhat distant worlds where her reliable book gets this fact wrong. But it was still irrational. She took a needless risk, when there was a simple safe option on the table.</p>
<p>I’m going to argue, at some length, that the best explanation of why it is irrational for Anisa to play Blue-True is that knowledge is interest-relative. Given her interests in learning about late medieval history, when she was at home reading the book, Anisa knew that the Battle of Agincourt did take place in 1415. Given her interests in playing this game well, Anisa does not know this. When she is moved into the game situation, she loses some knowledge she previously had.</p>
<p>Interest-relativity is often taken to be a wild and radical development in the theory of knowledge. And it is certainly a reform. It’s not a new reform proposal; both the proposal, and many of the details, are set out in works by Jeremy Fantl and Matthew McGrath <span class="citation">(<a href="references.html#ref-FantlMcGrath2002">2002</a>, <a href="references.html#ref-FantlMcGrath2009">2009</a>)</span>, John <span class="citation">Hawthorne (<a href="references.html#ref-Hawthorne2004">2004</a>)</span>, and Jason <span class="citation">Stanley (<a href="references.html#ref-Stanley2005">2005</a>)</span>. But relative to the epistemological status quo circa 1990, it is different. But then again, factors that were widely held to affect knowledge according to the status quo of either today or of 1990 would have seemed wild and radical relative to the epistemological status quo circa 1960. The factors that make a belief safe, or sensitive, or reliable, or undefeated, were well outside the realms of factors that late 20th century epistemologists thought relevent to knoweldge. There are many things that are irrelevant to how probable a belief is that are relevant to whether it is knowledge, as the epistemological literature of the late 20th Century makes clear. The proposal here is that interests are one more addition to this motley bunch.</p>
<p>The standard arguments for and against interest-relativity to date have not focussed on examples like Anisa’s, but on examples like Blaise that I’ll present shortly. There are exceptions. The structure of Anisa’s example is similar, in the features that matter to me, to the examples of low-cost checking that Bradley <span class="citation">Armour-Garb (<a href="references.html#ref-ArmourGarb2011">2011</a>)</span> discusses. (Though he draws contextualist conclusions from these examples, not interest-relative ones.) And it is similar to some of the cases of three-way choice that Charity Anderson and John Hawthorne deploy in arguing against interest-relativity <span class="citation">(<a href="references.html#ref-AndersonHawthorne2019a">2019</a><a href="references.html#ref-AndersonHawthorne2019a">a</a>, <a href="references.html#ref-AndersonHawthorne2019b">2019</a><a href="references.html#ref-AndersonHawthorne2019b">b</a>)</span>. But mostly people have focussed on cases like the following.</p>
<p>Last night, Blaise was reading the same book that Anisa was reading. And he too was struck by the fact that the Battle of Agincourt took place on October 25, 1415. Today he is visited by a representative of the supernatural world, and offered the following bet. (Blaise knows these are the terms of the bet, and doesn’t know anything else relevant.) If he declines the bet, life will go on as normal. If he accepts, one of two things will happen.</p>
<ul>
<li>If it is true that the Battle of Agincourt took place in 1415, an infant somewhere will receive one second’s worth of pure joy, of the kind infants often get playing peek-a-boo.</li>
<li>If it is false that the Battle of Agincourt took place in 1415, all of humanity will be cast into The Bad Place for all of eternity.</li>
</ul>
<p>Blaise takes the bet. The Battle of Agincourt was in 1415, and he can’t bear the thought of a lovable baby missing that second of pure joy.</p>
<p>Again, there is an intuition that Blaise did something horribly wrong here. And this intuition is best explained, I will argue, by letting knowledge be interest-relative. But the argument that the interest-relativity of knowledge is the best explanation of what’s going on is, in my view, somewhat weaker in Blaise’s case than in Anisa’s. It’s not that I think the interest-relative explanation of the case is wrong; in fact I think it’s basically correct. It’s rather that there are somewhat more plausible interest-invariant explanations of Blaise’s case than of Anisa’s. So I’ll focus on Anisa, not Blaise.</p>
<p>This choice of focus occasionally means that this book is less connected to the existing literature than I would like. I occasionally infer what a philosopher would say about cases like Anisa from what they have said about cases like Blaise. And I suspect in some cases I’ll get those inferences wrong. But I want to set out the best argument for the interest-relativity of knowledge that I know, and that means going via the example of Anisa.</p>
<p>Though I am starting with an example, and with an intuition about it, I am not starting with an intuition about what is known in the example. I don’t have any clear intuitions about what Anisa knows or doesn’t know while playing the Red-Blue game. The intuition that matters here is that her choice of Blue-True is irrational. It’s going to be a matter of inference, not intuition, that Anisa lacks knowledge.</p>
<p>And that inference will largely be by process of elimination. In section <a href="interests.html#fourfamilies">2.2</a> I will set out four possible things we can say about Anisa, and argue that one of them must be true. (The argument won’t appeal to any principles more controversial than the Law of Excluded Middle.) But all four of them, including the interest-relative view I favour, have fairly counterintuitive consequences. So something counterintuitive is true around here. And this puts a limit on how we can argue. At least one instance of the argument <em>this is counterintuitive, so it is false</em> must fail. And that casts doubt over all such arguments. This is a point that critics of interest-relativity haven’t sufficiently acknowledged, but it also puts constraints on how one can defend interest-relativity.</p>
<p>When Anisa starts playing the Red-Blue game, her practical situation changes. So you might think I’ve gone wrong in stressing Anisa’s interests, not her practical situation. I’ve put the focus on interests for two reasons. One is that if Anisa is totally indifferent to money, then there is no rational requirement to play Red-True. We need to posit something about Anisa’s interests to even get the data point that the interest-relative theory explains. The second reason, which I’ll talk about more in section <a href="interests.html#whatinterests">2.5</a>, is that sometimes we can lose knowledge due to a change not in our practical situation, but our theoretical interests.</p>
<p>In the existing literature, views like mine are sometimes called versions of <strong>subject-sensitive invariantism</strong>, since they make knowledge relevant to the stakes and salient alternatives available to the subject. But this is a bad name; of course whether a knowledge ascription is true is sensitive to who the subject of the ascription is. I know what I had for breakfast and you (probably) don’t. What is distinctive is which features of the subject’s situation that the interest-relative theory says are relevant, and calling it the interest-relative theory of knowledge makes it clear that it is the subject’s interests. In the past, I’ve called it <strong>interest-relative invariantism</strong>. But, for reasons I’ll say more about in section <a href="interests.html#neutrality">2.7</a>, I’m not committed to <em>invariantism</em> in this book. So it’s just the interest-relative theory, or IRT.</p>
</div>
<div id="fourfamilies" class="section level2">
<h2><span class="header-section-number">2.2</span> Four Families</h2>
<p>A lot of philosophers have written about cases like Anisa and Blaise over the last couple of decades. Relatedly, there are a huge number of theories that have been defended concerning these cases. Rather than describe them all, I’m going to start with a taxonomy of them. The taxonomy has some tricky edge cases, and it isn’t always trivial to classify a philosopher from their statements about the cases. But I find it a helpful way to start thinking about the possible moves available.</p>
<p>Our first group of theories are the <strong>sceptical</strong> theories. They deny that Anisa ever knew that the Battle of Agincourt was in 1415. The particular kind of sceptic I have in mind says that if someone’s epistemic position is, all things considered, better with respect to <span class="math inline">\(q\)</span> than with respect to <span class="math inline">\(p\)</span>, that person doesn’t know that <span class="math inline">\(p\)</span>. The core idea for this sceptic, which perhaps they draw from work by Peter <span class="citation">Unger (<a href="references.html#ref-Unger1975">1975</a>)</span>, is that knowledge is a maximal epistemic state, so any non-maximal state is not knowledge. The sceptics say that for almost any belief, Anisa’s belief that two plus two is four will have higher epistemic standing than that belief, so that belief doesn’t amount to knowledge.</p>
<p>Our second group of theories are what I’ll call <strong>epistemicist</strong> theories. The epistemicists say that Anisa’s reasoning, and perhaps Blaise’s reasoning too, is perfectly sound. They both know when the Battle of Agincourt took place, so they both know that the choices they take are optimal, so they are rational in taking those choices. The intuitions to the contrary are, say the epistemicist, at best confused. There is something off about Anisa and Blaise, perhaps, but it isn’t that these particular decisions are irrational.</p>
<p>It’s not essential to epistemicism as I’m construing it, but I think by far the most plausible form of epistemicism takes on board Maria Lasonen-Aarnio’s point that act-level and agent-level assessments might come apart.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Taking the bet reveals something bad about Blaise’s character, and arguably manifests a vice, but the act itself is rational. It’s that last claim, that the actions are rational, that is distinctive of epistemicism as I’m understanding it.</p>
<p>The third group is the <strong>pragmatist</strong> theories, and this group includes the interest-relative theory that I’ll defend. The pragmatists say that Anisa did know when the Battle of Agincourt was, but now she doesn’t. The change in her practical situation, combined with her interest in getting more money, destroys her knowledge.</p>
<p>And the final group are what I’ll call, a little tendentiously, the <strong>orthodox</strong> theories. Orthodoxy says that Anisa knew when the Battle of Agincourt was last night, since her belief satisfied every plausible criteria for testimonial knowledge. And it says she knows it today, since changing practical scenarios or interests like this doesn’t affect knowledge. But it also says that the actions that Anisa and Blaise take are wrong; they are both irrational, and Blaise’s is arguably immoral. And that is true because of how risky the actions are. So knowing that what you are doing is for the best is consistent with your action being faulted on epistemic grounds.</p>
<p>My reading of the literature is that a considerable majority of philosophers writing on these cases are orthodox. (Hence the name!) But I can’t be entirely sure, because a lot of these philosophers are more vocal about opposing pragmatist views than they are about supporting any particular view. There are some views that are clearly orthodox in the sense I’ve described, and I really think most of the people who have opposed pragmatist treatments of these cases are orthodox, but it’s possible more of them are sceptical or epistemicist than I’ve appreciated.</p>
<p>Calling this last group orthodox lets me conveniently label the other three groups as heterodox. And this lets me state what I hope to argue for in this book. I think that the interest-relative treatment of the cases is correct; and if it isn’t, then at least some pragmatist treatment is correct; and if it isn’t, then at least some heterodox treatment is correct.</p>
<p>And it’s worth laying out the interest-relative case in some detail, because we can only properly assess the options holistically. Every view is going to have some very counterintuitive consequences, and we can only weigh them up when we see them all laid out.</p>
<p>The last claim, that every view has counterintuitive consequences, deserves some defence. I’ll say much more about the challenges orthodoxy faces in section <a href="interests.html#orthodox">2.3</a>. But just to set out a simple version of the problems for each theory, observe all of the following look true.</p>
<ul>
<li>Sceptical theories imply that when Anisa is reading her book, she doesn’t gain knowledge even though the book is reliable and she believes it because of a well-supported belief in its reliability.</li>
<li>Epistemicist theories imply that Anisa and Blaise make rational choices, even though they take what look like absurd risks.</li>
<li>Pragmatist theories say that offering someone a bet can cause them to lose knowledge and, presumably, that withdrawing that offer can cause them to get the knowledge back.</li>
<li>Orthodox theories say that it is irrational to do something that one knows will get the best result simply because it might get a bad result.</li>
</ul>
<p>Much of what follows in this book, like much of what’s in this literature, will fall into one of two categories. Either it will be an attempt to sharpen one of these implausible consequences, so the view with that consequence looks even worse than it does now. Or it will be an attempt to dull one of them, by coming up with a version of the view that doesn’t have quite as bad a consequence. Sometimes this latter task is sophistry in the bad sense; it’s an attempt to make the implausible consequence of the theory harder to say, and so less of an apparent flaw on that ground alone. But sometimes it is valuable drawing of distinctions. (That is, scholasticism in the good sense.) It turns out that the alleged plausible claim is ambiguous; on one disambiguation we have really good reason to believe it is true, on another the theory in question violates it, but on no disambiguation do we get a violation of something really well-supported. I hope that they work I do here to defend the interest-relative theory is more scholastic than sophistic, but I’ll leave that for others to decide.</p>
<p>Still, if all of the theories are implausible in one way or another, shouldn’t we look for an alternative? Perhaps we should look, but we won’t find any. At least if we define the theories carefully enough, the truth is guaranteed to be among them. Let’s try placing theories by asking three yes/no questions.</p>
<ol style="list-style-type: decimal">
<li>Does the theory say that Anisa knew last night that the Battle of Agincourt was in 1415? If no, the theory is sceptical; if yes, go to question 2.</li>
<li>Does the theory say that Anisa is rational to play Blue-True? If yes, the theory is epistemicist; if no, go to question 3.</li>
<li>Does the theory say that Anisa still knows that the Battle of Agincourt was in 1415, at the time she chooses to play Blue-True? If no, the theory is pragmatist; if yes, the theory is orthodox.</li>
</ol>
<p>That’s it - those are your options. There are two two points of clarification that matter, but I don’t think they make a huge difference.</p>
<p>The first point of clarification is really a reminder that these are families of views. It might be that one member of the family is considerably less implausible than other members. Indeed, I’ve changed my mind a fair bit about what is the best kind of pragmatist theory since I first started writing on this. And there are a lot of possible orthodox theories. Finding out the best version of these kinds of theories, especially the last two kinds, is hard work, but it is worth doing. But I very much doubt it will lessen the implausibility of the resulting theory; some of the implausibility flows directly from how one answers the three questions.</p>
<p>The second point of clarification is that what I’ve really done here is classify what the different theories say about Anisa’s case. They may say different things about other cases. A theory might take an epistemicist stand on Anisa’s case, but an orthodox one on Blaise’s case, for example. Or it might be orthodox about Anisa, but would be epistemicist if the blue sentence was something much more secure, such as that the Battle of Hastings was in 1066. If this taxonomy is going to be complete, it needs to say something about theories that treat different cases differently. So here is the more general taxonomy I will use.</p>
<p>The cases I’ll quantify over have the following structure. The hero is given strong evidence for some truth <span class="math inline">\(p\)</span>, and they believe it on the basis of that evidence. There are no defeaters, the belief is caused by the truth of the proposition in the right way, and in general all the conditions for knowledge that people worried about in the traditional (i.e., late twentieth century) epistemological literature are met. Then they are offered a choice, where one of the options will have an optimal outcome if <span class="math inline">\(p\)</span>, but will not maximise expected value unless the probability of <span class="math inline">\(p\)</span> is absurdly close to 1. And while hero’s evidence is strong, it isn’t that strong. Despite this, hero takes the risky option, using the fact that <span class="math inline">\(p\)</span> as a key part of their reasoning. Now consider the following three questions.</p>
<ol style="list-style-type: decimal">
<li>In cases with this form, does the theory say that when the hero first forms the belief that <span class="math inline">\(p\)</span>, they know that <span class="math inline">\(p\)</span>? If the answer is that this is <em>generally</em> the case, then restrict attention to those cases where they do know that <span class="math inline">\(p\)</span>, and move to question 2. Otherwise, the theory is sceptical.</li>
<li>In the cases that remain, is hero rational in taking the option that is optimal if <span class="math inline">\(p\)</span>, but requires very high probability to maximise expected returns? If the answer is yes in <em>every</em> case, the theory is epistemicist. Otherwise, restrict attention to cases where this choice is irrational, and move to question 3.</li>
<li>In <em>any</em> of the cases that remain, does the fact that hero was offered the choice destroy their knowledge that <span class="math inline">\(p\)</span>? If yes, the theory is pragmatic. If no, the theory is orthodox.</li>
</ol>
<p>So I’m taking epistemicism to be a very strong theory - it says that knowledge always suffices for action that is optimal given what’s known, and that offers of bets never constitute a loss of knowledge. The epistemicist can allow that the offer of a bet may cause a person to ‘lose their nerve’, and hence their belief that <span class="math inline">\(p\)</span>, and hence their knowledge that <span class="math inline">\(p\)</span>. But if they remain confident in <span class="math inline">\(p\)</span>, they retain knowledge that <span class="math inline">\(p\)</span>.</p>
<p>And I’m taking pragmatism to be a very weak theory - it says sometimes the offer of a bet can constitute a loss of knowledge. The justification for defending such a weak theory is that so many philosophers are aghast at the idea that practical considerations like this could ever be relevant to knowledge. So even showing that the existential claim is true, that sometimes practical issues matter, would be a big deal.</p>
<p>And orthodoxy is a weak claim on one point, and a strong claim on another. It says there are some cases where knowledge does not suffice for action - though it might take these cases to be very rare. It is common in defences of orthodoxy to say that the cases are quite rare, and use this fact to explain away intuitions that threaten orthodoxy. But it says that pragmatic factors never matter - so it can be threatened by a single case like Anisa.</p>
</div>
<div id="orthodox" class="section level2">
<h2><span class="header-section-number">2.3</span> Against Orthodoxy</h2>
<p>The orthodox view of cases like Blaise’s is that offering him that that does not change what he knows, but still he is irrational to take the bet. In this section, I’m going to run through a series of arguments against the orthodox view. The reason I making so many arguments is not that I think any one of them is particularly week. Rather, it is because the orthodox view is so widespread that I think we need to stress how many strange consequences it has.</p>
<div id="orthodoxmoore" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Moore’s Paradox</h3>
<p>Start by thinking about what the Orthodox few says of rational person in Blaise’s situation would do. Call this rational person Chamari. According to the orthodox view, offering someone a bit does not make them lose knowledge. So Chamari still knows when the Battle of Avignon was fought. But Chamari is rational, so Chamari will clearly decline the bet. Think about how Chamari might respond when you ask her to justify declining the bet.</p>
<blockquote>
<p>You: When was the Battle of Avignon?<br />
Chamari: October 25, 1415.<br />
You: If that’s true, what will happen if you accept the bet?<br />
Chamari: A child will get a moment of joy.<br />
You: Is that a good thing?<br />
Chamari: Yes.<br />
You: So why didn’t you take the bet?<br />
Chamari: Because it’s too risky.<br />
You: Why is it risky?<br />
Chamari: Because it might lose.<br />
You: You mean the Battle of Avignon might not have been fought in 1415.<br />
Chamari: Yes.<br />
You: So the Battle of Avignon was fought in 1415, but it might not have been fought then?<br />
Chamari: Yes, the Battle of Avignon was fought in 1415, but it might not have been fought then, and that’s why I’m not taking the bet.</p>
</blockquote>
<p>I think Chamari has given the best possible answer at each point. But she has ended up assenting to a Moore-paradoxical sentence. In particular, she has assented to a sentence of the form <em>p, but it might be that not p</em>. And it is very widely held that sentences like this cannot be rationally assented to. Since Chamari was, by stipulation, the model for what the orthodox view thinks a rational person is, this shows that the orthodox view is false.</p>
<p>There are three ways out of this puzzle, and none of them seems particularly attractive.</p>
<p>One is to deny that there’s anything wrong with where Chamari ends up. Perhaps in this case the Moore-paradoxical claim is perfectly assertable. I have some sympathy for the general idea that philosophers over-state the badness of Moore-paradoxicality <span class="citation">(Maitra and Weatherson <a href="references.html#ref-MaitraWeatherson2010">2010</a>)</span>. But I have to say, in this instance it seems like a terrible way to end the conversation.</p>
<p>Another is to deny that the fact that Chamari knows something licences her in asserting it. I’ve assumed in the argument that if Chamari knows that <span class="math inline">\(p\)</span>, she can say that <span class="math inline">\(p\)</span>. But maybe that’s strong an assumption. The conversation, says this reply, goes off the rails at the very first line. But on this way of thinking, it is hard to know what the point of knowledge is. If knowing something isn’t sufficiently good reason to assert it, it is hard to know what would be.</p>
<p>The orthodox theorist has a couple of choices here, neither of them good. One is to say that although knowledge is not interest-relative, the epistemic standards for assertion are interest-relative. Basically, Chamari meets the epistemic standard for saying that <span class="math inline">\(p\)</span> only if Chamari knows that <span class="math inline">\(p\)</span> according to the (false!) interest-relative theory. But at this point, given how plausible it is that knowledge is closely connected with testimony, it seems we would need an excellent reason to not simply identify knowledge with this epistemic standard. The other is to say that there is some interest-invariant standard for assertion. But versions of Blaise’s case show that this standard would have to be something like Cartesian certainty. So most everything we say, every single day, would be norm violating. Such a norm is not plausible.</p>
<p>So we get to the third way out, one that is only available to a subset of orthodox theorists. We can say that ‘knows’ is context-sensitive, that in Chamari’s context the sentence “I know when the Battle of Agincourt was fought” is <em>false</em>, and use these facts to explain away what goes wrong in the conversation with Chamari. <span class="citation">Armour-Garb (<a href="references.html#ref-ArmourGarb2011">2011</a>)</span>, who points out how much trouble non-contextualist orthodox theorists get into with these Moore-paradoxical claims, suggests a contextualist resolution of the puzzles. And this is probably the least bad way to handle the case, but it’s worth noting just how odd it is.</p>
<p>It’s not immediately obvious how to get from contextualism to a resolution of the puzzle. Chamari doesn’t use the verb ‘to know’ or any of its cognates. She does use the modal ‘might’, and the contextualist will presumably want to say that it is context sensitive. But that doesn’t look like a helpful way to solve the problem, since her assertion that the Battle might have been on a different day seems like the good part of what she says. What’s problematic is the unqualified assertion about when the battle was. And we need some way of connecting contextualism about epistemic verbs to a claim about the inappropriateness of this assertion.</p>
<p>The standard move by contextualists here is to simply deny that there is a tight connection between knowledge and assertion <span class="citation">(DeRose <a href="references.html#ref-DeRose2002">2002</a>; Cohen <a href="references.html#ref-Cohen2004">2004</a>)</span>. (So this is really a form of the response I just rejected.) What they say instead is that there is a kind of meta-linguistic standard for assertion. It is epistemically responsible to say that <span class="math inline">\(p\)</span> iff it would be true to say <em>I know that p</em>. And since it would not be true for Chamari to say she knows when the Battle of Agincourt was fought, she can’t responsibly say when it was fought.</p>
<p>The most obvious worry with this line of reasoning is with the very idea of meta-linguistic norms like this. Imagine we were conversing with Chamari about her reasons for declining the bet in Bengali rather than English, but at every line a contribution with the same content was made? Would the reason her first answer was inappropriate be that some English sentence would be false if uttered in her context, or that some Bengali sentence would be false? If it’s an English sentence, it’s very weird that English would have this normative force over conversations in Bengali. If it’s Bengali, then it’s odd that the standard for assertion changes from language to language.</p>
<p>If there were a human language that didn’t have a verb for knowledge, then that last point could be made with particular force. What would the contextualists say is the standard for assertion in such a language? But there is, quite surprisingly, no such language <span class="citation">(Nagel <a href="references.html#ref-Nagel2014">2014</a>)</span>. It’s still a bit interesting to think about possible languages that do allow for assertions, but do not have a verb for knowledge. Just what the contextualists would say is the standard for assertion in such a language is a rather delicate matter.</p>
<p>But rather than thinking about these merely possible languages, let’s return to English, and end with a variant of the conversation with Chamari. Imagine that she hasn’t yet been offered the bet that Blaise is offered, and indeed that when the conversation starts, we’re just spending a pleasant few minutes idly chatting about medieval history.</p>
<blockquote>
<p>You: When was the Battle of Avignon?<br />
Chamari: October 25, 1415.<br />
You: Oh that’s interesting. Because you know there’s this bet, and if you accept it, and the Battle of Avignon was in 1415, then a small child gets a moment of joy.<br />
Chamari: That’s great, I should take that bet.<br />
You: Well, wait a second, I should tell you what happens if the Battle turns out to have been on any other date. [You explain what happens in some detail.]<br />
Chamari: That’s awful, I shouldn’t take the bet. The Battle might not have been in 1415, and it’s not worth the risk.<br />
You: So you won’t take the bet because it’s too risky?<br />
Chamari: That’s right, I won’t take it because it’s too risky.<br />
You: Why is it risky?<br />
Chamari: Because it might lose.<br />
You: You mean the Battle of Avignon might not have been fought in 1415.<br />
Chamari: Yes.<br />
You: Hang on, you just say it was fought in 1415, on October 25 to be precise.<br />
Chamari: That’s true, I did say that.<br />
You: Were you wrong to have said it?<br />
Chamari: Probably not; it was probably right that I said it.<br />
You: You probably knew when the battle was, but you don’t now know it?<br />
Chamari: No, I definitely didn’t know when the battle was, but it was probably right to have said it was in 1415.</p>
</blockquote>
<p>And you can probably see all sorts of ways of making Chamari’s position sound terrible. The argument I’m giving here is of course just a version of the argument John <span class="citation">MacFarlane (<a href="references.html#ref-MacFarlane2005-Knowledge">2005</a>)</span> gives arguing that contextualists have a problem with retraction. And Chamari’s position does sound very bad here.</p>
<p>But I don’t want to lean too much weight on how she sounds. Every position in this area ends up saying some strange things. The very idea that the epistemic standard for assertion could be meta-linguistic is, to my mind, even more implausible than the idea that we should end up where Chamari does.</p>
</div>
<div id="superknow" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Super Knowledge to the Rescue?</h3>
<p>Let’s leave Blaise and Chamari for a little and return to Anisa. The orthodox view agrees that it is irrational for Anisa to play Blue-True. So it needs to explain why this is so. I think there is a simple explanation. If she plays Red-True, she knows she will get $50; if she plays Blue-True, she does not know that - though she knows she will get at most $50. So Red-True is the weakly dominant option; she knows it won’t do worse than any other option, and there is no other option that she knows won’t do worse than any other option.</p>
<p>The orthodox theorist can’t offer this explanation. They think Anisa knows that Blue-True will get $50 as well. So what can they offer instead? There are two broad kinds of explanation thet can try. First, they might offer a structurally similar explanation to the one I just gave, but with some other epistemic notion at its centre. So while Anisa knows that Blue-True will get $50, she doesn’t <em>super-know</em> this, in some sense. Second, they can try to explain the asymmetry between Red-True and Blue-True in probablistic, rather than epistemic, terms. I’ll discuss the first option in this subjection, and the probabilistic notion in the next subsection.</p>
<p>What do I mean her by <em>super-knows</em>? I mean this phrase to be a placeholder for any kind of relation stronger than knowledge that could play the right kind of role in explaining why it is irrational for Anisa to play Blue-True. So super-knowledge might be iterated knowledge. Anisa super-knows something iff she knows that she knows that … she knows it. And she super-knows that two plus two is four, but not that the Battle of Agincourt was in 1415. Or super-knowledge might be (rational) certainty. Anisa is (rationally) certain that two plus two is four, but not that the Battle of Agincourt was in 1415. Or it might be some other similar relation. My objection to this kind of response won’t be sensitive to the details.</p>
<p>What is going to be important is that super-knowledge, whatever it is, looks like an epistemic relation. In particular, Anisa super-knows a conjunction (that she is considering) iff she super-knows each of the conjuncts. So we can’t equate super-knowledge with rational credence above a threshold, because rational credence above a threshold doesn’t satisfy this constraint. I’ll come back to credence based explanations of Anisa’s case in the next subsection.</p>
<p>The fact that Anisa doesn’t super-know when the Battle of Agincourt was can’t explain the asymmetry between Red-True and Blue-True. It can’t explain why Anisa rationally must choose Red-True. Even if she super-knows that two plus two is four, she doesn’t super-know the rules of the game. She has ordinary testimonial knowledge of those, just like she has ordinary testimonial knowledge about the Battle of Agincourt. In the description, I stipulated that she didn’t know anything relevant about the game set up other than what I said there. So she isn’t certain about the rules, and she doesn’t even know that she knows them. Maybe you think that last is unrealistic, and it is important that the example is realistic. But even on a realistic treatment of the game, she won’t super-know the rules. If testimony from careful historians can’t generate super-knowledge, neither can testimony from game-show hosts.</p>
<p>In fact, her knowledge of the rules of the game, in the sense that matters, is probably weaker than her knowledge of history. It is not unknown for game shows to promise prizes, then fail to deliver them, either because of malice or incompetence. Knowledge of the game rules, in particular knowledge that she will actually get $50 if she selects a true sentence, requires some knowledge of the future. And that seems harder to obtain than knowledge of what happened in history. After all, she has to know that there won’t be an alien invasion, or a giant asteroid, or an incompetent or malicious game orgAnisar.</p>
<p>So there is no way of understanding ‘super-knows’ such that 1 is true and 2 is false.</p>
<ol style="list-style-type: decimal">
<li>Anisa super-knows that if she plays Red-True, she’ll win $50.</li>
<li>Anisa does not super-know that if she plays Blue-True, she’ll win $50.</li>
</ol>
<p>And that’s the kind of contrast we need in order for a super-knowledge based explanation of why she should play Red-True to work.</p>
<p>The point I’m making here, that in thinking about these games we need to attend to the player’s epistemic attitude towards the game itself, is not original Dorit <span class="citation">Ganson (<a href="references.html#ref-Ganson2019">2019</a>)</span> uses this point for a very similar purpose, and quotes Robert <span class="citation">Nozick (<a href="references.html#ref-Nozick1981">1981</a>)</span> making a similar point. But I’ve belaboured it a bit here because it is so easily overlooked. It is easy to take things that one is told about a situation, such as the rules of a game that are being played, as somehow fixed and inviolable. They aren’t the kind of thing that can be questioned. But in any realistic case, that will not be how things are.</p>
<p>This is why I want to rest more weight on Anisa’s case than on Blaise’s. I can’t appeal to your judgment about what a realistic version of Blaise’s case would be like, because there are no realistic versions of Blaise’s case. But Anisa’s case is very easy to imagine and understand. And we can ask what a realistic version of it would be like. And that version would be such that the player would know what the rules of the game are, but would also know that sometimes game shows don’t keep their promises, sometimes they don’t describe their own games accurately, sometimes players misinterpret or misunderstand instructions, and so on. This shouldn’t lead us to scepticism: Anisa knows what game she’s playing. But she doesn’t super-know what game she’s playing. And that means she doesn’t super-know that she’ll win if she plays Red-True.</p>
</div>
<div id="probrescue" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Rational Credences to the Rescue?</h3>
<p>So imagine the orthodox theorist drops super-knowledge, and looks somewhere else. A natural alternative is to use credences. Assume that the probability that the rules of the game are as described is independent of the probabilities of the red and blue sentence. And assume that Anisa must, if she is to be rational, maximise expected utility. Then we get the natural result that Anisa should pick the sentence that is more probably true.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> And that can explain why she must choose Red-True, which is what the orthodox theorist needed to explain.</p>
<p>This kind of approach doesn’t really have any place for knowledge in its theory of action. One should simply maximise expected utility; since doing what one knows to be best might not maximise expected utility, we shouldn’t think knowledge has any particularly special role.</p>
<p>But there are many problems with this kind of approach. Several of these will be discussed elsewhere in this book at more length, so I’ll here just point to those other discussions. But some others I’ll address directly.</p>
<p>Like the view discussed in subsection <a href="interests.html#orthodoxmoore">2.3.1</a> that separates knowledge from assertion, separating knowledge from action leads to strange consequences. As <span class="citation">Williamson (<a href="references.html#ref-Williamson2005">2005</a>)</span> points out, once we break apart knowledge from action in this way, it becomes hard to see the point of action. And it’s worth pausing a bit more over the bizarreness of the claim that Blaise knows that taking the bet will work out for the best, but he shouldn’t take it - because of it’s possible consequences!</p>
<p>If one excludes knowledge from having an important role in one’s theory of decision, one ends up having a hard time explaining how dominance reasoning works. But it is a compulsory question for a theory of decision to explain how dominance reasoning works. Among other things, we need a good account of how dominance reasoning works in order to handle Newcomb problems, and we need to handle Newcomb problems in order to even clearly state a theory of expected utility maximisation. That little argument was very compressed, but I’ll return frequently through the book to issues about dominance reasoning, and for now I think it’s enough to leave it with this sketch.</p>
<p>Probabilistic models of reasoning and decision have their limits. But what we need to explain about the Red-Blue game goes beyond those limits. So probabilistic models can’t be the full story about the Red-Blue game.</p>
<p>To see this, imagine for a second that the Blue sentence is not about the Battle of Agincourt, but is instead a slightly more complicated arithmetic truth, like <em>Thirteen times seventeen equals two hundred and twenty one</em>, or a slightly complicated logical truth, like <span class="math inline">\(\neg q \rightarrow ((p \rightarrow q) \rightarrow \neg p)\)</span>. If either of those are the blue sentence, then it is still uniquely rational to play Red-True. But the probability of each of those sentences is one. So rational choice is more demanding than expected utility maximisation. In sections <a href="ratbel.html#lockecoin">6.2</a> and <a href="ratbel.html#lockegames">6.3</a> I’ll go over more cases of propositions whose probability is 1, but which should be treated as uncertain even it is certain that two plus two is four. The lesson is that we can’t just use expected utility maximisation to explain the Red-Blue game.</p>
<p>Finally, we need to understand the notion of probability that’s being appealed to in this explanation. It can’t be some purely subjective notion, like credence, because that couldn’t explain why some decisions are rational and others aren’t. And it can’t be some purely physical notion, like chance or frequency, because that won’t even get the cases right. (What is the chance, or frequency, of the Battle of Agincourt being in 1415?) It needs to be something like evidential probability. And that will run into problems in versions of the Red-Blue game where the Blue sentence is arguably (but not certainly) part of the player’s evidence. I’ll end my discussion of orthodoxy with a discussion of cases like these.</p>
</div>
<div id="orthodoxevidence" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Evidential Probability</h3>
<p>No matter which of these explanations the orthodox theorist goes for, they need a notion of evidence to support them. Let’s assume that we can find some doxastic attitude <span class="math inline">\(D\)</span> such that Anisa can’t rationally stand in <span class="math inline">\(D\)</span> to <em>Play Blue-True</em>, and that this is why she can’t rationally play Blue-True. Then we need to ask the further question, why doesn’t she stand in relation <span class="math inline">\(D\)</span> to <em>Play Blue-True</em>? And presumably the answer will be that she lacks sufficient evidence. If she had optimal evidence about when the Battle of Agincourt was, she could play Blue-True, after all.</p>
<p>The orthodox theorist also has to have an interest-invariant account of evidence. It’s logically possible to have evidence be interest-relative, but knowledge be interest-neutral, but it is very hard to see how one would motivate such a position.</p>
<p>And now we run into a problem. Imagine a version of the Red-Blue game where the blue sentence is something that, if known, is part of the player’s evidence. If it is still irrational to play Blue-True, then any orthodox explanation that relies on evidence sensitive notions (like super-knowledge or evidential probability) will be in trouble. The aim of this subsection is to spell out why this is.</p>
<p>So let’s imagine a new player for the red-blue game. Call her Parveen. She is playing the game in a restaurant. It is near her apartment in Ann Arbor, Michigan. Just before the game starts, she notices an old friend, Rahul, across the room. Rahul is someone she knows well, and can ordinarily recognise, but she had no idea he was in town. She thought Rahul was living in Italy. Still, we would ordinarily say that she now knows Rahul is in town; indeed that he is in the restaurant. As evidence for this, note that it would be perfectly acceptable for her to say to someone else, “I saw Rahul here”. Now the game starts.</p>
<ul>
<li>The red sentence is <em>Two plus two equals four</em>.</li>
<li>The blue sentence is <em>Rahul is in this restaurant</em>.</li>
</ul>
<p>And here is the problem. On the one hand, there is only one rational play for Parveen: Red-True. She hasn’t seen Rahul in ages, and she thought he was in Italy. A glimpse of him across a crowded restaurant isn’t enough for her to think that <em>Rahul is in this restaurant</em> is as likely as <em>Two plus two equals four</em>. She might be wrong about Rahul, so she should take the sure money and play Red-True. So playing the red-blue game with these sentences makes it the case that Parveen doesn’t know where Rahul is. This is another case where knowledge is interest-relative, and at first glance it doesn’t look very different to the other cases we’ve seen.</p>
<p>But take a second look at the story for why Parveen doesn’t know where Rahul is. It can’t be just that her evidence makes it certain that two plus two equals four, but not certain that Rahul is in the restaurant. At least, it can’t be that unless it is not part of her evidence that Rahul is in the restaurant. And if evidence is not interest-relative, then I think we should say that it is part of Parveen’s evidence that Rahul is in the restaurant. This isn’t something she infers; it is a fact about the world she simply appreciated. Ordinarily, it is a starting point for her later deliberations, such as deliberations about whether to walk over to another part of the restaurant to say hi to Rahul. That is, ordinarily it is part of her evidence.</p>
<p>So the orthodox theorist has a challenge. If they say that it is part of Parveen’s evidence that Rahul is in the restaurant, then they can’t turn around and say that the evidential probability that he is in the restaurant is insufficiently high for her to play Blue-True. After all, it’s evidential probability is one. If they say that it is no part of Parveen’s evidence that Rahul is in the restaurant because she is playing this version of the Red-Blue game, they give up orthodoxy. So they have to say that our evidence never includes things like Rahul is in the restaurant.</p>
<p>This can be generalised. Take any proposition such that if the red sentence was that two plus two is four and that proposition was the content of the blue sentence, then it would be irrational to play Blue-True. Any orthodox explanation of the Red-Blue game entails that this proposition is no part of your evidence - whether you are playing the game or not. But once we strip all these propositions out of your evidence, you don’t have enough evidence to rationally believe, or even rationally make probable, very much at all.</p>
<p>Descartes, via very different means, walked into a version of this problem. And his answer was to (implicitly) take us to be infallible observers of our own minds, and (explicitly) offer a theistic explanation for how we can know about the external world given just this psychologistic evidence. Nowadays, most people think that’s wrong on both counts: we can be rationally uncertain about even our own minds, and there is no good path from purely psychological evidence to knowledge of the external world. The orthodox theorist ends up in a state worse than Cartesian scepticism.</p>
</div>
</div>
<div id="oddsandstakes" class="section level2">
<h2><span class="header-section-number">2.4</span> Odds and Stakes</h2>
<p>If orthodox views are wrong, then it is important to get clear on which heterodox view is most plausible.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> I’m defending a version of the pragmatic view. But it’s a different version to the most prominent versions defended in the literature. The difference can be most readily seen by looking at the class of cases that have motivated pragmatic views.</p>
<p>The cases involve a subject making a practical decision. The subject has a safe choice, which has a guaranteed return of <span class="math inline">\(S\)</span>. And they have a risky choice. If things go well, the return of the risky choice is <span class="math inline">\(S + G\)</span>, so they will gain <span class="math inline">\(G\)</span> from taking the risk. If things go badly, the return of the risky choice is <span class="math inline">\(S - L\)</span>, so they will lose <span class="math inline">\(L\)</span> from taking the risk. What it takes for things to go well is that a particular proposition <span class="math inline">\(p\)</span> is true. All of this is known by the subject facing the choice. What the subject doesn’t (uncontroversially) know is that they satisfy all the conditions for knowing <span class="math inline">\(p\)</span> that would have been endorsed by a well-informed epistemologist circa 1997. (That is, by a proponent of the traditional view.) So <span class="math inline">\(p\)</span> is true, and things won’t go badly for them if they take the risk. But still, in a lot of these cases, there is a strong intuition that they do not know that <span class="math inline">\(p\)</span>, and as I’ve just been arguing, that is hard to square with the idea that they know that <span class="math inline">\(p\)</span>. So assuming the traditional view is right about the subject as they were before facing the practical choice, having this choice in front of them causes them to lose knowledge that <span class="math inline">\(p\)</span>.</p>
<p>But what is it about these choices that triggers a loss of knowledge? There is a familiar answer to this, one explicitly endorsed by <span class="citation">Hawthorne (<a href="references.html#ref-Hawthorne2004">2004</a>)</span> and <span class="citation">Stanley (<a href="references.html#ref-Stanley2005">2005</a>)</span>. It is that they are facing a ‘high stakes’ choice. Now what it is for a choice to be high stakes is never made entirely clear, and <span class="citation">Anderson and Hawthorne (<a href="references.html#ref-AndersonHawthorne2019a">2019</a><a href="references.html#ref-AndersonHawthorne2019a">a</a>)</span> show that it is hard to provide an adequate definition in full generality. But in the simple cases described in the previous paragraph, it is easy enough to say what a high stakes case is. It just means that <span class="math inline">\(L\)</span> is large. So one gets the suggestion that practical factors kick in when faced with a case where there is a chance of a large loss.</p>
<p>This is not the view I defend. I think <span class="math inline">\(L\)</span> matters, but only indirectly. What is (typically) true in these cases is that the subject should maximise expected utility relative to what they know.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> And taking the risky choice maximises expected utility only if this equation is true.</p>
<p><span class="math display">\[
\frac{\Pr(p)}{1 - \Pr(p)} &gt; \frac{L}{G}
\]</span></p>
<p>The left hand side expresses the odds that <span class="math inline">\(p\)</span> is true. The right hand side expresses how high those odds have to be before the risk is worth taking. If the equation fails to hold, then the risk is not worth taking. And if risk is not worth taking, then the subject doesn’t know that <span class="math inline">\(p\)</span></p>
<p>Since the numerator of the right hand side is <span class="math inline">\(L\)</span>, then one way to destroy knowledge that <span class="math inline">\(p\)</span> is to present the subject with a situation where <span class="math inline">\(L\)</span> is very high. But it isn’t the only way. Since the denominator of the right hand side is <span class="math inline">\(G\)</span>, another way to destroy knowledge that <span class="math inline">\(p\)</span> is to present the subject with a situation where <span class="math inline">\(G\)</span> is very low.</p>
<p>In effect, we’ve seen such a situation with Anisa. But to make the parallel to Blaise’s case even clearer, consider Darja’s case. She has been reading books about World War One, and yesterday read that Franz Ferdinand was assassinated on St Vitus’s Day, June 28, 1914. She is now offered a chance to play a slightly unusual quiz game. She has to answer the question <em>What was the date of Franz Ferdinand’s assassination?</em> If she gets it right, she wins $50. If she gets it wrong, she wins nothing. Here’s what is strange about the game. She is allowed to Google the answer before answering. So here are the two live options for Darja. In the table, and in what follows, <span class="math inline">\(p\)</span> is the proposition that Franz Ferdinand was indeed assassinated on June 28, 1914.</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(p\)</span></td>
<td align="center"><span class="math inline">\(\neg p\)</span></td>
</tr>
<tr class="even">
<td>Say “June 28, 1914”</td>
<td align="center"><span class="math inline">\(50\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td>Google the answer</td>
<td align="center"><span class="math inline">\(50 - \varepsilon\)</span></td>
<td align="center"><span class="math inline">\(50 - \varepsilon\)</span></td>
</tr>
</tbody>
</table>
<p>If Darja has her phone near her, and has cheap easy access to Google, then <span class="math inline">\(\varepsilon\)</span> might be really low. And then she should take the safe option, unless she is incredibly sure that the book she read is reliable, and that she has precisely remembered it. In a lot of realistic cases, that won’t happen, and <span class="math inline">\(\Pr(p)\)</span> will be too low for her to take the risky option of saying “June 28, 1914”. She should take the safe option of Googling the answer. And that means she doesn’t know that <span class="math inline">\(p\)</span>, even if she remembers reading it in a book that is actually reliable. Facing a long odds bet can cause knowledge loss, even in low stakes situations.</p>
<p>I’m not the first to focus on these long odds/low stakes cases. Jessica <span class="citation">Brown (<a href="references.html#ref-Brown2008">2008</a>, 176)</span> notes that these cases raise problems for the stakes-centric version of IRT. And <span class="citation">Anderson and Hawthorne (<a href="references.html#ref-AndersonHawthorne2019a">2019</a><a href="references.html#ref-AndersonHawthorne2019a">a</a>)</span> argue that once we get beyond the simple two-state/two-option choices, it isn’t at all easy to say what situations are and are not high-stakes choices. I’ll return to their objection in chapter <a href="#objections"><strong>??</strong></a>, but for now I just want to note that the version of IRT I’m defending doesn’t give any special significance to high stakes choices. What makes knowledge hard is, to a first approximation, facing a long odds bet, not facing a high stakes bet.</p>
</div>
<div id="whatinterests" class="section level2">
<h2><span class="header-section-number">2.5</span> Theoretical Interests Matter</h2>
<p>When saying why I called my theory IRT, one of the reasons I gave was that I wanted theoretical, and not just practical, interests to matter to knowledge.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> This is also something of a break with the existing literature. After all, Jason Stanley’s book on interest-relative epistemology is called <em>Knowledge and Practical Interests</em>. And he defends a theory on which what an agent knows depends on the practical questions they face. But there are strong reasons to think that theoretical reasons matter as well.</p>
<p>In section <a href="interests.html#oddsandstakes">2.4</a>, I suggested that someone knows that <span class="math inline">\(p\)</span> only if the rational choice to make would also be rational given <span class="math inline">\(p\)</span>. That is, someone knows that <span class="math inline">\(p\)</span> only if the answer to the question <em>What should I do?</em> is the same unconditionally as it is conditional on <span class="math inline">\(p\)</span>. My preferred version of IRT generalises this approach. Someone knows that <span class="math inline">\(p\)</span> only if the rational answer to a question she is interested in is the same unconditionally as it is conditional on <span class="math inline">\(p\)</span>. Interests matter because they determine just what it is for the person to be interested in a question. If that’s how one thinks of IRT, the question of this section becomes, should we restrict questions the agent is interested in to just being questions about what choice to make? Or should they include questions that turn on her thoeretical interests, but which are irrelevant to choices before her? There are two primary motivations for allowing theoretical interests as well as practical interests to matter.</p>
<p>The first comes from the arguments for what Jeremy Fantl and Matthew McGrath call the Unity Thesis  <span class="citation">(Fantl and McGrath <a href="references.html#ref-FantlMcGrath2009">2009</a>, 73–76)</span>. They are interested in the thesis that whether or not <span class="math inline">\(p\)</span> is a reason for someone is independent of whether they are engaged in practical or theoretical deliberation. But one doesn’t have to be so invested in the ideology of reasons to appreciate their argument. Note that if only practical interests matter, then they know different things when considering the question <em>What to do in situation <em>S</em></em> in situation <em>S</em> and other situations. And if they know different things, those differential pieces of knowledge could lead to different answers. And that’s very unintuitive. After all, they might be deliberating about this question because situation <em>S</em> might arise, and they want to be practically ready for it.</p>
<p>Let’s make that a little less abstract. Imagine Anisa is not actually faced with the choice between Red-True, Blue-True, Red False and Blue-False with these particular red and blue sentences. In fact, she has no practical decision to make that turns on the date of the Battle of Agincourt. But she is idly musing over what she would do if she were playing that game. If she knows when the battle was, then she should be indifferent between Red-True and Blue-True. After all, she knows they will both win $50. But intuitively she should think Red-True is preferable, even in the abstract setting. And this seems to be the totally general case.</p>
<p>The general lesson is that if whether one can take <span class="math inline">\(p\)</span> for granted is relevant to the choice between A and B, it is similarly relevant to the theoretical question of whether one would choose A or B, given a choice. And since those questions should receive the same answer, if <span class="math inline">\(p\)</span> can’t be known while making the practical deliberation between A and B, it can’t be known while musing on whether A or B is more choiceworthy.</p>
<p>There is a second reason for including theoretical interests in what’s relevant to knowledge. There is something odd about the following reasoning: The probability of <em>p is precisely x</em>, therefore <span class="math inline">\(p\)</span>, in any case where <span class="math inline">\(x &lt; 1\)</span>. It is a little hard to say, though, why this is problematic, since we often take ourselves to know things on what we would admit, if pushed, are purely probabilistic grounds. The version of IRT that includes theoretical interests allows for this. If we are consciously thinking about whether the probability of <span class="math inline">\(p\)</span> is <em>x</em>, then that’s a relevant question to us. Conditional on <span class="math inline">\(p\)</span>, the answer to that question is clearly no, since conditional on <span class="math inline">\(p\)</span>, the probability of <span class="math inline">\(p\)</span> is 1. So anyone who is thinking about the precise probability of <span class="math inline">\(p\)</span>, and not thinking it is 1, is not in a position to know <span class="math inline">\(p\)</span>. And that’s why it is wrong, when thinking about <span class="math inline">\(p\)</span>’s probability, to infer <span class="math inline">\(p\)</span> from its high probability.</p>
<p>Putting the ideas so far together, we get the following picture of how interests matter. Someone knows that <span class="math inline">\(p\)</span> only if the evidential probability of <span class="math inline">\(p\)</span> is close enough to certainty for all the purposes that are relevant, given their theoretical and practical interests. Assuming the background theory of knowledge is non-sceptical, this will entail that interests matter.</p>
</div>
<div id="global" class="section level2">
<h2><span class="header-section-number">2.6</span> Global Interest Relativity</h2>
<p>IRT was introduced as a thesis about knowledge. I’m going to argue in chapter <a href="ratbel.html#ratbel">6</a> that it also extends to rational belief. Not every case where interests matter to knowledge generates a Dharmottara case. But we need not stop there. At the extreme, we could argue that every epistemologially interesting notion is interest-relative. Doing so gives us a global version of IRT. And that is what I’m going to defend here.</p>
<p>Jason <span class="citation">Stanley (<a href="references.html#ref-Stanley2005">2005</a>)</span> comes close to defending a global version. He notes that if one has both IRT, and a ‘knowledge first’ epistemology  <span class="citation">(Williamson <a href="references.html#ref-Williamson2000">2000</a>)</span>, then one is a long way to towards global IRT. Even if one doesn’t accept the whole knowledge first package, but just accepts the thesis that evidence is all and only what one knows, then one is a long way towards globalism. After all, if evidence is interest-relative, then probability, justification, rationality, and evidential support are interest-relative too.</p>
<p>That’s close to the path I’ll take to global IRT, but not exactly it. In chapter <a href="evidence.html#evidence">5</a> I’m going to argue that evidence is indeed interest-relative, and so all those other notions are interest-relative too. But the version of IRT I’ll put forward implies that evidence is a subset of knowledge.</p>
<p>There is a deep puzzle here for IRT that for a long time I couldn’t see a way out of. On the one hand, the arguments for IRT look like they will generalise to arguments for the interest-relativity of evidence. (This is something Tom Donaldson convinced me of in conversations some years back.) On the other hand, the explanation I want to offer of cases like Anisa’s presupposes that we can identify Anisa’s evidence independent of her interests. I want to say that Anisa shouldn’t play Blue-True because the evidential probability of the blue sentence being true is lower than the evidential probability of the red sentence being true. And she can’t know the blue sentence is true because she can’t play Blue-True. This turns into a nice story about when changes of interests lead to changes in belief if we can independently identify Anisa’s evidence, and hence the evidential probability of different propositions. But the story looks much less nice if interests also affect evidence.</p>
<p>The aim of chapter <a href="evidence.html#evidence">5</a> is to tell a story that avoids the worst of these problems. On the story I’ll tell, evidence is indeed interest-relative. And that means we can’t tell a simple story about precisely when changes in interests will lead to changes in knowledge. But it will still be true that people lose knowledge when the evidential probability of a proposition is no longer high enough for them to take it for granted with respect to every question they are interested in. And I will be able to say how interests impact evidence in a way that doesn’t require antecedently identifying how interests impact knowledge, so the story will still be somewhat reductive. But it won’t be as simple a story as I once believed in.</p>
</div>
<div id="neutrality" class="section level2">
<h2><span class="header-section-number">2.7</span> Neutrality</h2>
<p>This book defends, at some length, the idea that knowledge is interest-relative. But I’m staying neutral on a number of other topics in the vicinity.</p>
<p>Most notably, I’m not taking any stand on whether <em>contextualist</em> theories of knowledge are true or false. If you think that contextualism is true, then what I’m defending is that the view that ‘knowledge’ picks out in this context, and in most other contexts, is interest-relative.</p>
<p>Contextualist theories of knowledge have a lot in common with interest-relative theories. The kind of cases that motivate the interest-relative theories, cases like Anisa’s and Blaise’s, also motivate contextualism. They might even be seen as competitors, since they are offering rival explanations of similar phenomena. But they are not strictly inconsistent. Consider principles A and B below.</p>
<ol style="list-style-type: upper-alpha">
<li>A’s utterance that <em>B knows that p</em> is true only if for any question <em>Q?</em> in which A is interested, the rational answer for B to give is the same unconditionally as it is conditional on <span class="math inline">\(p\)</span>.</li>
<li>A’s utterance that <em>B knows that p</em> is true only if for any question <em>Q?</em> in which B is interested, the rational answer for B to give is the same unconditionally as it is conditional on <span class="math inline">\(p\)</span>.</li>
</ol>
<p>I endorse principle B, and that’s why I endorse an interest-relative theory of knowledge. If I endorsed principle A, then I would be (more or less) committed to a contextualist theory of knowledge. And principle A is not inconsistent with principle B.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>It isn’t hard to see why cases like Anisa and Blaise can move one to endorse principle A, and hence contextualism. It would be very odd for Anisa to say “This morning, I knew the Battle of Agincourt was in 1415.” That’s odd because she can’t now take it as given that the Battle of Agincourt was in 1415, and in some sense she wasn’t in any better or worse evidential position this morning with respect to the date of the battle. Perhaps, and this is the key point, it would even be false for Anisa to say this now. The contextualist, especially the contextualist who endorses principle A, has a good explanation for why that’s false. The interest-relative theorist doesn’t have anything to say about that. Personally I think it’s not obvious whether this would be false for Anisa to say, or merely inappropriate, and even if it is false, there may be decent explanations of this that are not contextualist. But there is clearly an argument for contextualism here. And it isn’t one that I’m going to endorse or reject.</p>
<p>As I’ve already noted, I’m making heavy use of the principle that Jessica Brown calls K-Suff. I’m going to defend that at much greater length in what follows. What I’m not defending is the converse of that principle, what she calls K-Nec.</p>
<dl>
<dt>K-Nec</dt>
<dd>An agent can properly use <span class="math inline">\(p\)</span> as a reason for action only if she knows that <span class="math inline">\(p\)</span>.
</dd>
</dl>
<p>The existing arguments for and against K-Nec are intricate and interesting, and I don’t have anything useful to add to them. All I will note is that the argument of this book doesn’t rely on K-Nec, and I’m just going to set it aside.</p>
<p>And I’m obviously not going to offer anything like a full theory of knowledge. I am defending a particular necessary condition on knowledge. That condition, plus some commonsensical claims about what we know in ordinary situations, entails that knowledge is interest-relative. And that’s just about as far as I’ll go.</p>
<p>I will be making one claim about how interests typically enter into the theory of knowledge. I’ll argue that there is a certain kind of defeater. A person only knows that <span class="math inline">\(p\)</span> if the belief that <span class="math inline">\(p\)</span> coheres in the right way with the rest of their attitudes. What’s ‘the right way’? That, I argue, is interest-relative. In particular, some kinds of incoherence are compatible with knowledge if the incoherence concerns questions that are not interesting.</p>
<p>So the impact of interests is (typically) very indirect. Even if the other conditions for knowledge are satisifed, someone might fail to know something because it doesn’t cohere well with the rest of their beliefs. But there is an exception to this exception clause. Incoherence with respect to uninteresting questions is compatible with knowledge.</p>
<p>This is going to matter because it affects how we think about what happen when interests change. It is odd to think that a change in interests could make one know something. But it isn’t as odd to think that a change in interests could block or defeat something that was potentially going to block or defeat an otherwise well supported belief from being knowledge. This is something I will return to repeatedly in chapter <a href="#changesobjection"><strong>??</strong></a>.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>See Lasonen-Aarnio <span class="citation">(<a href="references.html#ref-Lasonen-Aarnio2010b">2010</a>, <a href="references.html#ref-Lasonen-Aarnio2014">2014</a>)</span> for more details on her view. In <em>Normative Externalism</em>, I describe the difference between act-level and agent-level assessments as the difference between asking whether what Anisa does is rational, and whether Anisa’s action manifests wisdom <span class="citation">(Weatherson <a href="references.html#ref-Weatherson2019">2019</a>, 124–5)</span>. The best form of epistemicism, I’m suggesting, says that Anisa and Blaise are rational but unwise. This isn’t Lasonen-Aarnio’s terminology, but otherwise I’m just coopting her ideas.<a href="interests.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Strictly speaking, we need one more assumption - namely that for any unexpected way for the game to be, the probability of it being that way is independent of the truth of both the red and blue sentences. But it’s natural to assume that this is true.<a href="interests.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>This section is based on §3 of my <span class="citation">(<a href="references.html#ref-Weatherson2016">2016</a>)</span>.<a href="interests.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>This simplifies a little the relationship between rational choice and expected utility maximisation. Later in the book I’ll have to be much more careful about this relationship. See chapter <a href="ties.html#ties">7</a> for many more details.<a href="interests.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>This section is based on §4 of my <span class="citation">(<a href="references.html#ref-Weatherson2017-WEAII">2017</a>)</span>.<a href="interests.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>There is a technical difficulty in how to understand one person answering an infinitival question that another person is asking themselves. But the points I’m making in this section aren’t sensitive to this level of technical detail.<a href="interests.html#fnref6" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="prologue.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="belief.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Kahis.pdf", "Kahis.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
