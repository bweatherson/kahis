<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 7 Hard Choices | Knowledge</title>
  <meta name="description" content="A defence of an interest-relative theory of knowledge">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 7 Hard Choices | Knowledge" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://brian.weatherson.org/kahis/" />
  <meta property="og:image" content="https://brian.weatherson.org/kahis/tree.jpg" />
  <meta property="og:description" content="A defence of an interest-relative theory of knowledge" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Hard Choices | Knowledge" />
  
  <meta name="twitter:description" content="A defence of an interest-relative theory of knowledge" />
  <meta name="twitter:image" content="https://brian.weatherson.org/kahis/tree.jpg" />

<meta name="author" content="Brian Weatherson">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ratbel.html">
<link rel="next" href="preface.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="kahis.html">Knowledge: A Human Interest Story</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Front Matter</a></li>
<li class="chapter" data-level="1" data-path="prologue.html"><a href="prologue.html"><i class="fa fa-check"></i><b>1</b> Prologue</a></li>
<li class="chapter" data-level="2" data-path="interests.html"><a href="interests.html"><i class="fa fa-check"></i><b>2</b> Interests in Epistemology</a><ul>
<li class="chapter" data-level="2.1" data-path="interests.html"><a href="interests.html#redblue"><i class="fa fa-check"></i><b>2.1</b> Red or Blue?</a></li>
<li class="chapter" data-level="2.2" data-path="interests.html"><a href="interests.html#fourfamilies"><i class="fa fa-check"></i><b>2.2</b> Four Families</a></li>
<li class="chapter" data-level="2.3" data-path="interests.html"><a href="interests.html#orthodox"><i class="fa fa-check"></i><b>2.3</b> Against Orthodoxy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="interests.html"><a href="interests.html#orthodoxmoore"><i class="fa fa-check"></i><b>2.3.1</b> Moore’s Paradox</a></li>
<li class="chapter" data-level="2.3.2" data-path="interests.html"><a href="interests.html#superknow"><i class="fa fa-check"></i><b>2.3.2</b> Super Knowledge to the Rescue?</a></li>
<li class="chapter" data-level="2.3.3" data-path="interests.html"><a href="interests.html#probrescue"><i class="fa fa-check"></i><b>2.3.3</b> Rational Credences to the Rescue?</a></li>
<li class="chapter" data-level="2.3.4" data-path="interests.html"><a href="interests.html#orthodoxevidence"><i class="fa fa-check"></i><b>2.3.4</b> Evidential Probability</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="interests.html"><a href="interests.html#oddsandstakes"><i class="fa fa-check"></i><b>2.4</b> Odds and Stakes</a></li>
<li class="chapter" data-level="2.5" data-path="interests.html"><a href="interests.html#whatinterests"><i class="fa fa-check"></i><b>2.5</b> Theoretical Interests Matter</a></li>
<li class="chapter" data-level="2.6" data-path="interests.html"><a href="interests.html#global"><i class="fa fa-check"></i><b>2.6</b> Global Interest Relativity</a></li>
<li class="chapter" data-level="2.7" data-path="interests.html"><a href="interests.html#neutrality"><i class="fa fa-check"></i><b>2.7</b> Neutrality</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="belief.html"><a href="belief.html"><i class="fa fa-check"></i><b>3</b> Belief</a><ul>
<li class="chapter" data-level="3.1" data-path="belief.html"><a href="belief.html#beliefsinterests"><i class="fa fa-check"></i><b>3.1</b> Beliefs and Interests</a></li>
<li class="chapter" data-level="3.2" data-path="belief.html"><a href="belief.html#mapslegends"><i class="fa fa-check"></i><b>3.2</b> Maps and Legends</a></li>
<li class="chapter" data-level="3.3" data-path="belief.html"><a href="belief.html#given"><i class="fa fa-check"></i><b>3.3</b> Taking As Given</a></li>
<li class="chapter" data-level="3.4" data-path="belief.html"><a href="belief.html#block"><i class="fa fa-check"></i><b>3.4</b> Blocking Belief</a></li>
<li class="chapter" data-level="3.5" data-path="belief.html"><a href="belief.html#questions"><i class="fa fa-check"></i><b>3.5</b> Questions and Conditional Questions</a></li>
<li class="chapter" data-level="3.6" data-path="belief.html"><a href="belief.html#changes"><i class="fa fa-check"></i><b>3.6</b> A Million Dead End Streets</a><ul>
<li class="chapter" data-level="3.6.1" data-path="belief.html"><a href="belief.html#mecorrect"><i class="fa fa-check"></i><b>3.6.1</b> Correctness</a></li>
<li class="chapter" data-level="3.6.2" data-path="belief.html"><a href="belief.html#meimpractical"><i class="fa fa-check"></i><b>3.6.2</b> Impractical Propositions</a></li>
<li class="chapter" data-level="3.6.3" data-path="belief.html"><a href="belief.html#threeway"><i class="fa fa-check"></i><b>3.6.3</b> Choices with More Than Two Options</a></li>
<li class="chapter" data-level="3.6.4" data-path="belief.html"><a href="belief.html#meties"><i class="fa fa-check"></i><b>3.6.4</b> Hard Times and Close Calls</a></li>
<li class="chapter" data-level="3.6.5" data-path="belief.html"><a href="belief.html#modalupdate"><i class="fa fa-check"></i><b>3.6.5</b> Updates and Modals</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="belief.html"><a href="belief.html#nearby-views"><i class="fa fa-check"></i><b>3.7</b> Nearby Views</a><ul>
<li class="chapter" data-level="3.7.1" data-path="belief.html"><a href="belief.html#ganson"><i class="fa fa-check"></i><b>3.7.1</b> Ganson’s Theory</a></li>
<li class="chapter" data-level="3.7.2" data-path="belief.html"><a href="belief.html#usc"><i class="fa fa-check"></i><b>3.7.2</b> Ross and Schroeder’s Theory</a></li>
<li class="chapter" data-level="3.7.3" data-path="belief.html"><a href="belief.html#leitgeb"><i class="fa fa-check"></i><b>3.7.3</b> Leitgeb’s Stability Theory</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="belief.html"><a href="belief.html#weakbelief"><i class="fa fa-check"></i><b>3.8</b> Weak Belief</a></li>
<li class="chapter" data-level="3.9" data-path="belief.html"><a href="belief.html#probone"><i class="fa fa-check"></i><b>3.9</b> Belief as Probability One</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="knowledge.html"><a href="knowledge.html"><i class="fa fa-check"></i><b>4</b> Knowledge</a><ul>
<li class="chapter" data-level="4.1" data-path="knowledge.html"><a href="knowledge.html#structure"><i class="fa fa-check"></i><b>4.1</b> Knowledge and Practical Interests</a></li>
<li class="chapter" data-level="4.2" data-path="knowledge.html"><a href="knowledge.html#theoreticalknowledge"><i class="fa fa-check"></i><b>4.2</b> Theoretical Knowledge</a></li>
<li class="chapter" data-level="4.3" data-path="knowledge.html"><a href="knowledge.html#knowledge-and-closure"><i class="fa fa-check"></i><b>4.3</b> Knowledge and Closure</a><ul>
<li class="chapter" data-level="4.3.1" data-path="knowledge.html"><a href="knowledge.html#andelim"><i class="fa fa-check"></i><b>4.3.1</b> Single Premise Closure</a></li>
<li class="chapter" data-level="4.3.2" data-path="knowledge.html"><a href="knowledge.html#andintro"><i class="fa fa-check"></i><b>4.3.2</b> Multiple Premise Closure</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="knowledge.html"><a href="knowledge.html#closuresummary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="evidence.html"><a href="evidence.html"><i class="fa fa-check"></i><b>5</b> Evidence</a><ul>
<li class="chapter" data-level="5.1" data-path="evidence.html"><a href="evidence.html#evpuzzle"><i class="fa fa-check"></i><b>5.1</b> A Puzzle About Evidence</a></li>
<li class="chapter" data-level="5.2" data-path="evidence.html"><a href="evidence.html#simplesolution"><i class="fa fa-check"></i><b>5.2</b> A Simple, but Incomplete, Solution</a></li>
<li class="chapter" data-level="5.3" data-path="evidence.html"><a href="evidence.html#radicalinterpretation"><i class="fa fa-check"></i><b>5.3</b> The Radical Interpreter</a></li>
<li class="chapter" data-level="5.4" data-path="evidence.html"><a href="evidence.html#globalgame"><i class="fa fa-check"></i><b>5.4</b> Motivating Risk-Dominant Equilibria</a><ul>
<li class="chapter" data-level="5.4.1" data-path="evidence.html"><a href="evidence.html#cvdproof"><i class="fa fa-check"></i><b>5.4.1</b> The Dominance Argument for Risk-Dominant Equilibria</a></li>
<li class="chapter" data-level="5.4.2" data-path="evidence.html"><a href="evidence.html#perfectri"><i class="fa fa-check"></i><b>5.4.2</b> Making One Signal Precise</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="evidence.html"><a href="evidence.html#evsolution"><i class="fa fa-check"></i><b>5.5</b> Objections and Replies</a></li>
<li class="chapter" data-level="5.6" data-path="evidence.html"><a href="evidence.html#cutelim"><i class="fa fa-check"></i><b>5.6</b> Evidence, Knowledge and Cut-Elimination</a></li>
<li class="chapter" data-level="5.7" data-path="evidence.html"><a href="evidence.html#basic"><i class="fa fa-check"></i><b>5.7</b> Basic Knowledge and Non-Inferential Knowledge</a></li>
<li class="chapter" data-level="5.8" data-path="evidence.html"><a href="evidence.html#weakness"><i class="fa fa-check"></i><b>5.8</b> Epistemic Weakness</a></li>
<li class="chapter" data-level="5.9" data-path="evidence.html"><a href="evidence.html#neta"><i class="fa fa-check"></i><b>5.9</b> Holism and Defeaters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ratbel.html"><a href="ratbel.html"><i class="fa fa-check"></i><b>6</b> Rational Belief</a><ul>
<li class="chapter" data-level="6.1" data-path="ratbel.html"><a href="ratbel.html#atomism"><i class="fa fa-check"></i><b>6.1</b> Atomism about Rational Belief</a></li>
<li class="chapter" data-level="6.2" data-path="ratbel.html"><a href="ratbel.html#lockecoin"><i class="fa fa-check"></i><b>6.2</b> Coin Puzzles</a></li>
<li class="chapter" data-level="6.3" data-path="ratbel.html"><a href="ratbel.html#lockegames"><i class="fa fa-check"></i><b>6.3</b> Playing Games</a></li>
<li class="chapter" data-level="6.4" data-path="ratbel.html"><a href="ratbel.html#lockepuzzles"><i class="fa fa-check"></i><b>6.4</b> Puzzles for Lockeans</a><ul>
<li class="chapter" data-level="6.4.1" data-path="ratbel.html"><a href="ratbel.html#lockearb"><i class="fa fa-check"></i><b>6.4.1</b> Arbitrariness</a></li>
<li class="chapter" data-level="6.4.2" data-path="ratbel.html"><a href="ratbel.html#lockecorrect"><i class="fa fa-check"></i><b>6.4.2</b> Correctness</a></li>
<li class="chapter" data-level="6.4.3" data-path="ratbel.html"><a href="ratbel.html#lockemoore"><i class="fa fa-check"></i><b>6.4.3</b> Moorean Paradoxes</a></li>
<li class="chapter" data-level="6.4.4" data-path="ratbel.html"><a href="ratbel.html#closure"><i class="fa fa-check"></i><b>6.4.4</b> Closure and the Lockean Theory</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ratbel.html"><a href="ratbel.html#solving"><i class="fa fa-check"></i><b>6.5</b> Solving the Challenges</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ratbel.html"><a href="ratbel.html#coins"><i class="fa fa-check"></i><b>6.5.1</b> Coins</a></li>
<li class="chapter" data-level="6.5.2" data-path="ratbel.html"><a href="ratbel.html#games"><i class="fa fa-check"></i><b>6.5.2</b> Games</a></li>
<li class="chapter" data-level="6.5.3" data-path="ratbel.html"><a href="ratbel.html#arbitrariness"><i class="fa fa-check"></i><b>6.5.3</b> Arbitrariness</a></li>
<li class="chapter" data-level="6.5.4" data-path="ratbel.html"><a href="ratbel.html#moore"><i class="fa fa-check"></i><b>6.5.4</b> Moore</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ties.html"><a href="ties.html"><i class="fa fa-check"></i><b>7</b> Hard Choices</a><ul>
<li class="chapter" data-level="7.1" data-path="ties.html"><a href="ties.html#frankielee"><i class="fa fa-check"></i><b>7.1</b> An Example</a></li>
<li class="chapter" data-level="7.2" data-path="ties.html"><a href="ties.html#tiesresponse"><i class="fa fa-check"></i><b>7.2</b> Responding to the Challenge, Quickly</a></li>
<li class="chapter" data-level="7.3" data-path="ties.html"><a href="ties.html#backearth"><i class="fa fa-check"></i><b>7.3</b> Back to Earth</a></li>
<li class="chapter" data-level="7.4" data-path="ties.html"><a href="ties.html#supermarketquestions"><i class="fa fa-check"></i><b>7.4</b> I Have Questions</a></li>
<li class="chapter" data-level="7.5" data-path="ties.html"><a href="ties.html#satisfied"><i class="fa fa-check"></i><b>7.5</b> You’ll Never Be Satisfied (If You Try to Maximise)</a></li>
<li class="chapter" data-level="7.6" data-path="ties.html"><a href="ties.html#deliberationcosts"><i class="fa fa-check"></i><b>7.6</b> Deliberation Costs and Infinite Regresses</a></li>
<li class="chapter" data-level="7.7" data-path="ties.html"><a href="ties.html#ignorancebliss"><i class="fa fa-check"></i><b>7.7</b> Ignorance is Bliss</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>8</b> To Be Written</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Knowledge</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ties" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Hard Choices</h1>
<p>I’ve mentioned a couple of times that a natural version of IRT leads to unpleasant closure failures. Alex <span class="citation">Zweber (<a href="references.html#ref-Zweber2016">2016</a>)</span> and, separately, Charity Anderson and John Hawthorne <span class="citation">(<a href="references.html#ref-AndersonHawthorne2019a">2019</a><a href="references.html#ref-AndersonHawthorne2019a">a</a>)</span>, showed that the following principle cannot be the only way interests enter into our theory of knowledge.</p>
<dl>
<dt>Conditional Preferences</dt>
<dd>If S knows that <span class="math inline">\(p\)</span>, and is trying to decide between X and Y, then her preferences over X and Y are the same unconditionally as they are conditional on <span class="math inline">\(p\)</span>.
</dd>
</dl>
<p>They show that this adding this principle, and no other principle, to a natural theory of knowledge leads to an absurd situation where S knows <span class="math inline">\(p \wedge q\)</span> but does not know <span class="math inline">\(p\)</span>. I’ve mentioned a couple of times that my version of IRT does not fall into exactly this trap.</p>
<p>What I haven’t mentioned is that their objection to IRT doesn’t stop there. They go on to argue that the natural ways for versions of IRT to avoid this trap will lead to implausible amounts of scepticism. The point of this chapter, in effect, is to respond to this challenge.</p>
<p>There are a couple of reasons that I’m spending this much time on this objection. One is that thinking about it brings up interesting points about the relationship between analysing action in terms of reasons and knowledge, and analysing action in terms of utility maximisation. And another is that it turns out I have to concede a little more than I expected that I would. That is to say, the version of IRT I’m defending here is somewhat more sceptical than I would have realised were it not for thinking through this case. I don’t think this is enough reason to reject the theory; indeed, I think working through the cases gives us reason to accept the moderately sceptical conclusions. Still, this is a more concessive response than I’ll offer to subsequent objections, and the point here is not just to argue that a particular objection fails, but to see some consequences of the best form of IRT.</p>
<ul>
<li>Include summary of chapter once I, you know, write it.</li>
</ul>
<div id="frankielee" class="section level2">
<h2><span class="header-section-number">7.1</span> An Example</h2>
<p>Let’s start with an example from a great thinker. It will require a little exegesis, but that’s not unusual when using classic texts.</p>
<blockquote>
<div class="line-block">Well Frankie Lee and Judas Priest<br />
They were the best of friends<br />
So when Frankie Lee needed money one day<br />
Judas quickly pulled out a roll of tens<br />
And placed them on the footstool<br />
Just above the potted plain<br />
Saying &quot;Take your pick, Frankie boy,<br />
My loss will be your gain.&quot;<br />
          (“The Ballad of Frankie Lee and Judas Priest”, 1968. Lyrics from Bob <span class="citation">Dylan (<a href="references.html#ref-Dylan2016">2016</a>)</span> 225)</div>
</blockquote>
<p>On a common reading of this, Judas Priest isn’t just asking Frankie Lee how much money he wants to take, but which invididual notes. Let’s simplify, and say that it is common ground that Frankie should only take $10, so the choice Frankie Lee has is which of the individual notes he will take. This will be enough to set up the puzzle.</p>
<p>Assume something else that isn’t in the text, but which isn’t an implausible addition to the story. The world Frankie Lee and Judas Priest live in is not completely free of counterfeit notes. And it would be bad for Frankie Lee to take a counterfeit note. It won’t matter just how common these notes are, or how bad it would be. But our puzzle will be most vivid if each of these are relatively small quantities. So there aren’t that many counterfeit notes in circulation, and the (expected) disutility to Frankie Lee of having one of them is not great. There is some chance that he will get in trouble, but the chance isn’t high, and the trouble isn’t any worse than he’s suffered before. Still, other things exactly equal, Frankie Lee would prefer a genuine note to a counterfeit one.</p>
<p>Now for some terminology to help us state the problem Frankie Lee is in. Assume there are <span class="math inline">\(k\)</span> notes on the footstool. Call them <span class="math inline">\(n_1, \dots, n_k\)</span>. Let <span class="math inline">\(c_i\)</span> be the proposition that note <span class="math inline">\(n_i\)</span> is counterfeit, and its negation <span class="math inline">\(g_i\)</span> be that it is genuine. Let <span class="math inline">\(g\)</span>, without a subscript, be the conjunction <span class="math inline">\(g_1 \wedge \dots \wedge g_n\)</span>; i.e., the proposition that all the notes are genuine. Let <span class="math inline">\(t_i\)</span> be the act of taking note <span class="math inline">\(n_i\)</span>. Let <span class="math inline">\(U\)</span> be Frankie Lee’s utility function, and <span class="math inline">\(Cr\)</span> his credence function.</p>
<p>In our first version of the example, we’ll make two more assumptions. Apart from the issue of whether the note is real or counterfeit, Frankie Lee is indifferent between the notes, so for some <span class="math inline">\(h, l\)</span>, <span class="math inline">\(U(t_i | g_i) = h\)</span> and <span class="math inline">\(U(t_i | c_i) = l\)</span> for all <span class="math inline">\(i\)</span>, with of course <span class="math inline">\(h &gt; l\)</span>. And Frankie Lee thinks each of the banknotes is equally likely to be genuine, so for some <span class="math inline">\(p\)</span>, <span class="math inline">\(Cr(g_i) = p\)</span> for all <span class="math inline">\(i\)</span>. (And the probability of any of them being a counterfeit is independent of the probability of any of the others being counterfeit.)</p>
<p>That’s enough to get us three puzzles for the form of IRT that just uses <strong>Conditional Preferences</strong>. I’m going to refer to this form of IRT a lot, so let’s give it the memorable moniker IRT-CP.</p>
<p>First, Frankie Lee doesn’t know of any note that it is genuine. As things stand, Frankie is indifferent between <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> for any <span class="math inline">\(i, j\)</span>. But conditional on <span class="math inline">\(g_i\)</span>, Frankie prefers <span class="math inline">\(t_i\)</span> to <span class="math inline">\(t_j\)</span>. Right now, the expected utility of taking either <span class="math inline">\(i\)</span> or <span class="math inline">\(j\)</span> is <span class="math inline">\(ph + (1-p)l\)</span>. If Frankie Lee conditionalises on <span class="math inline">\(g_i\)</span>, then the utility of <span class="math inline">\(t_j\)</span> doesn’t change, but the utility of <span class="math inline">\(t_i\)</span> now becomes <span class="math inline">\(h\)</span>, and that’s higher than <span class="math inline">\(ph + (1-p)l\)</span>. Since IRT-CP says that knowledge is inconsistent with conditionalising on that ‘knowledge’ changing preferences over pragmatically salient options, and <span class="math inline">\(t_i\)</span> and <span class="math inline">\(t_j\)</span> are really salient to Frankie Lee, it follows that he doesn’t know <span class="math inline">\(g_i\)</span>. And <span class="math inline">\(i\)</span> was arbitrary in this proof, so he doesn’t know of any of the notes that they are genuine. That’s not very intuitive, but worse is to follow.</p>
<p>Second, Frankie Lee does know that all the notes are genuine, although he doesn’t know of any note that it is genuine. Conditional on <span class="math inline">\(g\)</span>, Frankie Lee’s preferences are the same as they are unconditionally. He used to be indifferent between the notes; after conditionalising he is still indifferent. So the one principle that IRT-CP adds to a standard theory of knowledge does not rule out that Frankie Lee knows <span class="math inline">\(g\)</span>. So he knows <span class="math inline">\(g\)</span>; but doesn’t know any of its constituent conjuncts. This is a very unappealing option.</p>
<p>Change the example a bit, so we can generate the third problem. Keep that the probabilities of each note being genuine are equal and independent. But assume that the notes are laid out in a line, and Frankie Lee is at one end of that line. So to get a note that is further away from him, he has to reach further. And this has an ever so small disutility. Let <span class="math inline">\(r_i\)</span> be the disutility of reaching for note <span class="math inline">\(i\)</span>. And assume this value increases as <span class="math inline">\(i\)</span> increases, but is always smaller than <span class="math inline">\((1-p)(h-l)\)</span>. That last quantity is important, because it is the difference between the utility of taking an arbitrary note (with no penalty for the cost of reaching for it), and the utility of taking a genuine banknote.</p>
<p>If all these assumptions are added, Frankie Lee knows one more thing. He knows <span class="math inline">\(g_1\)</span>. That’s because as things stand, he prefers <span class="math inline">\(t_1\)</span> to the other options. Conditional on <span class="math inline">\(g_i\)</span> for any <span class="math inline">\(i \geq 2\)</span>, he prefers <span class="math inline">\(t_i\)</span> to <span class="math inline">\(t_1\)</span>. But if <span class="math inline">\(i &gt; 2\)</span>, conditionalising on <span class="math inline">\(g_i\)</span> changes Frankie’s preferences, so he doesn’t know <span class="math inline">\(g_i\)</span>.</p>
<p>This third puzzle is striking for two reasons. One is that it involves a change of strict preferences. Unconditionally, Frankie strictly prefers <span class="math inline">\(t_1\)</span> to <span class="math inline">\(t_i\)</span>; conditional on <span class="math inline">\(g_i\)</span> he strictly prefers <span class="math inline">\(t_i\)</span> to <span class="math inline">\(t_1\)</span>. When I first saw these puzzles, I thought we could possibly get around them by restricting attention to cases where conditionalisation changes a strict preference. This example shows that way of rescuing IRT-CP won’t work. And the other reason is that it heightens the implausibility of the sceptical result that Frankie doesn’t know <span class="math inline">\(g_i\)</span>. It’s one thing to say that the weird situation that Judas Priest puts Frankie Lee makes Frankie Lee lose a lot of knowledge he ordinarily has. That’s just IRT in action. It’s another to say that within this very situation, Frankie Lee knows of some notes that they are genuine but does not know that others are genuine, even though his evidence for the genuineness of each note is the same.</p>
<p>I already discussed how my version of IRT handles this kind of puzzle back in section <a href="evidence.html#neta">5.9</a>. The complicated story I tell about evidence is no part of IRT-CP. By definition, IRT-CP just adds <strong>Conditional Preferences</strong> to a standard theory of knowledge. So the story I tell in section <a href="evidence.html#neta">5.9</a> is not available to the defender of IRT-CP. But it turns out to be more revealing to see how to rescue IRT-CP rather than jump to the distinctive version of IRT I defend. This let’s us see both what the options for defenders of IRT are and, hopefully, see some reasons for choosing the option that I defend.</p>
<p>So we have three puzzles to try to solve, if we want to defend anything like IRT-CP.</p>
<ol style="list-style-type: decimal">
<li>In the case where Frankie Lee has no reason to choose one note rather than another, he doesn’t know of any note that it is genuine. And this is surprisingly sceptical.</li>
<li>In the case where he has a weak reason to choose one note, he knows that note is genuine, but not the others. This retains the surprisingly sceptical consequence of the first puzzle, and adds a surprising asymmetry.</li>
<li>In both cases, there seems to be a really bad closure failure, with Frankie Leeknowing that all the notes are genuine, but not knowing of all or most individual notes that they are genuine.</li>
</ol>
<p>Before we leave Frankie Lee for a while, let’s note one variation on the case that somewhat helps IRT. Imagine that the country they are in has just reached the level of technological sophistication where it can produce plastic banknotes. And, as Frankie Lee knows, no one in the country has yet figured out how to produce forgeries of plastic banknotes that are even remotely plausible. Finally, assume that one of the notes, lucky <span class="math inline">\(n_8\)</span>, is one of the new plastic notes, while the others are the old paper notes. If Frankie Lee cares about counterfeit avoidance at all, he should take <span class="math inline">\(n_8\)</span>. And he should do so because it definitely isn’t a counterfeit, while each of the others might be. So in that case, Frankie Lee doesn’t know that the other notes are genuine, while he does know that <span class="math inline">\(n_8\)</span> is genuine.</p>
<p>Now we have a case where IRT-CP gives the right answers for the right reasons. A theory that disagrees with IRT-CP about this case has to either (a) deny this intuition that the uniquely rational choice for Frankie Lee is <span class="math inline">\(n_8\)</span>, or (b) say that Frankie Lee should choose <span class="math inline">\(n_8\)</span> because the other choices are too risky, even though he knows the risk in question will not eventuate. Neither option is particularly appealing, at least if one is unhappy with making Moore-paradoxical assertions, so this is a good case for IRT-CP.</p>
</div>
<div id="tiesresponse" class="section level2">
<h2><span class="header-section-number">7.2</span> Responding to the Challenge, Quickly</h2>
<p>What follows is going to get into the weeds a bit about how choices do and should get made in cases like Frankie Lee’s. So before we do that, I am going to outline how my version of IRT, which of course differs from IRT-CP, handles these cases.</p>
<p>First, it avoids the closure problem by stressing that what matters is not that the conditional and unconditional questions end up with the same verdict, but that the process of getting to that verdict is the same. This is why Frankie Lee doesn’t know <span class="math inline">\(g\)</span>, the claim that all the notes are genuine. Right now, when choosing a note, he should be indifferent because the risk that any note is counterfeit, given his evidence, is more or less the same as the risk that any other note is counterfeit. When he is choosing conditional on <span class="math inline">\(g\)</span>, he doesn’t have to attend to risks, or his evidence, or anything that might be more or less equal to anything else. He just takes it as fixed, for purposes of answering the question of what to choose conditional on <span class="math inline">\(g\)</span>, that the notes are genuine. He ends up in the same place both times, indifference between the notes, but he gets there via different pathways. That’s enough to defeat knowledge that <span class="math inline">\(g\)</span>.</p>
<p>I’m appealing again here to a point I first made back in section <a href="belief.html#block">3.4</a>. In English, saying that two questions are answered the same way is ambiguous. It might mean that we end up in the same place when answering the two questions. Or it might mean that we get to that place the same way. There are any number of examples of this. The questions <em>What is three plus two</em>, and <em>How many Platonic solids are there</em>, get answered the same way in the first sense, but not the second sense. <strong>Conditional Preference</strong> stresses that certain conditional and unconditional questions get answered the same way in this first sense. My version of IRT says that what matters is that these conditional and unconditional questions get answered the same way in the second sense.</p>
<p>That deals with the closure problem satisfactorily, but it makes the sceptical problems worse. To solve those problems we need to rethink our theory of decision. In a lot of cases like Frankie Lee’s, the rational thing to do is to <em>ignore</em> the possibility that the notes are counterfeit. This will sometimes lead to taking a sub-optimal choice. But given that choice has computational (and sometimes investigative) costs, it is worth risking a sub-optimal choice if the reward is a cheaper decision procedure. And in some cases like this, the best decision procedure will ignore certain risks.</p>
<p>If Frankie Lee ignores the risk that the notes are counterfeit, then the argument that he doesn’t know <span class="math inline">\(g_1, g_2,\)</span> etc., doesn’t get off the ground. Given that he’s ignoring the risk that the notes are counterfeit, conditionalising on them not being counterfeit changes precisely nothing. So there is no pragmatic argument that he does not know they are genuine.</p>
<p>This approach will avoid the sceptical problems if, but only if, this kind of ‘ignoring’ is rational and widespread. I aim to make a case that it is. But first I want to make things if anything worse for IRT, by stressing how quotidian examples with the structure of Frankie Lee’s are. This will prevent me from being able to dismiss the example as a theorist’s fantasy, but will ultimately help see why ignoring in this sense is so prevalent, so important, and even so rational.</p>
</div>
<div id="backearth" class="section level2">
<h2><span class="header-section-number">7.3</span> Back to Earth</h2>
<p>The Frankie Lee and Judas Priest case is weird. Who offers someone money, then asks them to pick which note to take? And intuitions about such weird cases cases are sometimes deprecated. Perhaps the contrivance doesn’t reveal deep problems with a philosophical theory, but merely a quirk of our intuitions. I am not going to take a stand on any big questions about the epistemology of intuitions here. Rather, I’m going to note that cases with the same structure as the story of Frankie Lee and Judas Priest are incredibly common in the real world. Thinking about the real world examples can show us how pressing the problems are, and as I just hinted, eventually show us a way out of those problems.</p>
<p>So let’s leave Frankie Lee for now, just above the potted plain, and think about a new character. We will call this one David, and he is buying a few groceries on the way home from work. In particular, he has to buy a can of chickpeas, a bottle of milk, and a carton of eggs. To make life easy, we’ll assume each of these cost the same amount - $5.<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a> None of these purchases is entirely risk free. Canned goods are pretty safe, but sometimes they go bad. Milk is normally removed from sale when it goes sour, but not always. And eggs can crack, either in transit or just on the shelf. In David’s world, just like ours, each of these risks is greater than the one that came before.</p>
<p>David has a favorite brand of chickpeas, of milk, and of eggs. And he knows where in the store they are located. So his shopping is pretty easy. But it isn’t completely straightforward. First he gets the chickpeas. And that’s simple; he grabs the nearest can, and unless it is badly dented, or leaking, he puts in in his basket. Next he goes onto the milk. The milk bottles have sell-by dates printed in big letters on the front. And David checks that he isn’t picking up one that is about to expire. His store has been known to have adjacent bottles of milk with sell-by dates 10 days apart, so it’s worth checking. But as long as the date is far enough in the future, he takes it and moves on. Finally, he comes to the eggs. (Nothing so alike as eggs, he always thinks to himself.) Here he has to do a little more work. He takes the first carton, opens it to see there are no cracks on the top of the eggs, and, finding none, puts that in his basket too. He knows some of his friends do more than this; flipping the carton over to check for cracks underneath. But the one time he tried that, the eggs ended up on the floor. And he knows some of his friends do less; just picking up the carton by the underside, and only checking for cracks if the underside is sticky where the eggs have leaked. He thinks that makes sense too, but he is a little paranoid, and likes visual confirmation of what he’s getting. All done, he heads to the checkout, pays his $15, and goes home.</p>
<p>The choice David faces when getting the chickpeas is like the choice Frankie Lee faces. In a normal store, it will be more like the version where Frankie Lee has to reach further for some notes than others, but sometimes there will be multiple cans equidistant from David. More normally thought, some of the cans will be towards the front, and others towards the back, and it will be easier to grab one of the ones from the front. That’s why it is weird to get one from the back; reaching incurs costs without any particular payoff.</p>
<p>But in a deeper sense, in all three cases, the choice David faces is something like the choice Frankie Lee faced. He has to choose from among a bunch of very similar seeming options. In at least the chickpeas example, there is something you’d want to say that he knows: canned goods sold at reputable stores are safe. But the arguments above seem to show that David does not know this, at least if IRT-CP is true. Indeed, the argument so far relies just on <strong>Conditional Preferences</strong> being part of the correct theory of IRT, and that’s a much weaker claim than that IRT-CP is true. Assuming there is some probability of the chickpeas not being safe, and the costs of reaching for some other can are low enough, David is in exactly the same situation as Frankie Lee. Right now, he maximises utility by taking the front-most can. But conditional on one of the other cans being safe, he maximises utility by taking it. So he does not know of any of the other cans that they are safe.</p>
<p>Frankie Lee’s situation is weird. Who lays out some ten dollar bills and asks you to pick one? (Judas Priest, I guess.) But David’s situation is not weird. Looking at a fully stocked shelf of industrially produced food, and needing to pick one can out of an array of similar items, is a very common experience. If a theory of knowledge yields bizarre verdicts about a case like this, it is no defence at all to say the situation is too obscure. In this modern world, it’s an everyday occurrence.</p>
</div>
<div id="supermarketquestions" class="section level2">
<h2><span class="header-section-number">7.4</span> I Have Questions</h2>
<p>So far in this chapter I’ve assumed that these two questions are equivalent:</p>
<ol style="list-style-type: decimal">
<li>Which option has highest expected utility?</li>
<li>What to do?</li>
</ol>
<p>In doing this, I’ve faithfully reproduced the arguments of some critics of IRT. But those critics were hardly being unfair to proponents of IRT in treating these questions alike. They are explicitly treated as being interchangable in, for example, my <span class="citation">(<a href="references.html#ref-Weatherson2005">2005</a><a href="references.html#ref-Weatherson2005">b</a>)</span>. But this was a mistake I made in defending IRT, and the beginning of a solution to the problems raised by Frankie Lee is to separate the questions out. I already mentioned one respect in which these questions differ back in section <a href="belief.html#questions">3.5</a>. I’ll rehearse that difference, briefly mention a second difference, then spend some time on a third difference.</p>
<p>The point I made much of back in section <a href="belief.html#questions">3.5</a> was that someone might know the utility facts, but not know what to do. When Frankie sits down, with his fingers to his chin, and tries to decide which of the tens to take, it’s possible he knows that they each have the same utility. But he still has to pick one, and with his head spinning he can’t decide which one to take. In cases like these answering questions about utility comparisons won’t settle questions about what to do.<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a></p>
<p>A second reason for not treating the questions alike is that to treat them alike assumes away something that should not be assumed away. It simply assumes that risk-sensitive theories of choice, as defended by <span class="citation">Quiggin (<a href="references.html#ref-Quiggin1982">1982</a>)</span> and <span class="citation">Buchak (<a href="references.html#ref-BuchakRisk">2013</a>)</span>, are mistaken. We probably shouldn’t simply assume that. It turns out the difference between expected utility theory and these heterodox alternatives isn’t particularly relevant to Frankie’s or David’s choices, so I’ll leave this aside for the rest of the chapter.</p>
<p>The third way in which the equivalence is wrong takes a little longer to set up. The short version is that rational people are satisficers, and for a satisficer you can answer the question <em>What to do</em> without taking a stand on questions about relative utility. The longer version is set out in the next section.</p>
</div>
<div id="satisfied" class="section level2">
<h2><span class="header-section-number">7.5</span> You’ll Never Be Satisfied (If You Try to Maximise)</h2>
<p>The standard model of practical rationality that we use in philosophy is that of expected utility maximisation. But there are both theoretical and experimental reasons to think that this is not the right model for choices such as that faced by Frankie or David. Maximising expected utility is resource intensive, especially in contexts like a modern supermarket, and the returns on this resource expenditure are unimpressive. What people mostly do, and what they should do, is choose in a way that is sensitive to the costs of adopting one or other way.</p>
<p>There are two annoying terminological issues around here that I mostly want to set aside, but need to briefly address in order to forestall confusion.</p>
<p>I’m going to assume maximising expected utility means taking the option with the highest expected utility given facts that are readily available. So if one simply doesn’t process a relevant but observationally obvious fact, that can lead to an irrational choice. I might alternatively have said that the choice was rational (given the facts the chooser was aware of), but the observational process was irrational. But I suspect that terminology would just add needless complication. I’m going to come back to another point that is partially terminological, and partially substantive. That’s whether we should identify the choice consequentialists recommend in virtue of the fact that it maximises expected utility with one of the options (in the ordinary sense of option), or something antecedent.</p>
<p>And I’m going to call any search procedure that is sensitive to resource considerations a satisficing procedure. This isn’t an uncommon usage. Charles <span class="citation">Manski (<a href="references.html#ref-Manski2017">2017</a>)</span> uses the term this way, and notes that it has rarely been defined more precisely than that. But it isn’t the only way that it is used. Mauro <span class="citation">Papi (<a href="references.html#ref-Papi2013">2013</a>)</span> uses the term to exclusively mean that the chooser has a ‘reservation level’, and they choose the first option that crosses it. This kind of meaning will be something that becomes important again in a bit. And Chris <span class="citation">Tucker (<a href="references.html#ref-Tucker2016">2016</a>)</span>, following a long tradition in philosophy of religion, uses it to mean any choice procedure that does not optimize. Elena Reutskaja et al <span class="citation">(<a href="references.html#ref-Reutskaja2011">2011</a>)</span> contrast a ‘hybrid’ model that is sensitive to resource constraints with a ‘satisficing’ model that has a fixed reservation level. They end up offering reasons to think ordinary people do (and perhaps should) adopt this hybrid model. So though they don’t call this a satisficing approach, it just is a version of what Manski calls satisficing. Andrew Caplin et al (<span class="citation">(<a href="references.html#ref-Caplin2011">2011</a>)</span>), on the other hand, describe a very similar model to Reutskaja et al’s hybrid model - one where agents try to find something above a reservation level but the reservation level is sensitive to search costs - as a form of satisficing. So the terminology around here is a mess. I propose to use Manski’s terminology: agents satisfice if they choose in a way that is sensitive to resource constraints. Ideally they would maximise, subject to constraints, but saying just what this comes to runs into obvious regress problems <span class="citation">(Savage <a href="references.html#ref-Savage1967">1967</a>)</span>. Let’s set aside this theoretical point for a little, and go back to David and the chickpeas.</p>
<p>When David is facing the shelf of chickpeas, he can rationally take any one of them - apart perhaps from ones that are seriously damaged. How can expected utility theory capture that fact? It says that more than one choice is permissible only if the choices are equal in expected utility. So the different cans are equal in expected utility. But on reflection, this is an implausible claim. Some of the cans are ever so slightly easier to reach. Some of the cans will have ever so slight damage - a tiny dint here, a small tear in the label there - that just might indicate a more serious flaw. Of course, these small damages are almost always irrelevant, but as long as the probability that they indicate damage is positive, it breaks the equality of the expected utility of the cans. Even if there is no visible damage, some of the labels will be ever so slightly more faded, which indicates that the cans are older, which ever so slightly increases the probability that the goods will go bad before David gets to use them. Of course in reality this won’t matter more than one time in a million, but one in a million chances matter if you are asking whether two expected utilities are strictly equal.</p>
<p>The common thread to the last paragraph is that these objects on the shelves are almost duplicates, but the most careful quality control doesn’t produce consumer goods that are actual duplicates. This is particularly true in Frankie Lee’s choice situation. If all the notes he looks at are really duplicates, down to the serial numbers, he should run away. There are always some differences. It is unlikely that these differences make precisely zero difference to the expected utility of each choice. And even if they do, discovering that is hard work.</p>
<p>So it seems likely that, according to the expected utility model, it isn’t true that David could permissibly take any can of chickpeas that is easily reachable and not obviously flawed. Even if that is true, it is extremely unlikely that David could know it to be true. But one thing we know about situations like David’s is that any one of the (easily reached, not clearly flawed) cans can be permissibly chosen, and David can easily know that. So the expected utility model, as I’ve so far described it, is false.</p>
<p>I’ll return in the next section to the question of whether this is a problem for theories of decision based around expected utility maximisation broadly, or whether it is just a problem for the particular way I’ve spelled out the expected utility theory. But for now I want to run through two more arguments against the idea that supermarket shoppers like David should be maximising expected utility (so understood).</p>
<p>In all but a vanishingly small class of cases, the different cans will not have the same expected utility. But figuring out which can has the highest expected utility is a going to be work. It’s possible in principle, I suppose, that someone could be skilled at it, in the sense that they could instinctively pick out the can whose shape, label fading, etc., reveal it to have the highest expected utility. Such a skill seems likely to be rare - though I’ll come back to this point below when considering some other skills that are probably less rare. For most people, maximising expected utility will not be something that can be done through skill alone; it will take effort. And this effort will be costly, and almost certainly not worth it. Although one of the cans will be ever so fractionally higher in expected utility than the others, the cost of finding out which can this is will be greater than the difference in expected utility of the cans. So aiming to maximise expected utility will have the perverse effect of reducing one’s overall utility, in a predictable way.</p>
<p>The costs of trying to maximise expected utility go beyond the costs of engaging in search and computation. There is evidence that people who employ maximising strategies in consumer search end up worse off than those who don’t. <span class="citation">Schwartz (<a href="references.html#ref-SchwartzEtAl2002">2002</a>)</span> reported that consumers could be divided in ‘satisficers’ and ‘maximizers’. And once this division is made, it turns out that the maximizers are less happy with individual choices, and with their life in general. This finding has been extended to work on career choice (<span class="citation">Schwartz (<a href="references.html#ref-IyengarEtAl2006">2006</a>)</span>), where the maximisers end up with higher salaries but less job satisfaction, and to friend choice (<span class="citation">Newman et al. (<a href="references.html#ref-NewmanEtAl2018">2018</a>)</span>), where again the maximizers seem to end up less satisfied.</p>
<p>There is evidence here that maximizing is bad at what it sets out to achieve. But there are both empirical and theoretical reasons to be cautious about accepting these results at face value. Whether maximizers are worse off seems to be tied up to the ‘paradox of choice’ (<span class="citation">Schwartz (<a href="references.html#ref-Schwartz2004">2004</a>)</span>), the idea that sometimes giving people even more choices makes them less happy with their outcome, because they are more prone to regret. But it is unclear whether such a paradox exists. One meta-analysis (<span class="citation">Todd (<a href="references.html#ref-ScheibehenneEtAl2010">2010</a>)</span>) did not show the effect existing at all, though a later meta-analysis finds a significant mediated effect (<span class="citation">Goodman (<a href="references.html#ref-ChernevEtAl2015">2015</a>)</span>). But it could also be that the result is a feature of an idiosyncratic way of carving up the maximizers from the satisficers. Another way of dividing them up produces no effect at all (<span class="citation">Diab (<a href="references.html#ref-DiabEtAl2008">2008</a>)</span>).</p>
<p>The theoretical reasons relate to Newcomb’s problem. Even if we knew that maximizers were less satisfied with how things are going than satisficers, it isn’t obvious that any one person would be better off switching. They might be like a two-boxer who would get nothing if they took one-box. There is a little evidence in <span class="citation">Schwartz (<a href="references.html#ref-IyengarEtAl2006">2006</a>)</span> that this isn’t quite what is happening, but the overall situation is unclear.</p>
<p>The upshot of all this, I think, is that there are potentially two kinds of cost of engaging in certain kind of search and choice procedures. Some procedures are more costly to implement than others: they take more time, or more energy, or even more money. But further, some procedures might have a hedonic cost that extends beyond the time that the procedure is implemented. There is no theoretical or empirical guarantee that choosing widget W by procedure P1 will produce the same amount of happiness as choosing widget W by procedure P2. And especially for choices that are intended to produce happiness, this kind of factor should matter as well. In short, there are many more ways to assess a consumer choice procedure than the quality of the products in ends up choosing. And this will be the key to our resolution of the puzzles about closure.</p>
</div>
<div id="deliberationcosts" class="section level2">
<h2><span class="header-section-number">7.6</span> Deliberation Costs and Infinite Regresses</h2>
<p>The picture I’ve been building towards in the last section is not a new one. As I mentioned, the experimental work in <span class="citation">(<a href="references.html#ref-Reutskaja2011">2011</a>)</span> suggests this is how people do reason. But the idea that people should reason this way goes back much further. It is often traced back to a footnote of Knight’s. Here is the text that provides the context for the note.</p>
<blockquote>
<p>Let us take Marshall’s example of a boy gathering and eating berries … We can hardly suppose that the boy goes through such mental operations as drawing curves or making estimates of utility and disutility scales. What he does, in so far as he deliberates between the alternatives at all*, is to consider together with reference to successive amounts of his “commodity,” the utility of each increment against its “cost in effort,” and evaluate the net result as either positive or negative (<span class="citation">Knight (<a href="references.html#ref-Knight1921">1921</a>)</span> 66-7)</p>
</blockquote>
<p>And the footnote attached to ‘at all’ says this</p>
<blockquote>
<p>Which, to be sure, is not very far. Nor is this any criticism of the boy. Quite the contrary! It is evident that the rational thing to do is to be irrational, where deliberation and estimation cost more than they are worth. That this is very often true, and that men still oftener (perhaps) behave as if it were, does not vitiate economic reasoning to the extent that might be supposed. For these irrationalities (whether rational or irrational!) tend to offset each other. (<span class="citation">Knight (<a href="references.html#ref-Knight1921">1921</a>)</span> 67fn1)</p>
</blockquote>
<p>Knight doesn’t really give an argument for the claim that these effects will offset. And as John <span class="citation">Conlisk (<a href="references.html#ref-Conlisk1996">1996</a>)</span> shows in his fantastic survey of the late 20th century literature on bounded rationality, it very often isn’t true. Especially in game theoretic contexts, the thought that other players might think that “deliberation and estimation cost more than they are worth” can have striking consequences. But our aim here is not to think about economic theorising, but about the nature of rationality.</p>
<p>There is something paradoxical, almost incoherent, about Knight’s formulation. If it is “rational to be irrational”, then being “irrational” can’t really be irrational. There are two natural ways to get out of this paradox. One, loosely following David <span class="citation">Christensen (<a href="references.html#ref-Christensen2007">2007</a>)</span>, would be to say that “Murphy’s Law” applies here. Whatever one does will be irrational in some sense. But still some actions are less irrational than others, and the least irrational will be to decline to engage in deliberation that costs more than it is worth. I suspect what Knight had in mind though was something different (if not obviously better). He is using ‘rational’ as more or less a rigid designator of the the property of choosing as a Marshallian maximiser does. And what he means here is that the disposition to not choose in that way will be, in the long run, the disposition with maximal returns.</p>
<p>This latter idea is what motivates the thought that rational agents will take what Conlisk calls “deliberation costs” into account. And Conlisk thinks that this is what rational agents will do. But he also raises a problem for this view, and indeed offers one of the clearest (and most widely cited) statements of this problem.</p>
<blockquote>
<p>However, we quickly collide with a perplexing obstacle. Suppose that we first formulate a decision problem as a conventional optimization based on the assumption of unbounded rationality and thus on the assumption of zero deliberation cost. Suppose we then recognize that deliberation cost is positive; so we fold this further cost into the original problem. The difficulty is that the augmented optimization problem will itself be costly to analyze; and this new deliberation cost will be neglected. We can then formulate a third problem which includes the cost of solving the second, and then a fourth problem, and so on. We quickly find ourselves in an infinite and seemingly intractable regress. In rough notation, let <span class="math inline">\(P\)</span> denote the initial problem, and let <span class="math inline">\(F(.)\)</span> denote the operation of folding deliberation cost into a problem. Then the regress of problems is <span class="math inline">\(P, F(P), F^2(P), \ldots\)</span> (<span class="citation">Conlisk (<a href="references.html#ref-Conlisk1996">1996</a>)</span> 687)</p>
</blockquote>
<p>Conlisk’s own solution to this problem is not particularly satisfying. He notes that once we get to <span class="math inline">\(F^3\)</span> and <span class="math inline">\(F^4\)</span>, the problems are ‘overly convoluted’ and seem to be safely ignored. This isn’t enough for two reasons. First, even a problem that is convoluted to state can have serious consequences when we think about solving it. (What would <em>Econometrica</em> publish if this weren’t true?) Second, as is often noted, <span class="math inline">\(F^2(P)\)</span> might be a harder problem to solve than <span class="math inline">\(P\)</span>, so simply stopping the regress there and treating the rational agent as solving this problem seems to be an unmotivated choice.</p>
<p>As Conlisk notes, this problem has a long history, and is often used to dismiss the idea that folding deliberation costs into our model of the optimising agent is a good idea. I use ‘dismiss’ advisedly here. As he also notes, there is very little <em>discussion</em> of this infinite regress problem in the literature before 1996. The same remains true after 1996. What is done is that instead people appeal to the regress in a sentence or two to set aside approaches that incorporate deliberation cost in the way that Conlisk suggests.</p>
<p>Up to around the time of Conlisk’s article, the infinite regress problem was often appealed to by people arguing that we should, in effect, ignore deliberation costs. After his article, the appeals to the regress comes from a different direction. It is usually from theorists arguing that deliberation costs are real, but the regress means it will be impossible to consistently incorporate them into a model of an optimizing agent. So we should instead rely on experimental techniques to see how people actually handle deliberation costs; the theory of optimization has reached its limit. This kind of move is found in writers as diverse as <span class="citation">Selten (<a href="references.html#ref-GigerenzerSelton2001">2001</a>)</span>, <span class="citation">Odell (<a href="references.html#ref-Odell2002">2002</a>)</span>, <span class="citation">Pingle (<a href="references.html#ref-Pingle2006">2006</a>)</span>, <span class="citation">Slack (<a href="references.html#ref-ManganEtAl2010">2010</a>)</span>, <span class="citation">Tanaka (<a href="references.html#ref-OgakiTanaka2017">2017</a>)</span> and <span class="citation">Chakravarti (<a href="references.html#ref-Chakravarti2017">2017</a>)</span>. And proponents of taking deliberation costs seriously within broadly optimizing approaches, like Miles <span class="citation">Kimball (<a href="references.html#ref-Kimball2015">2015</a>)</span>, say that solving the regress problem is the biggest barrier to having such an approach taken seriously by economists.</p>
<p>And it really matters for the story of this book that there is a solution to the infinite regress problem within a broadly optimizing framework. More precisely, we need a solution to the regress problem that does not defeat knowledge. At least some of the time, the fact that a belief was formed by a rationally problematic procedure means that the belief is not a piece of knowledge. As we might say, the irrationality of the procedure is a defeater of the claim to knowledge. What I want to respond is that if the procedure is optimal, that defeats (or perhaps overrides) this defeater. ‘Optimal’ here doesn’t mean rationally optimal; it means optimal given the computational limitations on the agent. But now I’ve said enough to suggest that the regress problem will arise.</p>
<p>Here’s how I plan to solve the regress problem. What matters for optimality is that the thinker is following the procedure that is the optimal solution to <span class="math inline">\(F(P)\)</span>. It doesn’t matter that they compute that it is the optimal solution, or even that they are following it because it is the optimal solution. Because there is this external, success oriented condition, there isn’t any extra problem that arises by ‘folding in’ computational costs. In the terminology from Conlisk, <span class="math inline">\(F^n(P) = F(P)\)</span>, for all <span class="math inline">\(n &gt; 0\)</span>.</p>
<p>The solution to the regress problem is easy to state, but a little harder to motivate. There are two big questions to answer before we can say it is really motivated.</p>
<ol style="list-style-type: decimal">
<li>Why should we allow this kind of unreflective rule-following in our solution to the regress?</li>
<li>Why should we think that <span class="math inline">\(F(P)\)</span> is the point where this consideration kicks in, as opposed to <span class="math inline">\(P\)</span>, or anything else?</li>
</ol>
<p>There are a few ways to answer 1. One motivation traces back to the work by the artificial intelligence researcher Stuart <span class="citation">Russell (<a href="references.html#ref-Russell1997">1997</a>)</span>. (Although really it starts with the philosophers Russell cites as inspiration, such as <span class="citation">Cherniak (<a href="references.html#ref-Cherniak1986">1986</a>)</span> and <span class="citation">Harman (<a href="references.html#ref-Harman1973">1973</a>)</span>.) He stresses that we should think about the problem from the outside, as it were, not from inside the agent’s perspective. How would we program a machine that we knew would have to face the world with various limitations? We will give it rules to follow, but we won’t necessarily give it the desire (or even the capacity) to follow those rules self-consciously. That might be useful some of the time - though really what’s more useful is knowing the limitations of the rules. And that can be done without following the rules as such. It just requires good dispositions to complicate the rules one is following in cases where such complication will be justified.</p>
<p>Another motivation is right there in the quote from Knight that set this literature going. Most writers quote the footnote, where Knight suggests it might be rational to be irrational. But look back at what he’s saying in the text. The point is that it can be perfectly rational to use considerations other than drawing curves and making utility scales. What one has to do is follow internal rules that (non-accidentally) track what one would do if one was a self-consciously perfect Marshallian agent. That’s what I’m saying too, though I’m saying it one level up.</p>
<p>Finally, there is the simple point that on pain of regress any set of rules whatsoever must say that there are some rules that are simply followed. This is one of the less controversial conclusions of the debates about rule-following that were started by <span class="citation">Wittgenstein (<a href="references.html#ref-Wittgenstein1953">1953</a>)</span>. That we must at some stage simply follow rules, not follow them in virtue of following another rule, say the rule to compute how to follow the first rule and act accordingly, is an inevitable consequence of thinking that finite creatures can be rule followers.</p>
<p>So question 1 is not really a big problem. But question 2 is more serious. Why <span class="math inline">\(F(P)\)</span>, and why not something else? The short answer will be that any reason to think that rational actors maximize <em>expected</em> utility, as opposed to actual utility, will also be a reason to think that they solve <span class="math inline">\(F(P)\)</span> and not <span class="math inline">\(P\)</span>. The longer answer is a bit more roundabout, but it helps us to see what a solution to <span class="math inline">\(F(P)\)</span> will look like.</p>
<p>Start by stepping back and thinking about why we cared about <em>expected</em> utility in the first place. Why not just say that the best thing to do is to produce the best outcome, and be done with it? Well, we don’t say that because we take it as a fixed point of our inquiry that agents are informationally limited, and that the best thing to do is what is best given that limitation. Given some plausible assumptions, the best thing for the informationally limited agent to do would be to maximise expected utility. This is a second-best option, but the best is unavailable given the limitations that we are treating as unavoidable.</p>
<p>But agents are not just informationally limited, they are computationally limited too. And we could treat that as the core limitation to be modelled. As Conlisk says, it is “entertaining to imagine” theorists who worked in just this way <span class="citation">(Conlisk <a href="references.html#ref-Conlisk1996">1996</a>, 691)</span>. Let’s imagine that when we meet the Martian economists, that’s how they reason. Conlisk notes a few things that the Martian economists might do. They might disparage their colleagues who take informational limitations seriously as introducing ad hoc stipulations into theory. They might argue that informational limitations are bound to cancel out, or be eliminated by competition. They might argue that apparent informational limitations are really just computational ones, or at least can be modelled as computational ones. And so on.</p>
<p>What he doesn’t add is that they might suggest that there is a regress worry for any attempt to add informational constraints. Let <span class="math inline">\(Q\)</span> be the initial problem as the Martians see it. That is, <span class="math inline">\(Q\)</span> is the problem of finding the best outcome given full knowledge of the situation, but the actual computational limitations of the agent. Then we suggest that we should also account for the informational limitations. Let’s see if this will work, they say. Let <span class="math inline">\(I\)</span> be the function that transforms a problem into one that is sensitive to the informational limitations of the agent. But if we’re really sensitive to informational limitations, we should note that <span class="math inline">\(I(Q)\)</span> is also a problem the agent has to solve under conditions of less than full information.<a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a> So the informationally challenged agent will have to solve not just <span class="math inline">\(I(Q)\)</span>, but <span class="math inline">\(I^2(Q)\)</span>, and <span class="math inline">\(I^3(Q)\)</span> and so on.<a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a></p>
<p>Orthodox defenders of (human versions of) rational choice theory have to think this is a bad argument. And I think most of them will agree with roughly the solution I’m adopting. The right problem to solve is <span class="math inline">\(I(Q)\)</span>, on a model where <span class="math inline">\(Q\)</span> is in fact the problem of choosing the objectively best option. If one doesn’t know precisely what one’s knowledge is, then one has to maximise expected utility somewhat speculatively. But that doesn’t mean that one shouldn’t maximise expected utility.</p>
<p>But the bigger thing to say is that neither we nor the Martians really started with the right original problem. The original problem, <span class="math inline">\(O\)</span>, is the problem of choosing the objectively best option. The humans start by considering the problem <span class="math inline">\(I(O)\)</span>, i.e., <span class="math inline">\(P\)</span>, and then debate whether we should stick with that problem, or move to <span class="math inline">\(F(I(O))\)</span>. The Martians start by considering the problem <span class="math inline">\(F(O)\)</span>, i.e., <span class="math inline">\(Q\)</span>, then debate whether we should stick with that or move to <span class="math inline">\(I(F(O))\)</span>. And the answer in both cases is that we should move.</p>
<p>Given the plausible commutativity principle, that introducing two limitations to theorising has the same effect whichever order we introduce them, <span class="math inline">\(I(F(O)) = F(I(O)\)</span>$. That is, <span class="math inline">\(F(P) = I(Q)\)</span>. And that’s the problem that we should think the rational agent is solving.</p>
<p>But why solve that, rather than something more or less close to <span class="math inline">\(O\)</span>? Well, think about what we say about an agent in a Jackson case who tries to solve <span class="math inline">\(O\)</span> not <span class="math inline">\(I(O)\)</span>. (A Jackson case, in this sense, is a case where the choice with highest expected value is known to not have the highest objective value. So trying to get the highest objective value will mean definitely not maximising expected value.) We think it will be sheer luck if they succeed. We think in the long run they will almost certainly do worse than if they tried to solve <span class="math inline">\(I(O)\)</span>. And in the rare case where they do better, we think it isn’t a credit to them, but to their luck. In cases where the well-being of others is involved, we think aiming for the solution to <span class="math inline">\(O\)</span> involves needless, and often immoral, risk-taking.</p>
<p>The Martains can quite rightly say the same things about why <span class="math inline">\(F(O)\)</span> is a more theoretically interesting problem than <span class="math inline">\(O\)</span>. Assume we are in a situation where <span class="math inline">\(F(O)\)</span> is known to differ from <span class="math inline">\(O\)</span>. For example, imagine the decision maker will get a reward if they announce the correct answer to whether a particular sentence is a truth-functional tautology, and they are allowed to pay a small fee to use a computer that can decide whether any given sentence is a tautology. The solution to <span class="math inline">\(O\)</span> is to announce the correct answer, whatever it is. The solution to <span class="math inline">\(F(O)\)</span> is to pay to use the computer. And the Martians might point out that in the long run, solving <span class="math inline">\(F(O)\)</span> will yield better results. That if the agent does solve problems like <span class="math inline">\(O\)</span> correctly, even in the long run, this will just mean they were lucky not rational. That if the reward is that a third party does not suffer, then it is immorally reckless to not solve <span class="math inline">\(F(O)\)</span>, i.e., to not consult the computer. And in general, whatever we can say that motivated “Rational Choice Theory”, as opposed to “Choose the Best Choice Theory”, they can say too.</p>
<p>Both the human and the Martian arguments look good to me. We should add in both computational and informational limitations into our model of the ideal agent. But note something else that comes from thinking about these Jackson cases. In solving a limitation sensitive problem, we aren’t trying to approximate a solution to the limination insensitive problem. This is part of why the regress can stop here. To solve <span class="math inline">\(F(X)\)</span>, we don’t have to solve <span class="math inline">\(X\)</span>, and then see how close the various computationally feasible solutions get to this solution. That’s true in general because of Jackson cases, but it’s especially true when <span class="math inline">\(X\)</span> is itself a complex problem. In trying to solve <span class="math inline">\(F(I(O))\)</span>, i.e., <span class="math inline">\(I(F(O))\)</span>, we aren’t trying to maximise expected value, and then approximate that solution given computational limitations. Nor are we trying to be optimal by Martian standards (i.e., solve <span class="math inline">\(F(O)\)</span>), then approximate that given informational limitations. We’re just trying to get as good an outcome as we can, given our limitations. Doing that does not require solving any iterated problem about how well we can solve <span class="math inline">\(F(I(O))\)</span> given various limitations, any more than rationally picking berries requires drawing Marshallian curves.</p>
<p>So that’s the solution to the regress. It is legitimate to think that there is a rule that rational creatures follow immediately, on pain of thinking that all theories of rationality imply regresses. And thinking about the contingency of how Rational Choice Theory got to be the way it is suggests that the solution to what Conlisk calls <span class="math inline">\(F(P)\)</span>, or what I’ve called <span class="math inline">\(F(I(O))\)</span>, will be that point.</p>
<p>What might that stopping point look like in practice? In his discussion of the regress, Miles <span class="citation">Kimball (<a href="references.html#ref-Kimball2015">2015</a>)</span> suggests a few options. I want to focus on two of them.</p>
<blockquote>
<p>Least transgressive are models in which an agent sits down once in a long while to think very carefully about how carefully to think about decisions of a frequently encountered type. For example, it is not impossible that someone might spend one afternoon considering how much time to spend on each of many grocery-shopping trips in comparison shopping. In this type of modelling, the infrequent computations of how carefully to think about repeated types of decisions could be approximated as if there were no computational cost, even though the context of the problem implies that those computational costs are strictly positive. <span class="citation">(Kimball <a href="references.html#ref-Kimball2015">2015</a>, 174)</span></p>
</blockquote>
<p>And that’s obviously relevant to David in the supermarket. He could, in principle, spend one Saturday afternoon thinking about how carefully to check each of the items in the supermarket before putting it in his shopping cart. And then in future trips, he could just carry out this plan. This isn’t terrible, but I don’t think it’s optimal. For one thing, there are much better things to do with Saturday afternoons. For another, it suggests we are back in the business of equating solving <span class="math inline">\(F(P)\)</span> with approximately solving <span class="math inline">\(P\)</span>. And that’s a mistake. Better to just say that David is rational if he just does the things that he would do were he to waste a Saturday afternoon this way, and then plan it out. And that thought leads to Kimball’s more radical suggestion for how to avoid the regress,</p>
<blockquote>
<p>[M]odelling economic actors as doing constrained optimization in relation to a simpler economic model than the model treated as true in the analysis. This simpler economic model treated as true by the agent can be called a “folk theory” <span class="citation">(Kimball <a href="references.html#ref-Kimball2015">2015</a>, 175)</span></p>
</blockquote>
<p>It’s this last idea I plan to explore in more detail. (It has some similarities to the discussion of small worlds in <span class="citation">Joyce (<a href="references.html#ref-Joyce1999">1999</a>)</span> 70-77. The short version is that David can, and should, have a little toy model of the supermarket in his head, and should optimize relative to that model. The model will be false, and David will know it is false. And that won’t matter, as long as David treats the model the right way.</p>
</div>
<div id="ignorancebliss" class="section level2">
<h2><span class="header-section-number">7.7</span> Ignorance is Bliss</h2>
<p>There are a lot of things that could have gone wrong with a can of chickpeas. They could have gone bad inside the can. They could have been contaminated, either deliberately or through carelessness. They could have been sitting around so long they have expired. All these things are, at least logically, possible.</p>
<p>But these possibilities, while serious, have two quite distinctive features. One is that they are very rare. In some cases they may have never happened. (I’ve never heard of someone deliberately contaminating canned chickpeas, though other grocery products like strawberries have been contamination targets.) The other is that there are few easy ways to tell whether they are actualised. You can scan each of the cans for an expiry date, but it is really uncommon that this is relevant, and it takes work since the expiry dates are normally written in such small type. If a can is really badly dented, I guess that weakens the metal and raises ever so slightly the prospect of unintentional contamination. But it’s common to have shelves full of cans that have no dents, or at most very minor ones.</p>
<p>Given these two facts - the rarity of the problems and the difficulty in getting evidence that significantly shifts the probability that this is one of the (rare) problems - the rational thing to do is choose in a way that is insensitive to whether those problems are actualised. Or, perhaps more cautiously, one should be vigilant, in the sense of <span class="citation">Sperber et al. (<a href="references.html#ref-SperberEtAl2010">2010</a>)</span>, to some of these problems, and ignore the rest. But being vigilant about a problem means, I take it, being willing to consider it if and only if you get evidence that it is worth considering. In the short run, you still ignore the potential problem.</p>
<p>And to ignore a potential problem is to choose in a way that is insensitive to evidence for the problem. That makes sense for both the banknotes and the chickpeas, because engaging in a choice procedure that is sensitive to the probability of the problem will, in the long run, make you worse off.</p>
<p>In Kimball’s terms, the rational shopper will have a toy model of the supermarket in which all cans of chickpeas that aren’t obviously damaged are safe to eat. This will be a defeasible model, but on a typical grocery trip, it won’t be defeated. (In Joyce’s terms, the small worlds the shopper uses in setting up the decision problem they face will all be ones in which the chickpeas are safe.)</p>
<p>But if that’s the toy model the shopper has, then conditionalising on the fact that a particular can is safe doesn’t change anything. Assuming that the can was not obviously damaged, the toy model says that it is fine to eat. So there is no argument from interest-relativity that the shopper does not know this can is fine to eat. So we’ve avoided the worst of the sceptical challenges. By similar reasoning, Frankie Lee knows all of the banknotes are genuine.</p>
<p>This chapter started with a problem for pragmatic theories of knowledge like IRT. They seemed to lead to rampant scepticism in cases like Frankie Lee’s. But the solution to this problem was, ultimately, more pragmatism. The rational chooser won’t typically use a model where the probability that a particular note is a forgery, or a particular can is contaminated, is 0.99999. Using a model like that, rather than a model where the probability is 1, is more trouble than it’s worth, especially since there is no actionable difference between the models. At least, it’s usually more trouble than it’s worth. If passing a forged banknote will lead to torture, then maybe you use the more complex model. If there is a something to do about differences between nearly 1 and 1, as in the case when some banknotes are plastic, or in the case where you can look inside the egg carton, then you use the more complex model. But usually one should, and does, use the simpler model.</p>
<p>So David does know that the chickpeas are safe. He believes this on the basis of evidence that is connected in the right way to the truth of the proposition that the chickpeas are safe. There is a potential pragmatic defeater from the fact that <strong>Conditional Preference</strong> seems to rule out this knowledge. But there is a pragmatic defeater of that pragmatic defeater. The thought behind <strong>Conditional Preference</strong> assumes that David will be insensitive to deliberation costs when choosing. He shouldn’t be, on practical grounds. He should use a toy model that says all safe looking cans are safe. And once he uses that toy model, there is no pragmatic defeat of his well-supported, well-grounded true belief. He knows the chickpeas are safe.</p>
<p>But he doesn’t know the eggs aren’t cracked. The toy model that says all eggs on the shelf are uncracked is a bad toy model to use. It isn’t bad because it’s wrong. It’s bad because there is a model that will yield better long run results even once we account for its complexity. That’s the model that says that only eggs that have been visually inspected are certain to be uncracked; all other eggs are at best probably uncracked. So David doesn’t know the eggs aren’t cracked.</p>
<p>And note this would be true even if improvements in the supply chain made the probability of cracked eggs much lower than it is today. What matters in the canned goods case is not just that the risk of contamination is low, it’s also that there isn’t anything to do about it. As long as it remains easy to flip the lid of egg cartons to check whether they are cracked, it will be hard to know without flipping they aren’t cracked.</p>
<p>This is another illustration of how the form of IRT I endorse really doesn’t care about stakes. The stakes in this case are not zero - buying cracked eggs wastes money and that’s why David should check. But it isn’t ‘high stakes’ in anything like the sense that phrase is used. The stakes are exactly the same as in the chickpeas case. What matters is not the cost of being wrong about an assumption, but rather the relative cost of being wrong compared to the probability that one is wrong and the cost of checking.</p>
<p>The milk case is only slightly more complicated. At least in some places, the expiry date for milk is written in very large print on the front of the bottle. In those cases, it is worth checking that you aren’t buying milk that expires tomorrow. So before you check, you don’t know that the milk you pick up doesn’t expire tomorrow. (And, like in the eggs case, that’s true even if the shop very very rarely sells milk that close to the expiry date.) But there is no way to check whether a particular unexpired milk has gone bad. You can’t easily open a milk bottle in the supermarket and smell it, for example. So that’s the kind of rare and uncheckable problem that the sensible chooser will ignore. Their toy model will include that in a well functioning store, all milk that is well away from the expiry date is safe. So once they’ve checked the expiry date, they know it is safe (assuming it is safe).</p>
<p>And in the normal case, Frankie Lee knows that the notes aren’t forgeries. His toy model of the currency, like ours, should be that all bank notes are genuine unless there is a clear sign that they are not.<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a> So we have a solution from within IRT to both the closure problems and the sceptical problems.</p>
<p>In the next chapter, I’ll look at problems that can be addressed without taking this many detours into decision theory.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="38">
<li id="fn38"><p>If that sounds implausible to you, make the can/bottle/carton a different size, or change the currency to some other dollars than the one you’re instinctively using. But I think this examples works tolerably well when understand as involving, for example, East Carribean dollars.<a href="ties.html#fnref38" class="footnote-back">↩</a></p></li>
<li id="fn39"><p>James <span class="citation">Joyce (<a href="references.html#ref-Joyce2018">2018</a>)</span> suggests the following terminology. If Frankie is rational, then utility considerations settle questions about what to <em>choose</em>, but not questions about what to <em>pick</em> in the case of a tie. I haven’t quite followed that terminology; I’ve let Frankie pick and choose more freely than that. But I’m following Joyce in stressing this conceptual distinction.<a href="ties.html#fnref39" class="footnote-back">↩</a></p></li>
<li id="fn40"><p>At this point the Martians might note that while they are grateful that <span class="citation">Williamson (<a href="references.html#ref-Williamson2000">2000</a>)</span> has highlighted problems with the KK principle, and these problems show some of the reasons for wanting to idealise away from informational limitations, they aren’t in fact relying on Williamson’s work. All they need is that agents do not exactly what they know. And that will be true as long as the correct epistemic logic is weaker than S5. And that will be true as long as someone somewhere has a false belief. And it would just be weird, they think, to care about informational limitations but want to idealise away from the existence of false beliefs.<a href="ties.html#fnref40" class="footnote-back">↩</a></p></li>
<li id="fn41"><p>At this point, some of the Martians note that the existence of <span class="citation">Elster (<a href="references.html#ref-Elster1979">1979</a>)</span> restored their faith in humanity.<a href="ties.html#fnref41" class="footnote-back">↩</a></p></li>
<li id="fn42"><p>Or at least a sign. Arguably, the fact that a note is a high value one that someone is trying to use in the betting ring half an hour before the Melbourne Cup is in itself a sign that it is not genuine. A sceptical theory that says no one there knows whether they are passing on forged bank notes is not a problematic sceptical theory.<a href="ties.html#fnref42" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ratbel.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="preface.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Kahis.pdf", "Kahis.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
