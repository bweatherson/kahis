<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 5 Evidence | Knowledge</title>
  <meta name="description" content="A defence of an interest-relative theory of knowledge">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 5 Evidence | Knowledge" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://brian.weatherson.org/kahis/" />
  <meta property="og:image" content="https://brian.weatherson.org/kahis/tree.jpg" />
  <meta property="og:description" content="A defence of an interest-relative theory of knowledge" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Evidence | Knowledge" />
  
  <meta name="twitter:description" content="A defence of an interest-relative theory of knowledge" />
  <meta name="twitter:image" content="https://brian.weatherson.org/kahis/tree.jpg" />

<meta name="author" content="Brian Weatherson">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="knowledge.html">
<link rel="next" href="ratbel.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="kahis.html">Knowledge: A Human Interest Story</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Front Matter</a></li>
<li class="chapter" data-level="1" data-path="prologue.html"><a href="prologue.html"><i class="fa fa-check"></i><b>1</b> Prologue</a></li>
<li class="chapter" data-level="2" data-path="interests.html"><a href="interests.html"><i class="fa fa-check"></i><b>2</b> Interests in Epistemology</a><ul>
<li class="chapter" data-level="2.1" data-path="interests.html"><a href="interests.html#redblue"><i class="fa fa-check"></i><b>2.1</b> Red or Blue?</a></li>
<li class="chapter" data-level="2.2" data-path="interests.html"><a href="interests.html#fourfamilies"><i class="fa fa-check"></i><b>2.2</b> Four Families</a></li>
<li class="chapter" data-level="2.3" data-path="interests.html"><a href="interests.html#orthodox"><i class="fa fa-check"></i><b>2.3</b> Against Orthodoxy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="interests.html"><a href="interests.html#orthodoxmoore"><i class="fa fa-check"></i><b>2.3.1</b> Moore’s Paradox</a></li>
<li class="chapter" data-level="2.3.2" data-path="interests.html"><a href="interests.html#superknow"><i class="fa fa-check"></i><b>2.3.2</b> Super Knowledge to the Rescue?</a></li>
<li class="chapter" data-level="2.3.3" data-path="interests.html"><a href="interests.html#probrescue"><i class="fa fa-check"></i><b>2.3.3</b> Rational Credences to the Rescue?</a></li>
<li class="chapter" data-level="2.3.4" data-path="interests.html"><a href="interests.html#orthodoxevidence"><i class="fa fa-check"></i><b>2.3.4</b> Evidential Probability</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="interests.html"><a href="interests.html#oddsandstakes"><i class="fa fa-check"></i><b>2.4</b> Odds and Stakes</a></li>
<li class="chapter" data-level="2.5" data-path="interests.html"><a href="interests.html#whatinterests"><i class="fa fa-check"></i><b>2.5</b> Theoretical Interests Matter</a></li>
<li class="chapter" data-level="2.6" data-path="interests.html"><a href="interests.html#global"><i class="fa fa-check"></i><b>2.6</b> Global Interest Relativity</a></li>
<li class="chapter" data-level="2.7" data-path="interests.html"><a href="interests.html#neutrality"><i class="fa fa-check"></i><b>2.7</b> Neutrality</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="belief.html"><a href="belief.html"><i class="fa fa-check"></i><b>3</b> Belief</a><ul>
<li class="chapter" data-level="3.1" data-path="belief.html"><a href="belief.html#beliefsinterests"><i class="fa fa-check"></i><b>3.1</b> Beliefs and Interests</a></li>
<li class="chapter" data-level="3.2" data-path="belief.html"><a href="belief.html#mapslegends"><i class="fa fa-check"></i><b>3.2</b> Maps and Legends</a></li>
<li class="chapter" data-level="3.3" data-path="belief.html"><a href="belief.html#given"><i class="fa fa-check"></i><b>3.3</b> Taking As Given</a></li>
<li class="chapter" data-level="3.4" data-path="belief.html"><a href="belief.html#block"><i class="fa fa-check"></i><b>3.4</b> Blocking Belief</a></li>
<li class="chapter" data-level="3.5" data-path="belief.html"><a href="belief.html#questions"><i class="fa fa-check"></i><b>3.5</b> Questions and Conditional Questions</a></li>
<li class="chapter" data-level="3.6" data-path="belief.html"><a href="belief.html#changes"><i class="fa fa-check"></i><b>3.6</b> A Million Dead End Streets</a><ul>
<li class="chapter" data-level="3.6.1" data-path="belief.html"><a href="belief.html#mecorrect"><i class="fa fa-check"></i><b>3.6.1</b> Correctness</a></li>
<li class="chapter" data-level="3.6.2" data-path="belief.html"><a href="belief.html#meimpractical"><i class="fa fa-check"></i><b>3.6.2</b> Impractical Propositions</a></li>
<li class="chapter" data-level="3.6.3" data-path="belief.html"><a href="belief.html#threeway"><i class="fa fa-check"></i><b>3.6.3</b> Choices with More Than Two Options</a></li>
<li class="chapter" data-level="3.6.4" data-path="belief.html"><a href="belief.html#meties"><i class="fa fa-check"></i><b>3.6.4</b> Hard Times and Close Calls</a></li>
<li class="chapter" data-level="3.6.5" data-path="belief.html"><a href="belief.html#modalupdate"><i class="fa fa-check"></i><b>3.6.5</b> Updates and Modals</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="belief.html"><a href="belief.html#nearby-views"><i class="fa fa-check"></i><b>3.7</b> Nearby Views</a><ul>
<li class="chapter" data-level="3.7.1" data-path="belief.html"><a href="belief.html#ganson"><i class="fa fa-check"></i><b>3.7.1</b> Ganson’s Theory</a></li>
<li class="chapter" data-level="3.7.2" data-path="belief.html"><a href="belief.html#usc"><i class="fa fa-check"></i><b>3.7.2</b> Ross and Schroeder’s Theory</a></li>
<li class="chapter" data-level="3.7.3" data-path="belief.html"><a href="belief.html#leitgeb"><i class="fa fa-check"></i><b>3.7.3</b> Leitgeb’s Stability Theory</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="belief.html"><a href="belief.html#weakbelief"><i class="fa fa-check"></i><b>3.8</b> Weak Belief</a></li>
<li class="chapter" data-level="3.9" data-path="belief.html"><a href="belief.html#probone"><i class="fa fa-check"></i><b>3.9</b> Belief as Probability One</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="knowledge.html"><a href="knowledge.html"><i class="fa fa-check"></i><b>4</b> Knowledge</a><ul>
<li class="chapter" data-level="4.1" data-path="knowledge.html"><a href="knowledge.html#structure"><i class="fa fa-check"></i><b>4.1</b> Knowledge and Practical Interests</a></li>
<li class="chapter" data-level="4.2" data-path="knowledge.html"><a href="knowledge.html#theoreticalknowledge"><i class="fa fa-check"></i><b>4.2</b> Theoretical Knowledge</a></li>
<li class="chapter" data-level="4.3" data-path="knowledge.html"><a href="knowledge.html#knowledge-and-closure"><i class="fa fa-check"></i><b>4.3</b> Knowledge and Closure</a><ul>
<li class="chapter" data-level="4.3.1" data-path="knowledge.html"><a href="knowledge.html#andelim"><i class="fa fa-check"></i><b>4.3.1</b> Single Premise Closure</a></li>
<li class="chapter" data-level="4.3.2" data-path="knowledge.html"><a href="knowledge.html#andintro"><i class="fa fa-check"></i><b>4.3.2</b> Multiple Premise Closure</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="knowledge.html"><a href="knowledge.html#closuresummary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="evidence.html"><a href="evidence.html"><i class="fa fa-check"></i><b>5</b> Evidence</a><ul>
<li class="chapter" data-level="5.1" data-path="evidence.html"><a href="evidence.html#evpuzzle"><i class="fa fa-check"></i><b>5.1</b> A Puzzle About Evidence</a></li>
<li class="chapter" data-level="5.2" data-path="evidence.html"><a href="evidence.html#simplesolution"><i class="fa fa-check"></i><b>5.2</b> A Simple, but Incomplete, Solution</a></li>
<li class="chapter" data-level="5.3" data-path="evidence.html"><a href="evidence.html#radicalinterpretation"><i class="fa fa-check"></i><b>5.3</b> The Radical Interpreter</a></li>
<li class="chapter" data-level="5.4" data-path="evidence.html"><a href="evidence.html#globalgame"><i class="fa fa-check"></i><b>5.4</b> Motivating Risk-Dominant Equilibria</a><ul>
<li class="chapter" data-level="5.4.1" data-path="evidence.html"><a href="evidence.html#cvdproof"><i class="fa fa-check"></i><b>5.4.1</b> The Dominance Argument for Risk-Dominant Equilibria</a></li>
<li class="chapter" data-level="5.4.2" data-path="evidence.html"><a href="evidence.html#perfectri"><i class="fa fa-check"></i><b>5.4.2</b> Making One Signal Precise</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="evidence.html"><a href="evidence.html#evsolution"><i class="fa fa-check"></i><b>5.5</b> Objections and Replies</a></li>
<li class="chapter" data-level="5.6" data-path="evidence.html"><a href="evidence.html#cutelim"><i class="fa fa-check"></i><b>5.6</b> Evidence, Knowledge and Cut-Elimination</a></li>
<li class="chapter" data-level="5.7" data-path="evidence.html"><a href="evidence.html#basic"><i class="fa fa-check"></i><b>5.7</b> Basic Knowledge and Non-Inferential Knowledge</a></li>
<li class="chapter" data-level="5.8" data-path="evidence.html"><a href="evidence.html#weakness"><i class="fa fa-check"></i><b>5.8</b> Epistemic Weakness</a></li>
<li class="chapter" data-level="5.9" data-path="evidence.html"><a href="evidence.html#neta"><i class="fa fa-check"></i><b>5.9</b> Holism and Defeaters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ratbel.html"><a href="ratbel.html"><i class="fa fa-check"></i><b>6</b> Rational Belief</a><ul>
<li class="chapter" data-level="6.1" data-path="ratbel.html"><a href="ratbel.html#atomism"><i class="fa fa-check"></i><b>6.1</b> Atomism about Rational Belief</a></li>
<li class="chapter" data-level="6.2" data-path="ratbel.html"><a href="ratbel.html#lockecoin"><i class="fa fa-check"></i><b>6.2</b> Coin Puzzles</a></li>
<li class="chapter" data-level="6.3" data-path="ratbel.html"><a href="ratbel.html#lockegames"><i class="fa fa-check"></i><b>6.3</b> Playing Games</a></li>
<li class="chapter" data-level="6.4" data-path="ratbel.html"><a href="ratbel.html#lockepuzzles"><i class="fa fa-check"></i><b>6.4</b> Puzzles for Lockeans</a><ul>
<li class="chapter" data-level="6.4.1" data-path="ratbel.html"><a href="ratbel.html#lockearb"><i class="fa fa-check"></i><b>6.4.1</b> Arbitrariness</a></li>
<li class="chapter" data-level="6.4.2" data-path="ratbel.html"><a href="ratbel.html#lockecorrect"><i class="fa fa-check"></i><b>6.4.2</b> Correctness</a></li>
<li class="chapter" data-level="6.4.3" data-path="ratbel.html"><a href="ratbel.html#lockemoore"><i class="fa fa-check"></i><b>6.4.3</b> Moorean Paradoxes</a></li>
<li class="chapter" data-level="6.4.4" data-path="ratbel.html"><a href="ratbel.html#closure"><i class="fa fa-check"></i><b>6.4.4</b> Closure and the Lockean Theory</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ratbel.html"><a href="ratbel.html#solving"><i class="fa fa-check"></i><b>6.5</b> Solving the Challenges</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ratbel.html"><a href="ratbel.html#coins"><i class="fa fa-check"></i><b>6.5.1</b> Coins</a></li>
<li class="chapter" data-level="6.5.2" data-path="ratbel.html"><a href="ratbel.html#games"><i class="fa fa-check"></i><b>6.5.2</b> Games</a></li>
<li class="chapter" data-level="6.5.3" data-path="ratbel.html"><a href="ratbel.html#arbitrariness"><i class="fa fa-check"></i><b>6.5.3</b> Arbitrariness</a></li>
<li class="chapter" data-level="6.5.4" data-path="ratbel.html"><a href="ratbel.html#moore"><i class="fa fa-check"></i><b>6.5.4</b> Moore</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ties.html"><a href="ties.html"><i class="fa fa-check"></i><b>7</b> Hard Choices</a><ul>
<li class="chapter" data-level="7.1" data-path="ties.html"><a href="ties.html#frankielee"><i class="fa fa-check"></i><b>7.1</b> An Example</a></li>
<li class="chapter" data-level="7.2" data-path="ties.html"><a href="ties.html#tiesresponse"><i class="fa fa-check"></i><b>7.2</b> Responding to the Challenge, Quickly</a></li>
<li class="chapter" data-level="7.3" data-path="ties.html"><a href="ties.html#backearth"><i class="fa fa-check"></i><b>7.3</b> Back to Earth</a></li>
<li class="chapter" data-level="7.4" data-path="ties.html"><a href="ties.html#supermarketquestions"><i class="fa fa-check"></i><b>7.4</b> I Have Questions</a></li>
<li class="chapter" data-level="7.5" data-path="ties.html"><a href="ties.html#satisfied"><i class="fa fa-check"></i><b>7.5</b> You’ll Never Be Satisfied (If You Try to Maximise)</a></li>
<li class="chapter" data-level="7.6" data-path="ties.html"><a href="ties.html#deliberationcosts"><i class="fa fa-check"></i><b>7.6</b> Deliberation Costs and Infinite Regresses</a></li>
<li class="chapter" data-level="7.7" data-path="ties.html"><a href="ties.html#ignorancebliss"><i class="fa fa-check"></i><b>7.7</b> Ignorance is Bliss</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>8</b> To Be Written</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Knowledge</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="evidence" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Evidence</h1>
<div id="evpuzzle" class="section level2">
<h2><span class="header-section-number">5.1</span> A Puzzle About Evidence</h2>
<p>Think back to the red-blue game at the start of the book.<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a> But now consider a version of the game where:</p>
<ul>
<li>The red sentence is that two plus two equals four.</li>
<li>The blue sentence is something that, if known, would be part of the
agent’s evidence.</li>
</ul>
<p>I’m going to argue that there are cases where the only rational play is Red-True, but the blue sentence is something we want to say that, ordinarily, the subject knows. And I’ll argue that this is a problem for the theory I have described so far. It is not a problem that shows that anything I’ve said so far is untrue. But it does suggest that what I’ve said so far is incomplete, and in a key respect unexplanatory.</p>
<p>I have tried so far to argue that belief, rational belief, and knowledge are all interest-relative. And I have tried to tell a story about when they are interest-relative. In the case of knowledge, the story is reasonably simple. One loses knowledge that <span class="math inline">\(p\)</span> when the situation changes in such a way that one is no longer entitled to take <span class="math inline">\(p\)</span> as given in deliberation. But what does it mean to say that one is entitled to take something as given? I haven’t given anything like a full theory of this, but the suggestion has been to interpret this on broadly evidentialist lines. In general, one is not entitled to take <span class="math inline">\(p\)</span> as given if the optimal choice, given one’s evidence, is different unconditionally to what it is conditional on <span class="math inline">\(p\)</span>.</p>
<p>But that story doesn’t explain when practical considerations might affect what evidence one has. Indeed, it can’t explain anything about evidence, since it takes evidence as a given. So if the arguments for the interest-relativity of knowledge can be repurposed to show that evidence too is interest-relative, we have a problem. Since I think they can be repurposed in just this way, there is a problem. The aim of this chapter is to set out just what the problem is, and to suggest a solution to it. I’ll start by arguing that evidence is interest-relative, then come back to what the problem is, and how I’ll aim to solve it.</p>
<p>The main example I’ll work through is the example of Parveen from subsection <a href="interests.html#orthodoxevidence">2.3.4</a>. Recall that she’s in a restaurant and notices an old friend, Rahul, across the restaurant. The conditions for detecting people aren’t perfect, and she’s surprised Rahul is here. But still we’d ordinarily say it is part of her evidence that Rahul is in this restaurant. She doesn’t infer this from other facts, and she would not be called on to defend it if she relies on it in ordinary circumstances. She then plays the red-blue game, with these sentences.</p>
<ul>
<li>The red sentence is <em>Two plus two equals four</em>.</li>
<li>The blue sentence is <em>Rahul is in this restaurant</em>.</li>
</ul>
<p>And the intuitions that raise problems for my view are:</p>
<ul>
<li>The unique rational play for Parveen is Red-True; and</li>
<li>If evidence is interest-invariant, then the evidential probability that Rahul is in the restaurant is the same as the evidential probability that two plus two is four.</li>
</ul>
<p>Now these intuitions are not inconsistent if evidence is interest-relative. And the point of this chapter will be to investigate, and ultimately endorse, this possibility. But I haven’t told you a story about how evidence can be interest-relative. I haven’t even started such a story. All the stories I’ve told you so far about interest-relativity have presupposed that the relevant evidence can be identified, and then we ask what the evidence warrants as circumstances change. That model is by its nature incapable of saying anything about when interests, or practical situations, affect evidence. The model isn’t wrong - but it is in a crucial respect incomplete. On the one hand, all models are incomplete. On the other hand, it would be odd to have the model’s explanatory ambitions stop somewhere between Anisa’s case and Parveen’s. That’s the kind of explanatory failure that makes one wonder whether you’ve got the original cases right.</p>
<p>There are two ways out of this problem that I don’t want to take, but are notable enough that I want to set them aside explicitly.</p>
<p>One is to say that propositions like <em>Rahul is in this restaurant</em> are never part of Parveen’s evidence. Perhaps her evidence just consists of things like <em>I am being appeared to Rahul-like</em>. Such an approach is problematic for two reasons. The first is that it is subject to all the usual objections to psychological theories of evidence <span class="citation">(Williamson <a href="references.html#ref-Williamson2007">2007</a>)</span>. The second is that we can re-run the argument with the blue sentence being some claim about Parveen’s psychological state, and still get the result that the only rational play is Red-True. A retreat to a psychological conception of evidence will only help with this problem if agents are infallible judges of their own psychological states, and that is not in general true <span class="citation">(Schwitzgebel <a href="references.html#ref-Schwitzgebel2008">2008</a>)</span>.</p>
<p>Another option is to deny that any explanation is needed here. Perhaps pragmatic effects, like the particular sentences that are chosen for this instance of the red-blue game, mean that Parveen’s evidence no longer includes facts about Rahul, and this is a basic epistemic fact without explanation. Now we shouldn’t assume that everything relevant to epistemology will have an epistemic explanation. Facts about the way that proteins work in the brain do not have explanations within epistemology, although they are vitally important for there even being a subject matter of epistemology. So in principle there could be facts around here that ground epistemic explanations without having explanations within epistemology. But in practice things look less rosy. Without an explanation of why Parveen loses evidence, we don’t have a theory that makes predictions about how interests affect knowledge. And we don’t have a satisfying explanation of why playing Blue-True is irrational for Parveen. And we are forced, as already noted, to draw an implausible distinction between Anisa and Parveen.</p>
<p>We shouldn’t be content with simply saying Parveen loses evidence when playing the red-blue game. We should say why this is so. The aim of the rest of this chapter is to tell a story that meets this explanatory desideratum.</p>
</div>
<div id="simplesolution" class="section level2">
<h2><span class="header-section-number">5.2</span> A Simple, but Incomplete, Solution</h2>
<p>Let’s take a step back and look at the puzzle more abstractly. We have a person <em>S</em>, who has some option <em>O</em>, and it really matters whether or not the expected value of <em>O</em>, i.e., <span class="math inline">\(V(O)\)</span>, is at least <span class="math inline">\(x\)</span>. (I am assuming that Parveen is in the business of maximising expected utility here. In chapter @(ties) I’ll look cases where this is an unreasonable assumption. But it seems reasonable here, because it doesn’t take much work to see that Red-True has very high utility.) It is uncontroversial that her evidence includes some background <span class="math inline">\(K\)</span>, and controversial whether it includes some contested proposition <span class="math inline">\(p\)</span>. It is also uncontroversial that <span class="math inline">\(V(O | p) \geq x\)</span>, and we’re assuming that for any proposition <span class="math inline">\(q\)</span> that is in her evidence, <span class="math inline">\(V(O | q) = V(O)\)</span>. That is, we’re assuming the relevant values are conditional on evidence. We can capture that last assumption with one big assumption that probably isn’t true, but is a harmless idealisation for the purposes of this chapter. Say there is a prior value function <span class="math inline">\(V^-\)</span>, with a similar metaphysical status to the mythical, mystical prior probability function. Then for any choice <span class="math inline">\(C\)</span>, <span class="math inline">\(V(C) = V^-(C | E)\)</span>, where <span class="math inline">\(E\)</span> is the evidence the agent has.</p>
<p>Now I can offer a simple, but incomplete, solution. Let <span class="math inline">\(p\)</span> be the proposition that she might or might not know, and the question of whether <span class="math inline">\(V(O) \geq x\)</span> be the only salient one that <span class="math inline">\(p\)</span> is relevant to. Then she knows <span class="math inline">\(p\)</span> only if the following is true:</p>
<blockquote>
<p><span class="math inline">\(\frac{V^-(O | K) + V^-(O | K \wedge p)}{2} \geq x\)</span></p>
</blockquote>
<p>That is, we work out the value of <span class="math inline">\(O\)</span> with and without the evidence <span class="math inline">\(p\)</span>, and if the average is greater than <span class="math inline">\(x\)</span>, good enough!</p>
<p>That solves the problem of Parveen and Rahul. Parveen’s evidence may or may not include that Rahul is in the restaurant. If it does, then Blue-True has a value of $50. If it does not, then Blue-True’s value is somewhat lower. Even if the evidence includes that someone who looks a lot like Rahul is in the restaurant, the value of Blue-True might only be $45. Averaging them out, the value is less than $50. But she’d only play Blue-True if it was worthwhile it play it instead of Red-True, which is worth $50. So she shouldn’t play Blue-True.</p>
<p>Great! Well, great except for two monumental problems.</p>
<p>The first problem is that what I’ve said here really only helps with very simple cases, where there is a single decision problem that a single contested proposition is relevant to. There has to be some way to generalise the case to less constrained situations.</p>
<p>The second (and bigger) problem is that the solution is completely ad hoc. Why should the arithmetic mean of these two things have any philosophical significance? Why not the mean of two other things? Why not some other function, like the geometric mean of them? This looks like a formula plucked out of the air, and there are literally infinitely many other formulae that would do just as well by the one criteria I’ve laid down so far: imply that Parveen must play red-true.</p>
<p>Pragmatic encroachment starts with a very elegant, very intuitive, principle: you only know the things you can reasonable take to be settled for the purposes of current deliberation. It does not look like any such elegant, intuitive, principles will lead to some theorem about averaging out the value of an option with and without new evidence.</p>
<p>Happily, the two problems have a common solution. But the solution requires a detour into some technical work concerning coordination games.</p>
</div>
<div id="radicalinterpretation" class="section level2">
<h2><span class="header-section-number">5.3</span> The Radical Interpreter</h2>
<p>Many philosophical problems can be usefully thought of as games, and hence studied using game theoretic techniques. This is especially when the problems involve interactions of rational agents. Here, for example, is the game table for Newcomb’s problem, with the human who is usually the focus of the problem as Row, and the demon as Column.<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a></p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Predict 1 Box</th>
<th align="center">Predict 2 Boxes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Choose 1 Box</td>
<td align="center">1000, 1</td>
<td align="center">0,0</td>
</tr>
<tr class="even">
<td>Choose 2 Boxes</td>
<td align="center">1001, 0</td>
<td align="center">1, 1</td>
</tr>
</tbody>
</table>
<p>This game has a unique Nash equilbrium; the bottom right corner.<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a> And that’s one way of motivating the view that (a) the game is possible, and (b) the rational move for the human is to choose two boxes.</p>
<p>Let’s look at a more complicated game. I’ll call it The Interpretation Game. The game has two players. Just like in Newcomb’s problem, one of them is a human, the other is a philosophical invention. But in this case the invention is not a demon, but The Radical Interpreter. To know the payouts for the players, we need to know their value function. More colloquially, we need to know their goals.</p>
<ul>
<li>The Radical Interpreter assigns mental states to Human in such a way as to predict Human’s actions given Human rationality. We’ll assume here that evidence is a mental state, so saying what evidence Human has is among Radical Interpreter’s tasks. (Indeed, in the game play to come, it will be their primary task.)</li>
<li>Human acts so as to maximise the expected utility of their action, conditional on the evidence that they have. Human doesn’t always know what evidence they have; it depends on what The Radical Interpreter says.</li>
</ul>
<p>The result is that the game is a coordination game. The Radical Interpreter wants to assign evidence in a way that predicts rational Human action, and Human wants to do what’s rational given that assignment of evidence. Coordination games typically have multiple equilibria, and this one is no exception.</p>
<p>Let’s make all that (marginally) more concrete. Human is offered a bet on <em>p</em>. If the bet wins, it wins 1 util; if the bet loses, it loses 100 utils. Human’s only choice is to Take or Decline the bet. The proposition <em>p</em>, the subject of the bet, is like the claim that Rahul is in the restaurant. It is something that is arguably part of Human’s evidence. Unfortunately, it is also arguable that it is not part of Human’s evidence. We will let <span class="math inline">\(K\)</span> be the rest of Human’s evidence (apart from <span class="math inline">\(p\)</span>, and things entailed by <span class="math inline">\(K \cup p\)</span>), and stipulate that <span class="math inline">\(\Pr(p | K) = 0.9\)</span>. Each party now faces a choice.</p>
<ul>
<li>The Radical Interpreter has to choose whether <em>p</em> is part of Human’s evidence or not.</li>
<li>Human has to decide whether to Take or Decline the bet.</li>
</ul>
<p>The Radical Interpreter achieves their goal if human takes the bet iff <em>p</em> is part of their evidence. If <em>p</em> is part of the evidence, then The Radical Interpreter thinks that the bet has positive expected utility, so Human will take it. And if <em>p</em> is not part of the evidence, then The Radical Interpreter thinks that the bet has negative expected utility, so Human will decline it. Either way, The Radical Interpreter wants Human’s action to coordinate with theirs. And Human wants to maximise expected utility. So we get the following table for the game.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center"><span class="math inline">\(p \in E\)</span></th>
<th align="center"><span class="math inline">\(p \notin E\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Take the bet</td>
<td align="center">1, 1</td>
<td align="center">-9.1, 0</td>
</tr>
<tr class="even">
<td>Decline the bet</td>
<td align="center">0, 0</td>
<td align="center">0, 1</td>
</tr>
</tbody>
</table>
<p>We have, in effect, already covered The Radical Interpreter’s payouts. They win in the top-left and lower-right quadrants, and lose otherwise. Human’s payouts are only a little trickier. In the bottom row, they are guaranteed 0, since the bet is declined. In the top-left, the bet is a sure winner; their evidence entails it wins. So they get a payout of 1. In the top-right, the bet wins with probability 0.9, so the expected return of taking it is <span class="math inline">\(1 \times 0.9 - 100 \times 0.1 = -9.1\)</span>.</p>
<p>There are two Nash equilibria for the game - I’ve bolded them below.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center"><span class="math inline">\(p \in E\)</span></th>
<th align="center"><span class="math inline">\(p \notin E\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Take the bet</td>
<td align="center"><strong>1, 1</strong></td>
<td align="center">-9.1, 0</td>
</tr>
<tr class="even">
<td>Decline the bet</td>
<td align="center">0, 0</td>
<td align="center"><strong>0, 1</strong></td>
</tr>
</tbody>
</table>
<p>That there are two equilibria to this game should not come as a surprise. It’s a formal parallel to the fact that the pragmatic encroachment theory I’ve developed so far doesn’t make a firm prediction about this game. It is consistent with the theory developed so far that Human’s evidence includes <span class="math inline">\(p\)</span>, and they should take the bet, or that due to interest-sensitive features of the case, it does not include <span class="math inline">\(p\)</span>, and they should not take the bet. The aim of this chapter is to supplement that theory with one that, at least most of the time, makes a firm pronouncement about what the evidence is.</p>
<p>But to do that, I need to delve into somewhat more contested areas of game theory. In particular, I need to introduce some work on equilibrium choice. And to do this, it helps to think about a game that is inspired by an example of Rousseau’s.</p>
</div>
<div id="globalgame" class="section level2">
<h2><span class="header-section-number">5.4</span> Motivating Risk-Dominant Equilibria</h2>
<p>At an almost maximal level of abstraction, a two player, two option each game looks like this.</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(a\)</span></td>
<td align="center"><span class="math inline">\(b\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A\)</span></td>
<td align="center"><span class="math inline">\(r_{11}\)</span>, <span class="math inline">\(c_{11}\)</span></td>
<td align="center"><span class="math inline">\(r_{12}\)</span>, <span class="math inline">\(c_{12}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(B\)</span></td>
<td align="center"><span class="math inline">\(r_{21}\)</span>, <span class="math inline">\(c_{21}\)</span></td>
<td align="center"><span class="math inline">\(r_{22}\)</span>, <span class="math inline">\(c_{22}\)</span></td>
</tr>
</tbody>
</table>
<p>We’re going to focus on games that have the following eight properties:</p>
<ul>
<li><span class="math inline">\(r_{11} &gt; r_{21}\)</span></li>
<li><span class="math inline">\(r_{22} &gt; r_{12}\)</span></li>
<li><span class="math inline">\(c_{11} &gt; c_{12}\)</span></li>
<li><span class="math inline">\(c_{22} &gt; c_{21}\)</span></li>
<li><span class="math inline">\(r_{11} &gt; r_{22}\)</span></li>
<li><span class="math inline">\(c_{11} \geq c_{22}\)</span></li>
<li><span class="math inline">\(\frac{r_{21}+r_{22}}{2} &gt; \frac{r_{11}+r_{12}}{2}\)</span></li>
<li><span class="math inline">\(\frac{c_{12}+c_{22}}{2} \geq \frac{c_{11}+c_{21}}{2}\)</span></li>
</ul>
<p>The first four clauses say that the game has two (strict) Nash equilibria: <span class="math inline">\(Aa\)</span> and <span class="math inline">\(Bb\)</span>. The fifth and sixth clauses say that the <span class="math inline">\(Aa\)</span> equilibria is <strong>Pareto-optimal</strong>: no one prefers the other equilibria to it. In fact it says something a bit stronger: one of the players strictly prefers the <span class="math inline">\(Aa\)</span> equilibria, and the other player does not prefer <span class="math inline">\(Bb\)</span>. The seventh and eighth clauses say that the <span class="math inline">\(Bb\)</span> equilibria is <strong>risk-optimal</strong>.</p>
<p>I’m going to offer an argument from Hans Carlsson and Eric van Damme <span class="citation">(<a href="references.html#ref-CarlssonVanDamme1993">1993</a>)</span> for the idea that in these games, rational players will end up at <span class="math inline">\(Bb\)</span>. The game that Human and The Radical Interpreter are playing fits these eight conditions, and The Radical Interpreter is perfectly rational, so this will imply that in that game, The Radical Interpreter will say that <span class="math inline">\(p \notin E\)</span>, which is what we aimed to show.</p>
<p>Games satisfying these eight inequalities are sometimes called <em>Stag Hunt</em> games. There is some flexibility, and some vagueness, in which of the eight inequalities need to be strict, but that level of detail isn’t important here. The name comes from a thought experiment in Rousseau’s <em>Discourse on Inequality</em>.</p>
<blockquote>
<p>They were perfect strangers to foresight, and were so far from troubling themselves about the distant future, that they hardly thought of the morrow. If a deer was to be taken, every one saw that, in order to succeed, he must abide faithfully by his post: but if a hare happened to come within the reach of any one of them, it is not to be doubted that he pursued it without scruple, and, having seized his prey, cared very little, if by so doing he caused his companions to miss theirs.  <span class="citation">(Rousseau <a href="references.html#ref-Rousseau1913">1913</a>, 209–10)</span></p>
</blockquote>
<p>It is rather interesting to think through which real-life situations are best modeled as Stag Hunts, especially in situations where people have thought that the right model was a version of Prisoners’ Dilemma. This kind of thought is one way in to appreciating the virtues of Rousseau’s political outlook, and especially the idea that social coordination might not require anything like the heavy regulatory presence that, say, Hobbes thought was needed. But that’s a story for another day. What I’m going to focus on is why Rousseau was right to think that a ‘stranger to foresight’, who is just focussing on this game, should take the rabbit.</p>
<p>To make matters a little easier, we’ll focus on a very particular instance of Stag Hunt, as shown here. (From here I’m following Carlsson and van Damme very closely; this is their example, with just the labelling slightly altered.)</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(a\)</span></td>
<td align="center"><span class="math inline">\(b\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A\)</span></td>
<td align="center">4, 4</td>
<td align="center">0, 3</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(B\)</span></td>
<td align="center">3, 0</td>
<td align="center">3, 3</td>
</tr>
</tbody>
</table>
<p>At first glance it might seem like <span class="math inline">\(Aa\)</span> is the right choice; it produces the best outcome. This isn’t like Prisoners Dilemma, where the best collective outcome is dominated. In fact <span class="math inline">\(Aa\)</span> is the best outcome for each individual. But it is risky, and Carlsson and van Damme show how to turn that risk into an argument for choosing <span class="math inline">\(Bb\)</span>.</p>
<p>Embed this game in what they call a <em>global game</em>. We’ll start the game with each player knowing just that they will play a game with the following payout table, with <span class="math inline">\(x\)</span> to be selected at random from a flat distribution over <span class="math inline">\([-1, 5]\)</span>.</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(a\)</span></td>
<td align="center"><span class="math inline">\(b\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A\)</span></td>
<td align="center">4, 4</td>
<td align="center">0, <span class="math inline">\(x\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(B\)</span></td>
<td align="center"><span class="math inline">\(x\)</span>, 0</td>
<td align="center"><span class="math inline">\(x\)</span>, <span class="math inline">\(x\)</span></td>
</tr>
</tbody>
</table>
<p>Before they play the game, each player will get a noisy signal about the value of <span class="math inline">\(x\)</span>. There will be signals <span class="math inline">\(s_R\)</span> and <span class="math inline">\(s_C\)</span> chosen (independently) from a flat distribution over <span class="math inline">\([x - 0.25, x + 0.25]\)</span>, and shown to Row and Column respectively. So each player will know the value of <span class="math inline">\(x\)</span> to within <span class="math inline">\(\frac{1}{4}\)</span>, and know that the other player knows it to within <span class="math inline">\(\frac{1}{4}\)</span> as well. But this is a margin of error model, and in those models there is very little that is common knowledge. That, they argue, makes a huge difference.</p>
<p>In particular, they prove that iterated deletion of strictly dominated strategies (almost) removes all but one strategy pair. (I’ll go over the proof of this in the next subsection.) Each player will play <span class="math inline">\(A\)</span>/<span class="math inline">\(a\)</span> if the signal is greater than 2, and <span class="math inline">\(B\)</span>/<span class="math inline">\(b\)</span> otherwise.<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a> Surprisingly, this shows that players should play the risk-optimal strategy even when they know the other strategy is Pareto-optimal. When a player gets a signal in <span class="math inline">\((2, 3.75)\)</span>, then they know that <span class="math inline">\(x &lt; 4\)</span>, so <span class="math inline">\(Bb\)</span> is the Pareto-optimal equilibrium. But the logic of the global game suggests the risk-dominant equilibrium is what to play.</p>
<p>Carlsson and van Damme go on to show that many of the details of this case don’t matter. As long as (a) there is a margin of error in each side’s estimation of the payoffs, and (b) every choice is a dominant option in some version of the global game, then iterated deletion of strongly dominant strategies will lead to each player making the risk-dominant choice.</p>
<p>I conclude from that that risk-dominant choices are rational in these games. There is a limit assumption involved here; what’s true for games with arbitrarily small margins of error is true for games with no margin of error. (We’ll come back to that assumption below.) And since The Radical Interpreter is rational, they will play the strategy that is not eliminated by deleting dominant strategies. That is, they will play the risk-dominant strategy.</p>
<p>In game with Human, the rational (i.e., risk-dominant) strategy for The Radical Interpreter is to say that <span class="math inline">\(p \notin E\)</span>. And in the case of Parveen and Rahul, rational (i.e., risk-dominant) strategy for The Radical Interpreter is to say that it is not part of Parveen’s evidence that Rahul is in the restaurant. And this is an interest-relative theory of evidence; had Parveen been playing a different game, The Radical Interpreter would have said that it is part of Parveen’s evidence that Rahul was in the restaurant.</p>
<p>And from this point all the intuitions about the case fall into place. If it is part of Parveen’s evidence that Rahul is in the restaurant, then she knows this. Conversely, if she knows it, then The Radical Interpreter would have said it is part of her evidence, so it is part of her evidence. Parveen will perform the action that maximises expected utility given her evidence. And she will lose knowledge when that disposition makes her do things that would be known to be sub-optimal if she didn’t lose knowledge.</p>
<p>In short, this model keeps what was good about the pragmatic encroachment theory developed in the previous chapters, while also allowing that evidence can be interest-relative. It does require a slightly more complex theory of rationality than was previously used. Rather than just model rational agents as utility maximisers, they are modelled as playing playing risk-dominant strategies in coordination games. But it turns out that this is little more than assuming that they maximise evidential expected utility, and they expect others (at least perfectly rational abstract others) to do the same, and they expect those others to expect they will maximise expected utility, and so on.</p>
<p>The rest of this section goes into more technical detail about Carlsson and van Damme’s example. Readers not interested in these details can skip ahead to the next section. In the first subsection I summarise their argument that we only need iterated deletion of strictly dominated strategies to get the result that rational players will play the risk-dominant strategies. In the second subsection I offer a small generalisation of their argument, showing that it still goes through when one of the players gets a precise signal, and the other gets a noisy signal. And I discuss why that is relevant. (In short, the Radical Interpreter doesn’t have to deal with noise, and we want the argument to respect that fact.)</p>
<div id="cvdproof" class="section level3">
<h3><span class="header-section-number">5.4.1</span> The Dominance Argument for Risk-Dominant Equilibria</h3>
<p>Two players, Row (or R) and Column (or C) will play a version of the following game.</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(a\)</span></td>
<td align="center"><span class="math inline">\(b\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A\)</span></td>
<td align="center">4, 4</td>
<td align="center">0, <span class="math inline">\(x\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(B\)</span></td>
<td align="center"><span class="math inline">\(x\)</span>, 0</td>
<td align="center"><span class="math inline">\(x\)</span>, <span class="math inline">\(x\)</span></td>
</tr>
</tbody>
</table>
<p>They won’t be told what <span class="math inline">\(x\)</span> is, but they will get a noisy signal of <span class="math inline">\(x\)</span>, drawn from an even distribution over <span class="math inline">\([x - 0.25, x + 0.25]\)</span>. Call these signals <span class="math inline">\(s_R\)</span> and <span class="math inline">\(s_C\)</span>. Each player must then choose <span class="math inline">\(A\)</span>, getting either 4 or 0 depending on the other player’s choice, or choose <span class="math inline">\(B\)</span>, getting <span class="math inline">\(x\)</span> for sure.</p>
<p>Before getting the signal, the players must choose a strategy. A strategy is a function from signals to choices. Since the higher the signal is, the better it is to play <span class="math inline">\(B\)</span>, we can equate strategies with ‘tipping points’, where the player plays <span class="math inline">\(B\)</span> if the signal is above the tipping point, and <span class="math inline">\(A\)</span> below the tipping point. Strictly speaking, a tipping point will pick out not a strategy but an equivalence class of strategies, which differ in how they act if the signal is the tipping point. But since that happens with probability 0, the strategies in the equivalence class have the same expected return, and so I won’t distinguish them.</p>
<p>Also, strictly speaking, there are strategies that are not tipping points, because they map signals onto probabilities of playing <span class="math inline">\(A\)</span>, where the probability decreases as <span class="math inline">\(A\)</span> rises. I won’t discuss these directly, but it isn’t too hard to see how these are shown to be suboptimal using the argument that is about to come. It eases exposition to focus on the pure strategies, and to equate these with tipping points. And since my primary aim here is to explain why the result holds, not to simply repeat an already existing proof, I’ll mostly ignore these mixed strategies.</p>
<p>Call the tipping points for Row and Column respectively <span class="math inline">\(T_R\)</span> and <span class="math inline">\(T_C\)</span>. Since the game is symmetric, we’ll just have to show that in conditions of common knowledge of rationality, <span class="math inline">\(T_R = 2\)</span>. It follows by symmetry that <span class="math inline">\(T_C = 2\)</span> as well. And the only rule that will be used is iterated deletion of strictly dominated strategies. That is, neither player will play a strategy where another strategy does better no matter what the opponent chooses, and they won’t play strategies where another strategy does better provided the other player does not play a dominated strategy, and they won’t play strategies where another strategy does better provided the other player does not play a strategy ruled out by these first two conditions, and so on.</p>
<p>The return to a strategy is uncertain, even given the other player’s strategy. But given the strategies of each player, each players’ expected return can be computed. And that will be treated as the return to the strategy pair.</p>
<p>Note first that <span class="math inline">\(T_R = 4.25\)</span> strictly dominates any strategy where <span class="math inline">\(T_R = y &gt; 4.25\)</span>. If <span class="math inline">\(s_R \in (4.25, y)\)</span>, then <span class="math inline">\(T_R\)</span> is guaranteed to return above 4, and the alternative strategy is guaranteed to return 4. In all other cases, the strategies have the same return. And there is some chance that <span class="math inline">\(s_R \in (4.25, y)\)</span>. So we can delete all strategies <span class="math inline">\(T_R = y &gt; 4.25\)</span>, and similarly all strategies <span class="math inline">\(T_C = y &gt; 4.25\)</span>. By similar reasoning, we can rule out <span class="math inline">\(T_R &lt; -0.25\)</span> and <span class="math inline">\(T_C &lt; -0.25\)</span>.</p>
<p>If <span class="math inline">\(s_R \in [-0.75, 4.75]\)</span>, then it is equally likely that <span class="math inline">\(x\)</span> is above <span class="math inline">\(s_R\)</span> as it is below it. Indeed, the posterior distribution of <span class="math inline">\(x\)</span> is flat over <span class="math inline">\([s_R - 0.25, s_R + 0.25]\)</span>. From this it follows that the expected return of playing <span class="math inline">\(B\)</span> after seeing signal <span class="math inline">\(s_R\)</span> is just <span class="math inline">\(s_R\)</span>.</p>
<p>Now comes the important step. Assume that we know that <span class="math inline">\(T_C \leq y &gt; 2\)</span>. Now consider the expected return of playing <span class="math inline">\(A\)</span> given various values for <span class="math inline">\(s_R &gt; 2\)</span>. Given that the lower <span class="math inline">\(T_C\)</span> is, the higher the expected return is of playing <span class="math inline">\(A\)</span>, we’ll just work on the simple case where <span class="math inline">\(T_C = y\)</span>, realizing that this is an upper bound on the expected return of <span class="math inline">\(A\)</span> given <span class="math inline">\(T_C \leq y\)</span>. The expected return of <span class="math inline">\(A\)</span> is 4 times the probability that Column will play <span class="math inline">\(a\)</span>, i.e., 4 times the probability that <span class="math inline">\(s_C &lt; T_C\)</span>. Given all the symmetries that have been built into the puzzle, we know that the probability that <span class="math inline">\(s_C &lt; s_R\)</span> is 0.5. So the expected return of playing <span class="math inline">\(A\)</span> is at most 2 if <span class="math inline">\(s_R \geq y\)</span>. But the expected return of playing <span class="math inline">\(B\)</span> is, as we showed in the last paragraph, <span class="math inline">\(s_R\)</span>, which is greater than 2. So it is better to play <span class="math inline">\(B\)</span> than <span class="math inline">\(A\)</span> if <span class="math inline">\(s_R \geq y\)</span>. And the difference is substantial, so even if <span class="math inline">\(s_R\)</span> is epsilon less than that <span class="math inline">\(y\)</span>, it will still be better to play <span class="math inline">\(B\)</span>. (This is rather hand-wavy, but I’ll go over the more rigorous version presently.)</p>
<p>So if <span class="math inline">\(T_C \leq y &gt; 2\)</span> we can prove that <span class="math inline">\(T_R\)</span> should be lower still, because given that assumption it is better to play <span class="math inline">\(B\)</span> even if the signal is just less than <span class="math inline">\(y\)</span>. Repeating this reasoning over and over again pushes us to it being better to play <span class="math inline">\(B\)</span> than <span class="math inline">\(A\)</span> as long as <span class="math inline">\(s_R &gt; 2\)</span>. And the same kind of reasoning from the opposite end pushes us to it being better to play <span class="math inline">\(A\)</span> than <span class="math inline">\(B\)</span> as long as <span class="math inline">\(s_R &lt; 2\)</span>. So we get <span class="math inline">\(s_R = 2\)</span> as the uniquely rational solution to the game.</p>
<p>Let’s make that a touch more rigorous. Assume that <span class="math inline">\(T_C = y\)</span>, and <span class="math inline">\(s_r\)</span> is slightly less than <span class="math inline">\(y\)</span>. In particular, we’ll assume that <span class="math inline">\(z = y - s_R\)</span> is in <span class="math inline">\((0, 0.5)\)</span>. Then the probability that <span class="math inline">\(s_C &lt; y\)</span> is <span class="math inline">\(0.5 + 2z - 2z^2\)</span>. So the expected return of playing <span class="math inline">\(A\)</span> is <span class="math inline">\(2 + 8z - 8z^2\)</span>. And the expected return of playing <span class="math inline">\(B\)</span> is, again, <span class="math inline">\(s_R\)</span>. These will be equal when the following is true. (The working out is a tedious but trivial application of the quadratic formula, plus some rearranging.)</p>
<p><span class="math display">\[s_R = y + \frac{\sqrt{145-32y} - 9}{16}\]</span></p>
<p>So if we know that <span class="math inline">\(T_C \geq y\)</span>, we know that <span class="math inline">\(T_R \geq y + \frac{\sqrt{145-32y} - 9}{16}\)</span>, which will be less than <span class="math inline">\(y\)</span> if <span class="math inline">\(y &gt; 2\)</span>. And then by symmetry, we know that <span class="math inline">\(T_C\)</span> must be at most as large as that as well. And then we can use that fact to derive a further upper bound on <span class="math inline">\(T_R\)</span> and hence on <span class="math inline">\(T_C\)</span>, and so on. And this will continue until we push both down to 2. It does require quite a number of steps of iterated deletion. Here is the upper bound on the threshold after <span class="math inline">\(n\)</span> rounds of deletion of dominated strategies. (These numbers are precise for the first two rounds, then just to three significant figures after that.)</p>
<table>
<thead>
<tr class="header">
<th align="center">Round</th>
<th align="center">Upper Bound on Threshold</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">4.250</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">3.875</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">3.599</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">3.378</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">3.195</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">3.041</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">2.910</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="center">2.798</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="center">2.701</td>
</tr>
<tr class="even">
<td align="center">10</td>
<td align="center">2.617</td>
</tr>
</tbody>
</table>
<p>That is, <span class="math inline">\(T_R = 4.25\)</span> dominates any strategy with a tipping point above 4.25. And <span class="math inline">\(T_R = 3.875\)</span> dominates any strategy with a higher tipping point than that, assuming <span class="math inline">\(T_C \leq 4.25\)</span>. And <span class="math inline">\(T_R \approx 3.599\)</span> dominates any strategy with a higher tipping point than that, assuming <span class="math inline">\(T_C \leq 3.875\)</span>. And so on.</p>
<p>And similar reasoning shows that at each stage not only are all strategies with higher tipping points dominated, but so are strategies that assign positive probability (whether it is 1 or less than 1), to playing <span class="math inline">\(A\)</span> when the signal is above the ‘tipping point’. So this kind of reasoning rules out all mixed strategies (except those that respond probabilistically to <span class="math inline">\(s_R = 2\)</span>).</p>
<p>So it has been shown that iterated deletion of dominated strategies will rule out all strategies except the risk-optimal equilibrium. The possibility that <span class="math inline">\(x\)</span> is greater than the maximal return for <span class="math inline">\(A\)</span> is needed to get the iterated dominance going. And the signal to have an error bar to it, so that each round of iteration removes more strategies. But that’s all that was needed; the particular values used are irrelevant to the proof.</p>
</div>
<div id="perfectri" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Making One Signal Precise</h3>
<p>The aim of this sub-section is to prove something that Carllson and van Damme did not prove, namely that the analysis of the previous subsection goes through with very little change if one party gets a perfect signal, while the other gets a noisy signal. So I’m going to discuss the game that is just like the game of the previous subsection, but where it is common knowledge that the signal Column gets, <span class="math inline">\(s_C\)</span>, equals <span class="math inline">\(x\)</span>.</p>
<p>Since the game is no longer symmetric, I can’t just appeal to the symmetry of the game as frequently as in the previous subsection. But this only slows the proof down, it doesn’t stop it.</p>
<p>We can actually rule out slightly more at the first step in this game than in the previous game. Since Column could not be wrong about <span class="math inline">\(x\)</span>, Column knows that if <span class="math inline">\(s_C &gt; 4\)</span> then playing <span class="math inline">\(b\)</span> dominates playing <span class="math inline">\(a\)</span>. So one round of deleting dominated strategies rules out <span class="math inline">\(T_C &gt; 4\)</span>, as well as ruling out <span class="math inline">\(T_R &gt; 4.25\)</span>.</p>
<p>At any stage, if we know <span class="math inline">\(T_C \leq y &gt; 2\)</span>, then <span class="math inline">\(T_R = y\)</span> dominates <span class="math inline">\(T_R &gt; y\)</span>. That’s because if <span class="math inline">\(s_R \geq y\)</span>, and <span class="math inline">\(T_C \leq y\)</span>, then the probability that Column will play <span class="math inline">\(a\)</span> (given Row’s signal) is less than 0.5. After all, the signal is just as likely to be above <span class="math inline">\(x\)</span> as below it (as long as the signal isn’t too close to the extremes). So if <span class="math inline">\(s_R\)</span> is at or above <span class="math inline">\(T_C\)</span>, then it is at least 0.5 likely that <span class="math inline">\(s_C = x\)</span> is at or above <span class="math inline">\(T_C\)</span>. So the expected return of playing <span class="math inline">\(A\)</span> is at most 2. But the expected return of playing <span class="math inline">\(B\)</span> equals the signal, which is greater than 2. So if Row knows <span class="math inline">\(T_C \leq y &gt; 2\)</span>, Row also knows it is better to play <span class="math inline">\(B\)</span> if <span class="math inline">\(s_R \geq y\)</span>. And that just means that <span class="math inline">\(T_R \leq y\)</span>.</p>
<p>Assume now that it is common knowledge that <span class="math inline">\(T_R \leq y\)</span>, for some <span class="math inline">\(y &gt; 2\)</span>. And assume that <span class="math inline">\(x = s_C\)</span> is just a little less than <span class="math inline">\(y\)</span>. In particular, define <span class="math inline">\(z = y -x\)</span>, and assume <span class="math inline">\(z \in (0, 0.25)\)</span>. We want to work out the upper bound on the expected return to Column of playing <span class="math inline">\(a\)</span>. (The return of playing <span class="math inline">\(b\)</span> is known, it is <span class="math inline">\(x\)</span>.) The will be highest when <span class="math inline">\(T_R\)</span> is lowest, so assume <span class="math inline">\(T_R \leq y\)</span>. Then the probability that Row plays <span class="math inline">\(A\)</span> is <span class="math inline">\((1 + 2z)/2\)</span>. So the expected return of playing <span class="math inline">\(a\)</span> is <span class="math inline">\(2 + 4z\)</span>, i.e., <span class="math inline">\(2 + 4(y - x)\)</span>. That will be greater than <span class="math inline">\(x\)</span> only when</p>
<p><span class="math display">\[x &lt; \frac{2 + 4y}{5}\]</span></p>
<p>And so if it is common knowledge that <span class="math inline">\(T_R \leq y\)</span>, then it is best for Column to play <span class="math inline">\(b\)</span> unless <span class="math inline">\(x &lt; \frac{2 + 4y}{5}\)</span>. That is, if it is common knowledge that <span class="math inline">\(T_R \leq y\)</span>, then <span class="math inline">\(T_C\)</span> must be at most <span class="math inline">\(\frac{2 + 4y}{5}\)</span>.</p>
<p>So now we proceed in a zig-zag fashion. At one stage, we show that <span class="math inline">\(T_R\)</span> must be as low as <span class="math inline">\(T_C\)</span>. At the next, we show that if it has been proven that <span class="math inline">\(T_R\)</span> takes a particular value greater than 2, then <span class="math inline">\(T_C\)</span> must be lower still. And this process will eventually rule out all values for <span class="math inline">\(T_R\)</span> and <span class="math inline">\(T_C\)</span> greater than 2.</p>
<p>This case is crucial to the story of this chapter because The Radical Interpreter probably does not have an error bar in their estimation of the game they are playing. But it turns out the argument for risk-dominant equilibria being the unique solution to interpretation games is consistent with that. As long as one player has a margin of error, each player should play the risk-dominant equilibria.</p>
</div>
</div>
<div id="evsolution" class="section level2">
<h2><span class="header-section-number">5.5</span> Objections and Replies</h2>
<p><em>Objection</em>: The formal results of the previous section only go through if we assume that the agents do not know precisely what the payoffs are in the game. We shouldn’t assume that what holds for arbitrarily small margins of error will hold in the limit, i.e., when they do know the payoffs.</p>
<p><em>Reply</em>: If pushed, I would defend the use limit assumptions like this to resolve hard cases like Stag Hunt. But I don’t strictly need that assumption here, What is needed is that Parveen doesn’t know precisely the probability of Rahul being in the restaurant given the rest of her evidence. Given that evidence is not luminous, as <span class="citation">Williamson (<a href="references.html#ref-Williamson2000">2000</a>)</span> shows, this is a reasonable assumption. So the margin of error assumption that Carlsson and van Damme make is not, in this case, an assumption that merely makes the math easier; it is built into the case.</p>
<p><em>Objection</em>: Even if Parveen doesn’t know the payoffs precisely, The Radical Interpreter does. The Radical Interpreter is an idealisation, so they can be taken
to be ideal.</p>
<p><em>Reply</em>: It turns out that Carlsson and van Damme’s result doesn’t require that both parties are ignorant of the precise values of the payoffs. As long as one party doesn’t know the exact value of the payoff, the argument goes through. That was the point of the proof in subsection <a href="evidence.html#perfectri">5.4.2</a>.</p>
<p><em>Objection</em>: The formal argument requires that in the ‘global game’ there are values for <span class="math inline">\(x\)</span> that make <span class="math inline">\(A\)</span> the dominant choice. These cases serve as a base step for an inductive argument that follows. But in Parveen’s case, there is no such setting for <span class="math inline">\(x\)</span>, so the inductive argument can’t get going.</p>
<p><em>Reply</em>: What matters is that there are values of <span class="math inline">\(x\)</span> such that <span class="math inline">\(A\)</span> is the strictly dominant choice, and Human (or Parveen) doesn’t know that they know that they know, etc., that those values are not actual. And that’s true in our case. For all Human (or Parveen) knows that they know that they know that they know…, the proposition in question is not part of their evidence under a maximally expansive verdict on The Radical Interpreter’s part. So the relevant cases are there in the model, even if for some high value of <span class="math inline">\(n\)</span> they are known<span class="math inline">\(^n\)</span> not to obtain.</p>
<p><em>Objection</em>: This model is much more complex than the simple motivation for pragmatic encroachment.</p>
<p><em>Reply</em>: Sadly, this is true. I would like to have a simpler model, but I don’t know how to create one. I suspect any such simple model will just be incomplete; it won’t say what Parveen’s evidence is. In this respect, any simple model will look just like applying tools like Nash equilibria to coordination games. So more complexity will be needed, one way or another. I think paying this price in complexity is worth it overall, but I can see how some people might think otherwise.</p>
<p><em>Objection</em>: Change the case involving Human so that the bet loses 15 utils if <em>p</em> is false, rather than 100. Now the risk-dominant equilibrium is that Human takes the bet, and The Radical Interpreter says that <em>p</em> is part of Human’s evidence. But note that if it was clearly true that <em>p</em> was not part of Human’s evidence, then this would still be too risky a situation for them to know <em>p</em>. So whether it is possible that <em>p</em> is part of Human’s evidence matters.</p>
<p><em>Reply</em>: This is all true, and it shows that the view I’m putting forward is incompatible with some programs in epistemology. In particular, it is incompatible with E=K, since the what it takes to be evidence on this story is slightly different from what it takes to be knowledge. I will come back to this point in section <a href="evidence.html#cutelim">5.6</a>.</p>
<p><em>Objection</em>: Carlsson and van Damme discuss one kind of global game. But there are other global games that have different equilibria. For instance, changing the method by which the noisy signal is selected would change the equilibrium of the global game. So this kind of argument can’t show that the risk-dominant equilibrium is the one true solution.</p>
<p><em>Reply</em>: This is somewhat true. There are other ways of embedding the game involving Human and The Radical Interpreter in global games that lead to different outcomes. They are usually somewhat artificial; e.g., by having the signal be systematically biased in one way. But what really matters is the game where the error in Human’s knowledge of the payoffs is determined by their actual epistemic limitations. I think that will lead to something like the model we have here. But it is possible that the final result will differ a bit from what I have here, or (more likely) have some indeterminacy about just how interests interact with evidence and knowledge. The precise details are ultimately less important to me than whether we can provide a motivated story of how interests affect knowledge and evidence that does not presuppose we know what the agent’s evidence is. And the method I’ve outlined here shows that we can do that, even if we end up tinkering a bit with the details.</p>
</div>
<div id="cutelim" class="section level2">
<h2><span class="header-section-number">5.6</span> Evidence, Knowledge and Cut-Elimination</h2>
<p>In the previous section I noted that my theory of evidence is committed to denying Williamson’s E=K thesis. This is the thesis that says one’s evidence is all and only what one knows. What I say is consistent with, and arguably committed to, one half of that thesis. Nothing I’ve said here provides a reason to reject the implication that if <span class="math inline">\(p\)</span> is part of one’s evidence, then one knows <span class="math inline">\(p\)</span>. Indeed, the story I’m telling would have to be complicated even further if that fails. But I am committed to denying the other direction. On my view, there can be cases where someone knows <span class="math inline">\(p\)</span>, but <span class="math inline">\(p\)</span> is not part of their evidence.</p>
<p>My main reason for this comes from the kind of cases that Shyam <span class="citation">Nair (<a href="references.html#ref-Nair2019">2019</a>)</span> describes as failures of ‘cut-elimination’. I’ll quickly set out what Nair calls cut-elimination, and why it fails, and then look at how it raises problems for E=K.</p>
<p>Start by assuming that we have an operator <span class="math inline">\(\vDash\)</span> such that <span class="math inline">\(\Gamma \vDash A\)</span> means that <span class="math inline">\(A\)</span> can be rationally inferred from <span class="math inline">\(\Gamma\)</span>. I’m following Nair (and many others) in using a symbol usually associated with logical entailment here, though this is potentially misleading. A big plotline in what follows will be that <span class="math inline">\(\vDash\)</span>, so understood, behaves very differently from familiar logics.</p>
<p>For the purposes of this section, I’m staying somewhat neutral on what it means to be able to rationally infer <span class="math inline">\(A\)</span> from <span class="math inline">\(\Gamma\)</span>. In particular, I want everything that follows to be consistent with the interpretation that an inference is rational only if it produces knowledge. I don’t think that’s true; I think folks with misleading evidence can rationally form false beliefs, and I think the traveller in Dharmottara’s example rationally believes there is a fire. But there is a dialectical reason for staying neutral here. I’m arguing against one important part of the ‘knowledge first’ program, and I don’t want to do so by assuming the falsity of other parts of it. So for this section (only), I’ll write in a way that is consistent with saying rational belief requires knowledge.</p>
<p>So one way to interpret <span class="math inline">\(\Gamma \vDash A\)</span> is that <span class="math inline">\(A\)</span> can be known on the basis of <span class="math inline">\(\Gamma\)</span>. What can be known on the basis of what is a function of, among other things, who is doing the knowing, what their background evidence is, what their capacities are, and so on. Strictly speaking, that suggests we should have some subscripts on <span class="math inline">\(\vDash\)</span> for who is the knower, what their background evidence is, and so on. In the interests of readability, I’m going to leave all those implicit. Though in the next section it will be important to come back and look at whether the force of some of these arguments is diminished if we are careful about this relativisation.</p>
<p>So that’s our important notation. The principle <strong>Cut</strong> that Nair focusses on is that if 1 and 2 are true, so is 3.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\Gamma \vDash A\)</span></li>
<li><span class="math inline">\(\{A\} \cup \Delta \vDash B\)</span></li>
<li><span class="math inline">\(\Gamma \cup \Delta \vDash B\)</span></li>
</ol>
<p>The principle is intuitive. Indeed, it is often implicit in a lot of reasoning. Here is one instance of it in action.</p>
<blockquote>
<p>I heard from a friend that Jack went up the hill. This friend is trustworthy, so I’m happy to infer that Jack did indeed go up the hill. I heard from another friend that Jack and Jill did the same thing. This friend is also trustworthy, so I’m happy to infer that Jill did the same thing as Jack, i.e., go up the hill.</p>
</blockquote>
<p>Normally we wouldn’t spell out the ‘happy to infer’ steps, but I’ve included them in here to make the reasoning a bit more explicit. But note that what I didn’t need to make explicit, even in this laborious reconstruction. That Jack went up the hill goes from a conclusion of the first little argument, to a premise in a later argument. The later argument says that we can conclude from the fact that Jack went up the hill, and that a friend said Jack and Jill did the same thing, that Jill went up the hill. And what matters for our purposes is that there doesn’t seem to be a gap between the rationality of inferring that Jack went up the hill, and the rationality of using that as a premise in later reasoning. The idea that there is no gap here just is the idea that the principle <strong>Cut</strong> is true.</p>
<p>But while <strong>Cut</strong> seems intuitive in cases like this, Nair argues that it can’t be right in general. (And so we have a duty, one he takes up, to explain why cases like Jack and Jill seem like good reasoning.) For my purposes, it is helpful to divide the putative counterexamples to <strong>Cut</strong> into two categories. I’ll call them <em>monotonic</em> and <em>non-monotonic</em> counterexamples. The categorisation turns on whether <span class="math inline">\(\Gamma \cup \Delta \vDash A\)</span> is true. I’ll call cases where it is true monotonic instances of <strong>Cut</strong>, and cases where it is false non-monotonic instances.</p>
<p>That <strong>Cut</strong> fails in non-monotonic cases is fairly obvious. We can see this with an example that was hackneyed a generation ago.</p>
<blockquote>
<p><span class="math inline">\(\Gamma =\)</span> {Tweety is a bird}<br />
<span class="math inline">\(\Delta =\)</span> {Tweety is a penguin}<br />
<span class="math inline">\(A = B =\)</span> Tweety can fly</p>
</blockquote>
<p>From Tweety is a bird we can rationally infer that Tweety flies. And given that Tweety is a flying penguin, we can infer that she flies. But given that Tweety is a penguin and a bird, we cannot infer this. So principles 1 and 2 in <strong>Cut</strong> are true, but 3 is false. And the same pattern will recur any time <span class="math inline">\(\Delta\)</span> provides a defeater for the link between <span class="math inline">\(\Gamma\)</span> and <span class="math inline">\(A\)</span>.</p>
<p>These cases will matter in what follows, but they are rather different from the monotonic examples. Here is a monotonic example, based on an argument against E=K by Alvin <span class="citation">Goldman (<a href="references.html#ref-Goldman2009">2009</a>)</span>. In many ways the argument against E=K I’m going to give is just a notational variant on Goldman’s, but I think the notation helps see it in context.</p>
<p>Here’s the crucial background assumption for the example. (I’ll come back to how plausible this is after setting the example up.) The nature of <span class="math inline">\(F\)</span>s around here varies, but it varies very very slowly. If we find a pattern in common to all the <span class="math inline">\(F\)</span>s within distance <span class="math inline">\(d\)</span> of here, we can rationally infer that the pattern extends another mile. That’s just boring induction. But we can’t infer that it extends to infinity - this isn’t like working out the mass of an electron. (It’s more like working out the details of the diet of some animal.) Now here is the counterexample.</p>
<blockquote>
<p><span class="math inline">\(\Gamma = \Delta = \{\)</span>All the <span class="math inline">\(F\)</span>s within 3 miles of here are <span class="math inline">\(G\)</span>s.$}$<br />
<span class="math inline">\(A =\)</span> All the <span class="math inline">\(F\)</span>s between 3 and 4 miles from here are <span class="math inline">\(G\)</span>s.<br />
<span class="math inline">\(B =\)</span> All the <span class="math inline">\(F\)</span>s between 4 and 5 miles from here are <span class="math inline">\(G\)</span>s.</p>
</blockquote>
<p>If what I said was right, then this is a counterexample to <strong>Cut</strong>. <span class="math inline">\(\Gamma \vDash A\)</span> is true because it says given evidence about all the <span class="math inline">\(F\)</span>s within 3 miles of here, we can infer that all the <span class="math inline">\(F\)</span>s within 4 miles are like them. And <span class="math inline">\(\{A\} \cup \vDash B\)</span> is true because because it says given evidence about all the <span class="math inline">\(F\)</span>s within 4 miles of here, we can infer that all the <span class="math inline">\(F\)</span>s within 5 miles are like them. But <span class="math inline">\(\Gamma \cup \Delta \vDash A\)</span> is false, because it purports to say that given evidence about the <span class="math inline">\(F\)</span>s within 3 miles of here, we can infer that all the <span class="math inline">\(F\)</span>s within 5 miles are alike. And that’s an inductive bridge too far.</p>
<p>I don’t know if there are instances of <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> where this particular pattern obtains. That is, I don’t know if there are instances of <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> where given a perfect correlation holding within <span class="math inline">\(d\)</span> miles, we can rationally infer it holds within <span class="math inline">\(d + 1\)</span> miles, but not <span class="math inline">\(d + 2\)</span> miles. It seems likely to me that something like this could be right.</p>
<p>But note that what I really need for the argument is independent of how we think spatial distance relates to rational inductive inference. What I really need is that there is some similarity metric such that inductive inference is rational across short jumps in that similarity metric, but not across long jumps. One kind of similarity is physical distance from a salient point. But that’s not the only kind, and rarely the most important kind.</p>
<p>What is really needed to get this argument going is that there is some similarity metric with an ‘inductive margin of inference’. What I mean by an inductive margin of inference is that given that all the <span class="math inline">\(F\)</span>s that differ from a salient point (along this metric) by amount <span class="math inline">\(d\)</span> are <span class="math inline">\(G\)</span>, it is rational to infer that all the <span class="math inline">\(F\)</span>s that differ from that salient point by amount <span class="math inline">\(d + m\)</span> are <span class="math inline">\(G\)</span>, but not that all the <span class="math inline">\(F\)</span>s that differ from that salient point by amount <span class="math inline">\(d + 2m\)</span> are <span class="math inline">\(G\)</span>. And it seems very plausible to me that there are some metrics, and values of <span class="math inline">\(F, G, d, m\)</span> such that that’s true.</p>
<p>For example, given what I know about Miami’s weather, I can infer that it won’t snow there for the next few hundred Christmases. Indeed, I know that. But I can’t know that it won’t snow there for the next few million Christmases. There is some point, and I don’t know what it is, where my inductive knowledge about Miami’s snowfall (or lack thereof) gives out.</p>
<p>But while such cases are plausible, they are weird. And I don’t propose to offer any particular story about them. Here’s what is weird. It will be easier to go back to the case where the metric is physical distance to set this out, but the weirdness will extend to all cases. Imagine we investigate the area within 3 miles of here thoroughly, and find that all the <span class="math inline">\(F\)</span>s are <span class="math inline">\(G\)</span>s. We infer, and now know, that all the <span class="math inline">\(F\)</span>s within 4 miles of here are <span class="math inline">\(G\)</span>s. We keep investigating, and keep observing, and after a while we’ve observed all the <span class="math inline">\(F\)</span>s within 4 miles. And they are all <span class="math inline">\(G\)</span>, as we knew they would be. But now we are in a position to infer that all the <span class="math inline">\(F\)</span>s within 5 miles are <span class="math inline">\(G\)</span>. Observing something that we knew to be true gives us a reason to do something, i.e., make a further inference, that we couldn’t do before. That’s weird, and I’m going to come back in the next section to how it relates to the story I told about knowledge in chapter <a href="knowledge.html#knowledge">4</a>.</p>
<p>But for now I want to note that it undermines the E=K principle. There is a difference between knowing <span class="math inline">\(A\)</span> and being able to use <span class="math inline">\(A\)</span> to support further inductive inferences. It is very natural to call that the difference between knowing <span class="math inline">\(A\)</span> and having <span class="math inline">\(A\)</span> as evidence.</p>
<p>The reasoning that I’ve been criticising violates a principle Jonathan Weisberg calls <strong>No Feedback</strong> <span class="citation">(Weisberg <a href="references.html#ref-Weisberg2010">2010</a>, 533–4)</span>. This principle says that if a conclusion is derived from some premises, plus some intermediary conclusions, then it is only justified if it could, at least in principle, be derived from those premises alone. A natural way to read this is that we have some evidence, and things that we know on the basis of that evidence have a different functional role from the evidence. They can’t do what the evidence itself can do, even if known. This looks like a problem for E=K, as Weisberg himself notes <span class="citation">(<a href="references.html#ref-Weisberg2010">2010</a>, 536)</span>.</p>
<p>The non-monotonic cases where cut elimination fails are also tricky for the E=K theorist, but ultimately not as problematic. Here’s how to bring out the problem, and also ultimately how to solve it.</p>
<p>On day 1, Ankita gets as evidence that Tweety is a flying bird, while Bojan gets as evidence that Tweety is a bird, and infers that Tweety flies. At this stage he knows, as Ankita does, that Tweety flies; this was a perfectly good inference. On day 2, they both get as evidence that Tweety is a penguin. Now Ankita knows something special: Tweety is a flying penguin. But Bojan doesn’t know this. He can no longer infer that Tweety flies, so doesn’t know that Tweety is a flying penguin. And the mystery is to explain what’s happened.</p>
<p>The theorist who rejects E=K has an easy explanation. Ankita and Bojan had different evidence on day 1, though they knew the same things. Then when more evidence was added into their evidence set, they could do different things. That’s the full mystery solved.</p>
<p>The theorist who accepts E=K can’t say just this. They have to say that although Bojan did have as part of his evidence that Tweety flies back on day 1, on day 2 this is no longer part of his evidence. Why is it not? Presumably because it was defeated by the new information. Why was it defeated? The explanation for that can’t be that given the new information, his old evidence didn’t support the belief that Tweety flies. That can’t be right because Tweety’s being a penguin doesn’t get in the way of the ‘inference’ Tweety flies, therefore she flies. Instead the story must be, somehow, that this old evidence was defeated, not just the inference from the evidence to this knowledge.</p>
<p>I’m not sure that the E=K theorist has a good story to tell here. But I’m not sure that they don’t either. Alexander <span class="citation">Bird (<a href="references.html#ref-Bird2004">2004</a>)</span>, in the context of replying to a similar objection, points out that everyone is going to need a theory of evidential defeat. That’s right. Unless evidence is taken to be something that is infallible and indefeasible, we have to have some story for how it can be lost. And I certainly don’t want evidence to be infallible and indefeasible; if that were true we wouldn’t have very much evidence. So the puzzle for the E=K theorist - why does Bojan lose this evidence at this time - is a puzzle for everyone.</p>
<p>This case is still a problem for E=K. The theorist who rejects E=K has, at least in my opinion, a much nicer story to tell about why Bojan doesn’t know that Tweety is a bird. But a problem is not a refutation; and the puzzle this case raises for E=K is a puzzle everyone has to solve.</p>
<p>The real problems for E=K come from the monotonic counter-instances to cut-elimination. If any such cases exist, it looks like we need to distinguish between things the thinker knows by inference, and things they know by observation, in order to assess their inferences. That’s to say, some knowledge will not play the charactistic role of evidence. And that suggests that E=K is false.</p>
</div>
<div id="basic" class="section level2">
<h2><span class="header-section-number">5.7</span> Basic Knowledge and Non-Inferential Knowledge</h2>
<p>It would be natural to conclude from the examples I’ve discussed that evidence is something like non-inferential knowledge. This is very similar to a view defended by Patrick <span class="citation">Maher (<a href="references.html#ref-Maher1996">1996</a>)</span>. And it is, I will argue, close to the right view. But it can’t be exactly right, for reasons Alexander <span class="citation">Bird (<a href="references.html#ref-Bird2004">2004</a>)</span> brings out.</p>
<p>I will argue that evidence is not non-inferential knowledge, but rather basic knowledge. The primary difference between these two notions is that <em>being non-inferential</em> is a diachronic notion, it depends on the causal source of the knowledge, while being basic is a synchronic notion, it depends on how the knowledge is currently supported. In general, non-inferential knowledge will be basic knowledge, and basic knowledge will be non-inferential. But the two notions can come apart, and when they do, the evidence is what is basic, not what is non-inferential.</p>
<p>The following kind of case is central to Bird’s objection to the idea that evidence is non-inferential knowledge. Assume that our inquirer sees that <span class="math inline">\(A\)</span> and rationally infers <span class="math inline">\(B\)</span>. On the view that evidence is non-inferential knowledge, <span class="math inline">\(A\)</span> is evidence but <span class="math inline">\(B\)</span> is not. Now imagine that at some much later time, the inquirer remembers <span class="math inline">\(B\)</span>, but has forgotten that it is based on <span class="math inline">\(A\)</span>. This isn’t necessarily irrational. As <span class="citation">Harman (<a href="references.html#ref-Harman1986">1986</a>)</span> stresses, an obligation to remember our evidence is wildly unrealistic. The inquirer learns <span class="math inline">\(C\)</span>, and infers <span class="math inline">\(B \wedge C\)</span>. This seems perfectly rational. But why is it rational?</p>
<p>If evidence is non-inferential knowledge, then this is a mystery. Since <span class="math inline">\(B\)</span> was inferred, that can’t be the evidence that justifies <span class="math inline">\(B \wedge C\)</span>. So the only other option is that the evidence is the, now forgotten, <span class="math inline">\(A\)</span>. It is puzzling how something that is forgotten can now justify. But a bigger problem is that if <span class="math inline">\(A\)</span> is the inquirer’s evidence, then they should also be able to infer <span class="math inline">\(A \wedge C\)</span>. But this would be an irrational inference.</p>
<p>So I agree with Bird that we can’t identify evidence with non-inferential knowledge, if by that we mean knowledge that was not originally gained through inference. (And what else could it mean?) But a very similar theory of evidence can work. The thing about evidence is that it can play a distinctive role in reasoning, it provides a distinctive kind of reason. In particular, it provides basic reasons.</p>
<p>Evidence stops regresses. That’s why we can say that our fundamental starting points are self-evident. Now there is obviously a controversy about what things are self-evident. I don’t find it particularly likely that claims about the moral rights we were endowed with by our Creator are self-evident. But I do think it is true that a lot of things are self-evident. (Even including, perhaps, that we have moral rights.) And we should take this notion of self-evidence seriously. Our evidence is that knowledge which provides basic reasons.</p>
<p>What is it for a reason to be basic? It isn’t that it was not originally inferred. Something that was once inferred from long forgotten premises may now be a basic reason. Rather, it is something that needs no further reason given as support. (Its support is itself, since it is self-evident.) What makes a reason need further support? I’m an interest-relative epistemologist, so I think this will be a function of the agent’s interests. For example, I think facts reported in a reliable history book are pieces of basic evidence when we are thinking about history, but not when we are thinking about the reliability of that book. But this kind of interest-relativity is essential to the story. What is essential is that evidence provides a reason that does not in turn require more justification.</p>
<p>This picture suggests an odd result about cases of forgotten evidence. There is a much discussed puzzle about forgotten evidence that was set in motion by Gilbert <span class="citation">Harman (<a href="references.html#ref-Harman1986">1986</a>)</span>. He argued that if someone irrationally believes <span class="math inline">\(p\)</span> in the basis of some evidence, then later forgets the evidence but retains the belief, the belief may now be rational. It would not be rational if they remembered both the evidence, and that it was the evidence for <span class="math inline">\(p\)</span>. But, and this is what I want to take away from the case, there is no obligation on thinkers to keep track of why they believe each of the things they do. There is a large literature now on this case; Sinan <span class="citation">Dogramaci (<a href="references.html#ref-Dogramaci2015">2015</a>)</span> both provides a useful guide to the debate and moves it forward by considering what we might aim to achieve by offering one or other evaluation of the believer in this case. The view I’m offering here is, as far as I can tell, completely neutral on this case. But it has something striking to say about a similar case.</p>
<p>In this case the inquirer, call him Jaidyn, believes <span class="math inline">\(p\)</span> for the excellent reason that he read it in a book from a reliable historian <em>H</em>. Six months later, he has forgotten that that’s where he learned that <span class="math inline">\(p\)</span>, though he still believes that <span class="math inline">\(p\)</span>. In a discussion about historians, a friend of Jaidyn’s says that <em>H</em> is really unreliable. Jaidyn is a bit shocked, and literally can’t believe it. And this is for the best since <em>H</em> is in fact reliable, and his friend is suffering from a case of mistaken identity. But he is moved enough by the testimony to not believe that <em>H</em> is reliable, and so he forms a disposition to not believe anything <em>H</em> says without corroboration. Since he doesn’t know that he believes <span class="math inline">\(p\)</span> because <em>H</em> says so, he doesn’t do anything about this belief. What should we say about Jaidyn’s belief that <span class="math inline">\(p\)</span>?</p>
<p>Here’s what I want to say. I don’t claim this is particularly intuitive, but I’m not sure there is anything particularly intuitive; it’s best to just see what a theory says about the case. My theory says that Jaidyn still knows that <span class="math inline">\(p\)</span>. This knowledge was once based on <em>H</em>’s testimony, but it is no longer based on that. Indeed, it is no longer based on anything. Presumably, if Jaidyn is rational, the knowledge will be sensitive to the absence of counter-evidence, or to incoherence with the rest of his world-view. But these are checks and balances in Jaidyn’s doxastic system, they aren’t the basis of the belief. Since the belief is knowledge, and is a basic reason for Jaidyn, it is part of his evidence.</p>
<p>Note three things about that last conclusion. First, this is a case where a piece of inferential knowledge can be in someone’s evidence. By (reasonably) forgetting the source of the knowledge, it converts to being evidence. Second, almost any knowledge could make this jump. Whenever someone has no obligation to remember the source or basis of some knowledge, they can reasonably forget the source, and the basis, and the knowledge will become basic. And then it is evidence. The picture I’m working with is that pieces of knowledge can easily move in and out of one’s evidence set; sometimes all it takes is forgetting where the knowledge came from. But third, if Jaidyn had done better epistemically, and remembered the source, he would no longer know that <span class="math inline">\(p\)</span>.</p>
<p>It is somewhat surprising that knowledge can be dependent on forgetting. Jaidyn knows that <span class="math inline">\(p\)</span>, but if he’d done better at remembering why he believes <span class="math inline">\(p\)</span>, he wouldn’t know it. Still, the knowledge isn’t grounded in forgetting. It’s originally grounded in testimony from an actually reliable source, and Jaidyn did as good a job as he needed to in checking the reliability of the source before accepting the testimony. Now since Jaidyn is finite, he doesn’t have any obligation to remember everything. And it seems odd to demand that Jaidyn adjust his beliefs on the basis of where they are from if he isn’t even required to track where they are from. It would be very odd to say that Jaidyn’s evidence now includes neither <span class="math inline">\(p\)</span> (if it is undermined by his friend’s testimony), nor the fact that someone said that <span class="math inline">\(p\)</span>. That suggests any <span class="math inline">\(p\)</span>-related inferences Jaidyn makes are totally unsupported by his evidence, which doesn’t seem right.</p>
<p>So the picture of evidence as basic knowledge, combined with a plausible theory of when forgetting is permissible, suggests that the forgetful reader knows more than the reader with a better memory. I suspect the same thing will happen in versions of Goldman’s explosive inductive argument. Imagine a thinker observes all the <span class="math inline">\(F\)</span>s within 3 miles, sees they are all <span class="math inline">\(G\)</span>, and rationally infers that all the <span class="math inline">\(F\)</span>s within 4 miles are <span class="math inline">\(G\)</span>. Some time later they retain the belief, the knowledge actually, that all <span class="math inline">\(F\)</span>s within 4 miles are <span class="math inline">\(G\)</span>. But they forget that this was partially inferential knowledge, like Jaidyn forgot the source of his knowledge that <span class="math inline">\(p\)</span>. They then make the seemingly sensible inductive inference that all <span class="math inline">\(F\)</span>s within 5 miles are <span class="math inline">\(G\)</span>. Is this rational, and can it produce knowledge? I think the answer is yes; if they (not unreasonably) forget the source of their knowledge that the <span class="math inline">\(F\)</span>s 3 to 4 miles away are <span class="math inline">\(G\)</span>, then this knowledge becomes basic. If it’s basic, it is evidence. And if it is evidence, it can support one round of inductive reasoning.</p>
<p>I’ve drifted a fair way from discussing interest-relativity. And a lot of what I say here is inessential to defending IRT. So I’ll return to the main plotline with a discussion of how my view of evidence implies a rejection of a key principle in Jeremy Fantl and Matthew McGrath’s theory of knowledge.</p>
</div>
<div id="weakness" class="section level2">
<h2><span class="header-section-number">5.8</span> Epistemic Weakness</h2>
<p>The cases where cut-elimination fails raise a problem for the way that Jeremy Fantl and Matthew McGrath spell out their version of IRT. Here is a principle they rely on in motivating IRT.</p>
<blockquote>
<p>When you know a proposition <span class="math inline">\(p\)</span>, no weaknesses in your epistemic position with respect to <span class="math inline">\(p\)</span>—no weaknesses, that is, in your standing on any truth-relevant dimension with respect to <span class="math inline">\(p\)</span>—stand in the way of <span class="math inline">\(p\)</span> justifying you in having further beliefs. <span class="citation">(Fantl and McGrath <a href="references.html#ref-FantlMcGrath2009">2009</a>, 64)</span></p>
</blockquote>
<p>And a few pages later they offer the following gloss on this principle.</p>
<blockquote>
<p>We offer no analysis of the intuitive notion of ‘standing in the way’. But we do think that, when Y does not obtain, the following counterfactual condition is sufficient for a subject’s position on some dimension d to be something that stands in the way of Y obtaining: whether Y obtains can vary with variations in the subject’s position on d, holding fixed all other factors relevant to whether Y obtains. <span class="citation">(Fantl and McGrath <a href="references.html#ref-FantlMcGrath2009">2009</a>, 67)</span></p>
</blockquote>
<p>This gloss suggests that the difference between knowledge and evidence is something that stands in the way of an inference. The inquirer who knows that nearby <span class="math inline">\(F\)</span>s are <span class="math inline">\(G\)</span>s, but does not know that somewhat distant <span class="math inline">\(F\)</span>s are <span class="math inline">\(G\)</span>s, has many things standing in the way of this knowledge. One of them is, according to this test, that her evidence does not include that all nearby <span class="math inline">\(F\)</span>s are <span class="math inline">\(G\)</span>s. Yet this is something she knows. So a weakness in her epistemic position with respect to the nature of nearby <span class="math inline">\(F\)</span>s, that it is merely evidence and not knowledge, stands in the way of it justifying further beliefs.</p>
<p>The same thing will be true in the monontonic cases of cut-elimination failure. The thinker whose evidence includes <span class="math inline">\(\Gamma \cup \Delta\)</span>, and whose inferential knowledge includes <span class="math inline">\(A\)</span>, cannot infer <span class="math inline">\(B\)</span>. But if they had <span class="math inline">\(A\)</span> as evidence, and not merely as knowledge, then they could infer <span class="math inline">\(B\)</span>. So the weakness in their epistemic position, the gap between evidence and knowledge, stands in the way of something.</p>
<p>I didn’t endorse this principle, but I did endorse very similar principles, and one might wonder whether they are subject to the same criticism. The main principle I endorsed was that if one knows that <span class="math inline">\(p\)</span>, one is immune from criticism for using <span class="math inline">\(p\)</span> on the grounds that <span class="math inline">\(p\)</span> might be false, or is too risky to use. Equivalently, if the use of <span class="math inline">\(p\)</span> in an inference is defective, but <span class="math inline">\(p\)</span> is known, the explanation of why it is defective cannot be that <span class="math inline">\(p\)</span> is too risky. But now won’t the same problem arise? Our inquirer in the monotonic cut-elimination example can’t use <span class="math inline">\(A\)</span> in reasoning to <span class="math inline">\(B\)</span>. If <span class="math inline">\(A\)</span> was part of their evidence, then it wouldn’t be risky, and they would be able to use it. So the risk is part of what makes the use of it mistaken.</p>
<p>I reject the very last step in that criticism. The fact that something is wrong, and that it wouldn’t have been wrong if X, does not mean the non-obtaining of X is part of the ground, or explanation, for why it is wrong. If I break a law, then what I do is illegal. Had the law in question been struck down by a constitutional court, then my action wouldn’t have been illegal. Similarly, if the law had been repealed, my action would not have been illegal. But that doesn’t imply that the ground or explanation of the illegality of my action is the court’s not striking the law down, or the later legislature not repealing the law. That is to put too much into the notion of ground or explanation. No, what makes the act illegal is that a particular piece of legislation was passed, and this act violates it. This explanation is defeasible - it would be defeated if a court or later legislature had stepped in - but it is nonetheless complete.</p>
<p>The same thing is true in the case of knowledge and evidence. Imagine an inquirer who observes all the <span class="math inline">\(F\)</span>s within 3 miles being <span class="math inline">\(G\)</span>, and infers both that all the <span class="math inline">\(F\)</span>s within 4 miles are <span class="math inline">\(G\)</span>, and, therefore, that all the <span class="math inline">\(F\)</span>s within 5 miles are <span class="math inline">\(G\)</span>. The intermediate step is, in a sense, risky. And the later step is bad. And the later step wouldn’t have been bad if the intermediate step hadn’t been risky. But it’s not the riskiness that makes the second inference bad. No, what makes the second inference bad is that it violates Weisberg’s No Feedback principle. That’s what the reasoner can be criticised, not for taking an epistemic risk.</p>
<p>There are two differences then between the core principle I rely on - using reasons that are known provides immunity to criticism for taking epistemic risks - and the principle Fantl and McGrath rely on. I use a concept of epistemic risk where they use a concept of strength of epistemic position. I don’t think these are quite the same thing, but they are clearly similar. But the bigger difference is that they endorse a counterfactual gloss of their principle, and I reject any such counterfactual gloss. I don’t say that the person who uses known <span class="math inline">\(p\)</span> is immune to all criticisms that would have been vitiated had <span class="math inline">\(p\)</span> been less risky. I just say that the risk can’t be the ground of the criticism; something else must be. In some cases, including this one, that ‘something else’ might be correlated with risk. But it must be the explanation.</p>
<p>Of course, this difference between my version of IRT and Fantl and McGrath’s is tiny compared to how much our theories have in common. And indeed, it’s tiny compared to how much my theory simply borrows from theirs. But it’s helpful I think to highlight the differences to understand the choice points within versions of IRT.</p>
</div>
<div id="neta" class="section level2">
<h2><span class="header-section-number">5.9</span> Holism and Defeaters</h2>
<p>The picture of evidence I’ve outlined here grounds a natural response to a nice puzzle case due to Ram <span class="citation">Neta (<a href="references.html#ref-Neta2007">2007</a>)</span>.<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a></p>
<blockquote>
<p>Kate needs to get to Main Street by noon: her life depends upon it. She is desperately searching for Main Street when she comes to an intersection and looks up at the perpendicular street signs at that intersection. One street sign says “State Street” and the perpendicular street sign says “Main Street.” Now, it is a matter of complete indifference to Kate whether she is on State Street–nothing whatsoever depends upon it. <span class="citation">(Neta <a href="references.html#ref-Neta2007">2007</a>, 182)</span></p>
</blockquote>
<p>Neta argues that IRT implies Kate knows that she is on State Street, but does not know that she is on Main Street. And, he suggests, this is intuitively implausible. I think I agree with that intuition, so let’s take it for granted and ask whether IRT has this problematic implication.</p>
<p>Let’s also assume, something I’m not entirely sure is true, that it is not rational for Kate to take the street sign’s word for it. I think this is meant to be implicated by her life depending on it. But I often take actions that my life depends on going by the say so of signs. For example, I turn onto the ramp labelled on ramp, and not the ramp labelled off ramp, without really double checking. And if I was wrong about this it would be a mistake that is often fatal. But maybe Kate has some other way of checking where she is - like a map on a phone in her pocket - and it would be irrational to take the sign for granted and not check that other map.</p>
<p>So what evidence should The Radical Interpreter assign to Kate. It doesn’t seem to be at issue that Kate sees that the signs say State and Main. The big question is whether she can simply take it as evidence that she is on State and Main. That is, do the contents of the sign simply become part of Kate’s evidence? (Assume that the signs are accurate and there is no funny business going on, so it is plausible that the signs contribute this evidence.) There are three natural options.</p>
<ol style="list-style-type: decimal">
<li>Both signs contribute evidence directly to Kate, so her evidence includes that she is on State and that she is on Main.</li>
<li>Neither sign contributes evidence directly to Kate, so her evidence includes what the signs say, but nothing directly about her location.</li>
<li>One sign contributes evidence directly to Kate, but the other does not.</li>
</ol>
<p>Option 1 implies that Kate is rational to not check further whether she is on Main Street. And that’s irrational, so option 1 is out.</p>
<p>Option 3 implies that the signs behave differently, and that The Rational Interpreter will assign them different roles in Kate’s cognitive architecture. But this will be true even though the signs are equally reliable, and Kate’s evidence about their reliability is identical. So Kate treating them differently would be irrational, and The Radical Interpreter does not want to make Kate irrational if it can be helped. So option 3 is out.</p>
<p>That leaves Option 2. Kate’s evidence does not include that she is on State, and does not include that she is on Main. The latter ‘non-inclusion’ is directly explained by pragmatic factors. The former is explained by those factors plus the requirement that Kate’s evidence is what The Radical Interpreter says it is, and The Radical Interpreter’s desire to make Kate rational.</p>
<p>So Kate’s evidence doesn’t distinguish between the streets. It does, however, include that the signs say she is on State and that she is on Main. Could she be entitled in inferring that she is on State, but not that she is on Main?</p>
<p>It is hard to see how this could be so. Street signs are hardly basic epistemic sources. They are the kind of evidence we should be ‘conservative’ about in the sense of <span class="citation">Pryor (<a href="references.html#ref-Pryor2004">2004</a>)</span>. We should only use them if we antecedently believe they are correct. So for Kate to believe she’s on State, she’d have to believe the street signs she can see are correct. If not, she’d incoherently be relying on a source she doesn’t trust, even though it is not a basic source. But if she believes the street signs are correct, she’d believe she was on Main, and that would lead to practical irrationality. So there’s no way to coherently add the belief that she’s
on State Street to her stock of beliefs. So she doesn’t know, and can’t know, that she’s either on State or on Main. This is, in a roundabout way, due to the practical situation Kate faces.</p>
<p>Neta thinks that the best way for IRT to handle this case is to say that the high stakes associated with the proposition that Kate is on Main Street imply that certain methods of belief formation do not produce knowledge. And he argues, plausibly, that such a restriction will lead to implausibly sceptical results. What to say about this objection turns on how we understand what a ‘method’ is. If methods are individuated very finely, like <em>Trust street signs right here</em>, then it’s plausible that Kate should restrict what methods she uses, but implausible that this is badly sceptical. If methods are individuated very coarsely, like <em>Trust written testimomy</em>, then it’s plausible that this is badly sceptical, but implausible that Kate should give up on methods this general. I can rationally treat some parts of a book as providing direct evidence about the world, and other, more speculative, parts as providing direct evidence about what the author says, and hence indirect evidence about the world. Similarly, Kate can treat these street signs as indirect evidence about her location, while still treating other signs around her as providing direct evidence. So there is no sceptical threat here.</p>
<p>But while the case doesn’t show IRT is false, it does tell us something interesting about the implications of IRT. When a practical consideration defeats a claim to know that <span class="math inline">\(p\)</span>, it will often also knock out nearby knowledge claims. Some of these are obvious, like that the practical consideration defeats the claim to know <span class="math inline">\(0=0 \rightarrow p\)</span>. But some of these are more indirect. When the inquirer knows what her evidence is, and knows that she has just the same evidence for <span class="math inline">\(q\)</span> as for <span class="math inline">\(p\)</span>, then if a practical consideration defeats a claim to know <span class="math inline">\(p\)</span>, it also defeats a claim to know <span class="math inline">\(q\)</span>. In practice, this makes IRT a somewhat more sceptical theory than it may have first appeared. It’s not so sceptical as to be implausible, but it’s more sceptical than is immediately obvious. This kind of result, where IRT ends up being somewhat sceptical but not implausibly so, will be a familiar theme over the remaining chapters.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="25">
<li id="fn25"><p>The first five sections of this chapter are based on my <span class="citation">(<a href="references.html#ref-Weatherson2018-WEAIEA-2">2018</a>)</span>.<a href="evidence.html#fnref25" class="footnote-back">↩</a></p></li>
<li id="fn26"><p>In these games, Row chooses a row, and Column chooses a column, and that determines the cell that is the outcome of the game. The cells include two numbers. The first is Row’s payout, and the second is Column’s. The games are non-competitive; the players are simply trying to maximise their own returns, not maximise the difference between their return and the other player’s return.</p>
<p>The idea of treating Newcomb’s Problem, and similar decision-theoretic problems, as games traces back to <span class="citation">Harper (<a href="references.html#ref-Harper1986">1986</a>)</span>.<a href="evidence.html#fnref26" class="footnote-back">↩</a></p></li>
<li id="fn27"><p>A Nash equilibrium is an outcome of the game where every player does as well as they can given the moves of the other players. Equivalently, it is an outcome where no player can improve their payout by unilaterally defecting from the equilibrium.<a href="evidence.html#fnref27" class="footnote-back">↩</a></p></li>
<li id="fn28"><p>Strictly speaking, we can’t rule out various mixed strategies when the signal is precisely 2, but this makes little difference, since that occurs with probability 0.<a href="evidence.html#fnref28" class="footnote-back">↩</a></p></li>
<li id="fn29"><p>This section draws on section 5 of my <span class="citation">(<a href="references.html#ref-Weatherson2011-WEADIR">2011</a>)</span>.<a href="evidence.html#fnref29" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="knowledge.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ratbel.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Kahis.pdf", "Kahis.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
