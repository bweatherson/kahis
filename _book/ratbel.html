<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 6 Rational Belief | Knowledge</title>
  <meta name="description" content="A defence of an interest-relative theory of knowledge">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 6 Rational Belief | Knowledge" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://brian.weatherson.org/kahis/" />
  <meta property="og:image" content="https://brian.weatherson.org/kahis/tree.jpg" />
  <meta property="og:description" content="A defence of an interest-relative theory of knowledge" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Rational Belief | Knowledge" />
  
  <meta name="twitter:description" content="A defence of an interest-relative theory of knowledge" />
  <meta name="twitter:image" content="https://brian.weatherson.org/kahis/tree.jpg" />

<meta name="author" content="Brian Weatherson">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="evidence.html">
<link rel="next" href="ties.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="kahis.html">Knowledge: A Human Interest Story</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Front Matter</a></li>
<li class="chapter" data-level="1" data-path="prologue.html"><a href="prologue.html"><i class="fa fa-check"></i><b>1</b> Prologue</a></li>
<li class="chapter" data-level="2" data-path="interests.html"><a href="interests.html"><i class="fa fa-check"></i><b>2</b> Interests in Epistemology</a><ul>
<li class="chapter" data-level="2.1" data-path="interests.html"><a href="interests.html#redblue"><i class="fa fa-check"></i><b>2.1</b> Red or Blue?</a></li>
<li class="chapter" data-level="2.2" data-path="interests.html"><a href="interests.html#fourfamilies"><i class="fa fa-check"></i><b>2.2</b> Four Families</a></li>
<li class="chapter" data-level="2.3" data-path="interests.html"><a href="interests.html#orthodox"><i class="fa fa-check"></i><b>2.3</b> Against Orthodoxy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="interests.html"><a href="interests.html#orthodoxmoore"><i class="fa fa-check"></i><b>2.3.1</b> Moore’s Paradox</a></li>
<li class="chapter" data-level="2.3.2" data-path="interests.html"><a href="interests.html#superknow"><i class="fa fa-check"></i><b>2.3.2</b> Super Knowledge to the Rescue?</a></li>
<li class="chapter" data-level="2.3.3" data-path="interests.html"><a href="interests.html#probrescue"><i class="fa fa-check"></i><b>2.3.3</b> Rational Credences to the Rescue?</a></li>
<li class="chapter" data-level="2.3.4" data-path="interests.html"><a href="interests.html#orthodoxevidence"><i class="fa fa-check"></i><b>2.3.4</b> Evidential Probability</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="interests.html"><a href="interests.html#oddsandstakes"><i class="fa fa-check"></i><b>2.4</b> Odds and Stakes</a></li>
<li class="chapter" data-level="2.5" data-path="interests.html"><a href="interests.html#whatinterests"><i class="fa fa-check"></i><b>2.5</b> Theoretical Interests Matter</a></li>
<li class="chapter" data-level="2.6" data-path="interests.html"><a href="interests.html#global"><i class="fa fa-check"></i><b>2.6</b> Global Interest Relativity</a></li>
<li class="chapter" data-level="2.7" data-path="interests.html"><a href="interests.html#neutrality"><i class="fa fa-check"></i><b>2.7</b> Neutrality</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="belief.html"><a href="belief.html"><i class="fa fa-check"></i><b>3</b> Belief</a><ul>
<li class="chapter" data-level="3.1" data-path="belief.html"><a href="belief.html#beliefsinterests"><i class="fa fa-check"></i><b>3.1</b> Beliefs and Interests</a></li>
<li class="chapter" data-level="3.2" data-path="belief.html"><a href="belief.html#mapslegends"><i class="fa fa-check"></i><b>3.2</b> Maps and Legends</a></li>
<li class="chapter" data-level="3.3" data-path="belief.html"><a href="belief.html#given"><i class="fa fa-check"></i><b>3.3</b> Taking As Given</a></li>
<li class="chapter" data-level="3.4" data-path="belief.html"><a href="belief.html#block"><i class="fa fa-check"></i><b>3.4</b> Blocking Belief</a></li>
<li class="chapter" data-level="3.5" data-path="belief.html"><a href="belief.html#questions"><i class="fa fa-check"></i><b>3.5</b> Questions and Conditional Questions</a></li>
<li class="chapter" data-level="3.6" data-path="belief.html"><a href="belief.html#changes"><i class="fa fa-check"></i><b>3.6</b> A Million Dead End Streets</a><ul>
<li class="chapter" data-level="3.6.1" data-path="belief.html"><a href="belief.html#mecorrect"><i class="fa fa-check"></i><b>3.6.1</b> Correctness</a></li>
<li class="chapter" data-level="3.6.2" data-path="belief.html"><a href="belief.html#meimpractical"><i class="fa fa-check"></i><b>3.6.2</b> Impractical Propositions</a></li>
<li class="chapter" data-level="3.6.3" data-path="belief.html"><a href="belief.html#threeway"><i class="fa fa-check"></i><b>3.6.3</b> Choices with More Than Two Options</a></li>
<li class="chapter" data-level="3.6.4" data-path="belief.html"><a href="belief.html#meties"><i class="fa fa-check"></i><b>3.6.4</b> Hard Times and Close Calls</a></li>
<li class="chapter" data-level="3.6.5" data-path="belief.html"><a href="belief.html#modalupdate"><i class="fa fa-check"></i><b>3.6.5</b> Updates and Modals</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="belief.html"><a href="belief.html#nearby-views"><i class="fa fa-check"></i><b>3.7</b> Nearby Views</a><ul>
<li class="chapter" data-level="3.7.1" data-path="belief.html"><a href="belief.html#ganson"><i class="fa fa-check"></i><b>3.7.1</b> Ganson’s Theory</a></li>
<li class="chapter" data-level="3.7.2" data-path="belief.html"><a href="belief.html#usc"><i class="fa fa-check"></i><b>3.7.2</b> Ross and Schroeder’s Theory</a></li>
<li class="chapter" data-level="3.7.3" data-path="belief.html"><a href="belief.html#leitgeb"><i class="fa fa-check"></i><b>3.7.3</b> Leitgeb’s Stability Theory</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="belief.html"><a href="belief.html#weakbelief"><i class="fa fa-check"></i><b>3.8</b> Weak Belief</a></li>
<li class="chapter" data-level="3.9" data-path="belief.html"><a href="belief.html#probone"><i class="fa fa-check"></i><b>3.9</b> Belief as Probability One</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="knowledge.html"><a href="knowledge.html"><i class="fa fa-check"></i><b>4</b> Knowledge</a><ul>
<li class="chapter" data-level="4.1" data-path="knowledge.html"><a href="knowledge.html#structure"><i class="fa fa-check"></i><b>4.1</b> Knowledge and Practical Interests</a></li>
<li class="chapter" data-level="4.2" data-path="knowledge.html"><a href="knowledge.html#theoreticalknowledge"><i class="fa fa-check"></i><b>4.2</b> Theoretical Knowledge</a></li>
<li class="chapter" data-level="4.3" data-path="knowledge.html"><a href="knowledge.html#knowledge-and-closure"><i class="fa fa-check"></i><b>4.3</b> Knowledge and Closure</a><ul>
<li class="chapter" data-level="4.3.1" data-path="knowledge.html"><a href="knowledge.html#andelim"><i class="fa fa-check"></i><b>4.3.1</b> Single Premise Closure</a></li>
<li class="chapter" data-level="4.3.2" data-path="knowledge.html"><a href="knowledge.html#andintro"><i class="fa fa-check"></i><b>4.3.2</b> Multiple Premise Closure</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="knowledge.html"><a href="knowledge.html#closuresummary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="evidence.html"><a href="evidence.html"><i class="fa fa-check"></i><b>5</b> Evidence</a><ul>
<li class="chapter" data-level="5.1" data-path="evidence.html"><a href="evidence.html#evpuzzle"><i class="fa fa-check"></i><b>5.1</b> A Puzzle About Evidence</a></li>
<li class="chapter" data-level="5.2" data-path="evidence.html"><a href="evidence.html#simplesolution"><i class="fa fa-check"></i><b>5.2</b> A Simple, but Incomplete, Solution</a></li>
<li class="chapter" data-level="5.3" data-path="evidence.html"><a href="evidence.html#radicalinterpretation"><i class="fa fa-check"></i><b>5.3</b> The Radical Interpreter</a></li>
<li class="chapter" data-level="5.4" data-path="evidence.html"><a href="evidence.html#globalgame"><i class="fa fa-check"></i><b>5.4</b> Motivating Risk-Dominant Equilibria</a><ul>
<li class="chapter" data-level="5.4.1" data-path="evidence.html"><a href="evidence.html#cvdproof"><i class="fa fa-check"></i><b>5.4.1</b> The Dominance Argument for Risk-Dominant Equilibria</a></li>
<li class="chapter" data-level="5.4.2" data-path="evidence.html"><a href="evidence.html#perfectri"><i class="fa fa-check"></i><b>5.4.2</b> Making One Signal Precise</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="evidence.html"><a href="evidence.html#evsolution"><i class="fa fa-check"></i><b>5.5</b> Objections and Replies</a></li>
<li class="chapter" data-level="5.6" data-path="evidence.html"><a href="evidence.html#cutelim"><i class="fa fa-check"></i><b>5.6</b> Evidence, Knowledge and Cut-Elimination</a></li>
<li class="chapter" data-level="5.7" data-path="evidence.html"><a href="evidence.html#basic"><i class="fa fa-check"></i><b>5.7</b> Basic Knowledge and Non-Inferential Knowledge</a></li>
<li class="chapter" data-level="5.8" data-path="evidence.html"><a href="evidence.html#weakness"><i class="fa fa-check"></i><b>5.8</b> Epistemic Weakness</a></li>
<li class="chapter" data-level="5.9" data-path="evidence.html"><a href="evidence.html#neta"><i class="fa fa-check"></i><b>5.9</b> Holism and Defeaters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ratbel.html"><a href="ratbel.html"><i class="fa fa-check"></i><b>6</b> Rational Belief</a><ul>
<li class="chapter" data-level="6.1" data-path="ratbel.html"><a href="ratbel.html#atomism"><i class="fa fa-check"></i><b>6.1</b> Atomism about Rational Belief</a></li>
<li class="chapter" data-level="6.2" data-path="ratbel.html"><a href="ratbel.html#lockecoin"><i class="fa fa-check"></i><b>6.2</b> Coin Puzzles</a></li>
<li class="chapter" data-level="6.3" data-path="ratbel.html"><a href="ratbel.html#lockegames"><i class="fa fa-check"></i><b>6.3</b> Playing Games</a></li>
<li class="chapter" data-level="6.4" data-path="ratbel.html"><a href="ratbel.html#lockepuzzles"><i class="fa fa-check"></i><b>6.4</b> Puzzles for Lockeans</a><ul>
<li class="chapter" data-level="6.4.1" data-path="ratbel.html"><a href="ratbel.html#lockearb"><i class="fa fa-check"></i><b>6.4.1</b> Arbitrariness</a></li>
<li class="chapter" data-level="6.4.2" data-path="ratbel.html"><a href="ratbel.html#lockecorrect"><i class="fa fa-check"></i><b>6.4.2</b> Correctness</a></li>
<li class="chapter" data-level="6.4.3" data-path="ratbel.html"><a href="ratbel.html#lockemoore"><i class="fa fa-check"></i><b>6.4.3</b> Moorean Paradoxes</a></li>
<li class="chapter" data-level="6.4.4" data-path="ratbel.html"><a href="ratbel.html#closure"><i class="fa fa-check"></i><b>6.4.4</b> Closure and the Lockean Theory</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ratbel.html"><a href="ratbel.html#solving"><i class="fa fa-check"></i><b>6.5</b> Solving the Challenges</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ratbel.html"><a href="ratbel.html#coins"><i class="fa fa-check"></i><b>6.5.1</b> Coins</a></li>
<li class="chapter" data-level="6.5.2" data-path="ratbel.html"><a href="ratbel.html#games"><i class="fa fa-check"></i><b>6.5.2</b> Games</a></li>
<li class="chapter" data-level="6.5.3" data-path="ratbel.html"><a href="ratbel.html#arbitrariness"><i class="fa fa-check"></i><b>6.5.3</b> Arbitrariness</a></li>
<li class="chapter" data-level="6.5.4" data-path="ratbel.html"><a href="ratbel.html#moore"><i class="fa fa-check"></i><b>6.5.4</b> Moore</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ties.html"><a href="ties.html"><i class="fa fa-check"></i><b>7</b> Hard Choices</a><ul>
<li class="chapter" data-level="7.1" data-path="ties.html"><a href="ties.html#frankielee"><i class="fa fa-check"></i><b>7.1</b> An Example</a></li>
<li class="chapter" data-level="7.2" data-path="ties.html"><a href="ties.html#tiesresponse"><i class="fa fa-check"></i><b>7.2</b> Responding to the Challenge, Quickly</a></li>
<li class="chapter" data-level="7.3" data-path="ties.html"><a href="ties.html#backearth"><i class="fa fa-check"></i><b>7.3</b> Back to Earth</a></li>
<li class="chapter" data-level="7.4" data-path="ties.html"><a href="ties.html#supermarketquestions"><i class="fa fa-check"></i><b>7.4</b> I Have Questions</a></li>
<li class="chapter" data-level="7.5" data-path="ties.html"><a href="ties.html#satisfied"><i class="fa fa-check"></i><b>7.5</b> You’ll Never Be Satisfied (If You Try to Maximise)</a></li>
<li class="chapter" data-level="7.6" data-path="ties.html"><a href="ties.html#deliberationcosts"><i class="fa fa-check"></i><b>7.6</b> Deliberation Costs and Infinite Regresses</a></li>
<li class="chapter" data-level="7.7" data-path="ties.html"><a href="ties.html#ignorancebliss"><i class="fa fa-check"></i><b>7.7</b> Ignorance is Bliss</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>8</b> To Be Written</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Made with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Knowledge</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ratbel" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Rational Belief</h1>
<p>This chapter discusses the role of rational belief in the version of IRT that I defend. It starts by noting that the theory allows for a new kind of Dharmottara case, where a rational, true belief is not actually knowledge. And I argue that it is a good thing it allows this, for once we see the kind of case in question, it is plausible that it is a Dharmottara case. Then offer two arguments, one of them due to Timothy Williamson and the other novel, for the conclusion that it is possible to have rational credence 1 in a proposition without fully believing it. This undermines two prominent theories of the relationship between credence and full belief: that full belief is credence one, and that full belief is credence above some interest-invariant threshold. I’m going to focus primarily on the versions of those views that say that rational full belief is a matter of having rational credences that meet some property. The arguments that credence one is sometimes insufficient for belief are already problems for those views, but they each have a number of independent problems. I’ll end the chapter by noting how the view of rational belief that comes out of IRT is immune to those problems.</p>
<div id="atomism" class="section level2">
<h2><span class="header-section-number">6.1</span> Atomism about Rational Belief</h2>
<p>In chapter <a href="belief.html#belief">3</a> I argued for two individually necessary and jointly sufficient conditions for belief.<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a> They are</p>
<ol style="list-style-type: decimal">
<li>In some possible decision problem, <span class="math inline">\(p\)</span> is taken for granted.</li>
<li>For every question the agent is interested in, the agent answers the question the same way (i.e., giving the same answer for the same reasons) whether the question is asked unconditionally or conditional on <span class="math inline">\(p\)</span>.</li>
</ol>
<p>At this point one might think that offering a theory of rational belief would be easy. It is rational to believe <span class="math inline">\(p\)</span> just in case it is rational to satisfy these conditions. Unfortunately, this nice thought can’t be right. It can be irrational to satisfy these conditions while rationally believing <span class="math inline">\(p\)</span>.</p>
<p>Coraline is like Anisa and Chamari, in that she has read a reliable book saying that the Battle of Agincourt was in 1415. And she now believes that the Battle of Agincourt was indeed in 1415, for the very good reason that she read it in a reliable book.</p>
<p>In front of her is a sealed envelope, and inside the envelope a number is written on a slip of paper. Let <span class="math inline">\(X\)</span> denote that number, non-rigidly. (So when I say Coraline believes <span class="math inline">\(X = x\)</span>, it means she believes that the number written on the slip of paper is <span class="math inline">\(x\)</span>, where <span class="math inline">\(x\)</span> rigidly denotes some number.) Coraline is offered the following bet:</p>
<ul>
<li>If she declines the bet, nothing happens.</li>
<li>If she accepts the bet, and the Battle of Agincourt was in 1415, she wins $1.</li>
<li>If she accepts the bet, and the Battle of Agincourt was not in 1415, she loses <span class="math inline">\(X\)</span> dollars.</li>
</ul>
<p>For some reason, Coraline is convinced that <span class="math inline">\(X = 10\)</span>. This is very strange, since she was shown the slip of paper just a few minutes ago, and it clearly showed that <span class="math inline">\(X = 1,000,000,000\)</span>. Coraline wouldn’t bet on when the Battle of Agincourt was at odds of a billion to one. But she would take that bet at 10 to 1, which is what she thinks she is faced with. Indeed, she doesn’t even conceptualise it as a bet; it’s a free dollar she thinks. Right now, she is disposed to treat the date of the battle as a given. She is disposed to lose this disposition should a very long odds bet appear to depend on it. But she doesn’t believe she is facing such a bet.</p>
<p>So Coraline accepts the bet; she thinks it is a free dollar. And that’s when the battle took place, so she wins the dollar. All’s well that end’s well. But it was a really wildly irrational bet to take. You shouldn’t bet at those odds on something you remember from a history book. Neither memory nor history books are that reliable. Coraline was not rational to treat the questions <em>Should I take this bet?</em>, and <em>Conditional on the Battle of Agincourt being in 1415, should I take this bet?</em> the same way. Her treating them the same way was fortunate - she won a dollar - but irrational.</p>
<p>Yet it seems odd to say that Coraline’s belief about the Battle of Agincourt was irrational. What was irrational was her belief about the envelope, not her belief about the battle. To say that a particular disposition was irrational is to make a holistic assessment of the person with the disposition. But whether a belief is rational or not is, relatively speaking, atomistic.</p>
<p>That suggests the following condition on rational belief.</p>
<blockquote>
<p>S’s belief that <span class="math inline">\(p\)</span> is irrational if</p>
<ol style="list-style-type: decimal">
<li>S irrationally has one of the dispositions that is characteristic of belief that <span class="math inline">\(p\)</span>; and</li>
<li>What explains S having a disposition that is irrational in that way is her attitudes towards <span class="math inline">\(p\)</span>, not (solely) her attitudes towards other propositions, or her skills in practical reasoning.</li>
</ol>
</blockquote>
<p>In “Knowledge, Bets and Interests” I gave a similar theory about these cases - I said that S’s belief that <span class="math inline">\(p\)</span> was irrational if the irrational dispositions were caused by an irrationally high credence in <span class="math inline">\(p\)</span>. I mean this account to be ever so slightly more general. I’ll come back to that below, because first I want to spell out the second clause.</p>
<p>Intuitively, Coraline’s irrational acceptance of the belief is explained by her (irrational) belief about <span class="math inline">\(X\)</span>, not her (rational) belief about the Battle of Agincourt. We can take this notion of explanation as a primitive if we like; it’s in no worse philosophical shape than other notions we take as a primitive. But it is possible to spell it out a little more.</p>
<p>Coraline has a pattern of irrational dispositions related to the envelope. If you offer her $50 or <span class="math inline">\(X\)</span> dollars, she’ll take the $50. If you change the bet so it isn’t about Agincourt, but is instead about any other thing she has excellent but not quite conclusive evidence for, she’ll still take the bet.</p>
<p>On the other hand, she does not have a pattern of irrational dispositions related to the Battle of Agincourt. She has this one, but if you change the payouts so they are not related to this particular envelope, then for all we have said so far, she won’t do anything irrational.</p>
<p>That difference in patterns matters. We know that it’s the beliefs about the envelope, and not the beliefs about the battle, that are explanatory because of this pattern. We could try and create a reductive analysis of explanation in clause 2 using facts about patterns, like the way Lewis tries to create a reductive analysis of causation using similar facts about patterns in “Causation as Influence” <span class="citation">(Lewis <a href="references.html#ref-Lewis2004a">2004</a>)</span>. But doing so would invariably run up against edge cases that would be more trouble to resolve than they are worth.</p>
<p>That’s because there are ever so many ways in which someone could have an irrational disposition about any particular case. We can imagine Coraline having a rational belief about the envelope, but still taking the bet because of any of the following reasons:</p>
<ul>
<li>It has been her life goal to lose a billion dollars in a day, so taking the bet strictly dominates not taking it.</li>
<li>She believes (irrationally) that anyone who loses a billion dollars in a day goes to heaven, and she (rationally) values heaven above any monetary amount.</li>
<li>She consistently makes reasoning errors about billions, so the prospect of losing a billion dollars rarely triggers an awareness that she should reconsider things she normally takes for granted.</li>
</ul>
<p>The last one of these is especially interesting. The picture of rational agency I have in the background here owes a lot to the notion of epistemic vigilance, as developed by Dan Sperber and co-authors <span class="citation">(Sperber et al. <a href="references.html#ref-SperberEtAl2010">2010</a>)</span>. The rational agent will have all these beliefs in their head that they will drop when the costs of being wrong about them are too high, or the costs of re-opening inquiry into them are too low. They can’t reason, at least in any conscious way, about whether to drop these beliefs, because to do that is, in some sense, to call the belief into doubt. And what’s at issue is whether they should call the belief into doubt. So what they need is some kind of disposition to replace a belief that <span class="math inline">\(p\)</span> with an attitude that <span class="math inline">\(p\)</span> is highly probable, and this disposition should correlate with the cases where taking <span class="math inline">\(p\)</span> for granted will not maximise expected utility. This disposition will be a kind of vigilance. As Sperber et al show, we need some notion of vigilance to explain a lot of different aspects of epistemic evaluation, and I think it can be usefully pressed into service here.<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a></p>
<p>But if you need something like vigilance, then you have to allow that vigilance might fail. And maybe some irrational dispositions can be traced to that failure, and not to any propositional attitude the decider has. For example, if Coraline systematically fails to be vigilant when exactly one billion dollars is at stake, then we might want to say that her belief in <span class="math inline">\(p\)</span> is still rational, and she is practically, rather than theoretically, irrational. (Why could this happen? Perhaps she thinks of Dr Evil every time she hears the phrase “One billion dollars”, and this distractor prevents her normally reliable skill of being vigilant from kicking in.)</p>
<p>If one tries to turn the vague talk of patterns of bets involving one proposition or another into a reductive analysis of when one particular belief is irrational, one will inevitably run into hard cases where a decider has multiple failures. We can’t say that what makes Coraline’s belief about the envelope, and not her belief about the battle, irrational is that if you replaced the envelope, she would invariably have a rational disposition. After all, she might have some other irrational belief about whatever we replace the envelope with. Or she might have some failure of practical reasoning, like a vigilance failure. Any kind of universal claim, like that it is only bets about the envelope that she gets wrong, won’t do the job we need.</p>
<p>In “Knowledge, Bets and Interests”, I tried to use the machinery of credences to make something like this point. The idea was that Coraline’s belief in <span class="math inline">\(p\)</span> was rational because her belief just was her high credence in <span class="math inline">\(p\)</span>, and that credence was rational. I still think that’s approximately right, but it can’t be the full story.</p>
<p>For one thing, beliefs and credences aren’t as closely connected metaphysically as this suggests. To have a belief in <span class="math inline">\(p\)</span> isn’t just to have a high credence, it’s to be disposed to let <span class="math inline">\(p\)</span> play a certain role. (This will become important in the next two sections.)</p>
<p>For another thing, it is hard to identify precisely what a credence is in the case of an irrational agent. The usual ways we identify credences, via betting dispositions or representation theorems, assume away all irrationality. But an irrational person might still have some rational beliefs.</p>
<p>Attempts to generalise accounts of credences so that they cover the irrational person will end up saying something like what I’ve said about patterns. What it is to have credence 0.6 in <span class="math inline">\(p\)</span> isn’t to have a set of preferences that satisfies all the presuppositions of such and such a representation theorem, and that theorem to say that one can be represented by a probability function <span class="math inline">\(\Pr\)</span> and a utility function <span class="math inline">\(U\)</span> such that <span class="math inline">\(\Pr(p) = 0.6\)</span>. That can’t be right because some people will, intuitively, have credence about 0.6 in <span class="math inline">\(p\)</span> while not uniformly conforming to these constraints. But what makes them intuitive cases of credence roughly 0.6 in <span class="math inline">\(p\)</span> is that generally they behave like the perfectly rational person with credence 0.6 in <span class="math inline">\(p\)</span>, and most of the exceptions are explained by other features of their cognitive system other than their attitude to <span class="math inline">\(p\)</span>.</p>
<p>In other words, we don’t have a full theory of credences for irrational beings right now, and when we get one, it won’t be much simpler than the theory in terms of patterns and explanations I’ve offered here. So it’s best for now to just understand belief in terms of a pattern of dispositions, and say that the belief is rational just in case that pattern is rational. And that might mean that on some occasions <span class="math inline">\(p\)</span>-related activity is irrational even though the pattern of <span class="math inline">\(p\)</span>-related activity is a rational pattern. Any given action, like any thing whatsoever, can be classified in any number of ways. What matters here is what explains the irrationality of a particular irrational act, and that will be a matter of which patterns of irrational dispositions the actor has.</p>
<p>However we explain Coraline’s belief, the upshot is that she has a rational, true belief that is not knowledge. This is a novel kind of Dharmottara case. (Or Gettier case for folks who prefer that nomenclature.) It’s not the exact kind of case that Dharmottara originally described. Coraline doesn’t infer anything about the Battle of Agincourt from a false belief. But it’s a mistake to think that the class of rational, true beliefs that are not knowledge form a natural kind. In general, negatively defined classes are disjunctive; there are ever so many ways to not have a property. An upshot of this discussion of Coraline is that there is one more kind of Dharmottara case than was previously recognised. But as, for example, <span class="citation">Williamson (<a href="references.html#ref-WilliamsonLofoten">2013</a>)</span> and <span class="citation">Nagel (<a href="references.html#ref-Nagel2013-Williamson">2013</a>)</span> have shown, we have independent reason for thinking this is a very disjunctive class. So the fact that it doesn’t look anything like Dharmottara’s example shouldn’t make us doubt it is a rational, true belief that is not knowledge.</p>
</div>
<div id="lockecoin" class="section level2">
<h2><span class="header-section-number">6.2</span> Coin Puzzles</h2>
<p>So rational belief is not identical to rationally having the dispositions that constitute belief. But nor is rational belief a matter of rational high credence. I’ll offer two arguments for this, one in this section and one in the next.</p>
<p>The point of these two sections is as much metaphysical as it is normative. I’m interested in arguing against the ‘Lockean’ thesis that to believe <span class="math inline">\(p\)</span> just is to have a high credence in <span class="math inline">\(p\)</span>. Normally, this threshold of high enough belief for credence is taken to be interest-invariant, so this is a rival to IRT. But there is some variation in the literature about whether the phrase <em>The Lockean Thesis</em> refers to a metaphysical claim, belief is high credence, or a normative claim, rational belief is rational high credence. Since everyone who accepts the metaphysical claim also accepts the normative claim, and usually takes it to be a consequence of the metaphysical claim, arguing against the normative claim is a way of arguing against the metaphysical claim.</p>
<p>The first puzzle for this Lockean view comes from an argument that Timothy <span class="citation">Williamson (<a href="references.html#ref-Williamson2007">2007</a>)</span> made about certain kinds of infinitary events. A fair coin is about to be tossed. It will be tossed repeatedly until it lands heads twice. The coin tosses will get faster and faster, so even if there is an infinite sequence of tosses, it will finish in a finite time. (This isn’t physically realistic, but this need not detain us. All that will really matter for the example is that someone could believe this will happen, and that’s physically possible.)</p>
<p>Consider the following three propositions</p>
<ol style="list-style-type: upper-alpha">
<li>At least one of the coin tosses will land either heads or tails.</li>
<li>At least one of the coin tosses will land heads.</li>
<li>At least one of the coin tosses after the first toss will land heads.</li>
</ol>
<p>So if the first coin toss lands heads, and the rest land tails, (B) is true and (C) is false.</p>
<p>Now consider a few versions of the Red-Blue game (perhaps played by someone who takes this to be a realistic scenario). In the first instance, the red sentence says that (B) is true, and the blue sentence says that (C) is true. In the second instance, the red sentence says that (A) is true, and the blue sentence says that (B) is true. In both cases, it seems that the unique rational play is Red-True. But it’s really hard to explain this in a way consistent with the Lockean view.</p>
<p>Williamson argues that we have good reason to believe that the probability of all three sentences is 1. For (B) to be false requires (C) to be false, and for one more coin flip to land tails. So the probability that (B) is false is one-half the probability that (C) is false. But we also have good reason to believe that the probabilities of (B) and (C) are the same. In both cases, they are false if a countable infinity of coin flips lands tails. Assuming that the probability of some sequence having a property supervenes on the probabilities of individual events in that sequence (conditional, perhaps, on other events in the sequence), it follows that the probabilities of (B) and (C) are identical. And the only way for the probability that (B) is false to be half the probability that (C) is false, while (B) and (C) have the same probability, is for both of them to have probability 1. Since the probability of (A) is at least as high as the probability of (B) (since it is true whenever (B) is true, but not conversely), it follows that the probability of all three is 1.</p>
<p>But since betting on (A) weakly dominates betting on (B), and betting on (B) weakly dominates betting on (C), we shouldn’t have the same attitudes towards bets on these three propositions. Given a choice between betting on (B) and betting on (C), we should prefer to bet on (B) since there is no way that could make us worse off, and some way it could make us better off. Given that choice, we should prefer to bet on (B) (i.e., play Red-True when (B) and (C) are expressed by the red and blue sentences), because it might be that (B) is true and (C) false.</p>
<p>Assume (something the Lockean may not wish to acknowledge) that to say something might be the case is to reject believing its negation. Then a rational person faced with these choices will not believe <em>Either (B) is false or (C) is true</em>; they will take its negation to be possible. But that proposition is at least as probable as (C), so it too has probability 1. So probability 1 does not suffice for belief.</p>
<p>This is a real problem for the Lockean - no probability suffices for belief, not even probability 1. It’s also, of course, a problem for the view that belief is probability 1 that I discussed back in section <a href="belief.html#probone">3.9</a>. The next section discusses another way in which these two views are problematic.</p>
</div>
<div id="lockegames" class="section level2">
<h2><span class="header-section-number">6.3</span> Playing Games</h2>
<p>Some people might be nervous about resting too much weight on infinitary exaples like the coin sequence. So I’ll show how the same puzzle arises in a simple, and finite, game.<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a> The game itself is a nice illustration of how a number of distinct solution concepts in game theory come apart. (Indeed, the use I’ll make of it isn’t a million miles from the use that <span class="citation">Kohlberg and Mertens (<a href="references.html#ref-KohlbergMertens1986">1986</a>)</span> make of it.) To set the problem up, I need to say a few words about how I think of game theory. This won’t be at all original - most of what I say is taken from important works by Robert Stalnaker <span class="citation">(<a href="references.html#ref-Stalnaker1994">1994</a>, <a href="references.html#ref-Stalnaker1996">1996</a>, <a href="references.html#ref-Stalnaker1998">1998</a>, <a href="references.html#ref-Stalnaker1999">1999</a>)</span>. But the underlying philosophical points are important, and it is easy to get confused about them. (At least, I used to get these points all wrong, and that’s got to be evidence they are easy to get confused about, right?) So I’ll set the basic points slowly, and then circle back to the puzzle for the Lockeans.<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a></p>
<p>Start with a simple decision problem, where the agent has a choice between two acts <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span>, and there are two possible states of the world, <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>, and the agent knows the payouts for each act-state pair are given by the following table.</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(S_1\)</span></td>
<td align="center"><span class="math inline">\(S_2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_1\)</span></td>
<td align="center">4</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A_2\)</span></td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<p>What to do? I hope you share the intuition that it is radically underdetermined by the information I’ve given you so far. If <span class="math inline">\(S_2\)</span> is much more probable than <span class="math inline">\(S_1\)</span>, then <span class="math inline">\(A_2\)</span> should be chosen; otherwise <span class="math inline">\(A_1\)</span> should be chosen. But I haven’t said anything about the relative probability of those two states. Now compare that to a simple game. Row has two choices, which I’ll call <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span>. Column also has two choices, which I’ll call <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>. It is common knowledge that each player is rational, and that the payouts for the pairs of choices are given in the following table. (As always, Row’s payouts are given first.)</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(S_1\)</span></td>
<td align="center"><span class="math inline">\(S_2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_1\)</span></td>
<td align="center">4, 0</td>
<td align="center">0, 1</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A_2\)</span></td>
<td align="center">1, 0</td>
<td align="center">1, 1</td>
</tr>
</tbody>
</table>
<p>What should Row do? This one is easy. Column gets 1 for sure if she plays <span class="math inline">\(S_2\)</span>, and 0 for sure if she plays <span class="math inline">\(S_1\)</span>. So she’ll play <span class="math inline">\(S_2\)</span>. And given that she’s playing <span class="math inline">\(S_2\)</span>, it is best for Row to play <span class="math inline">\(A_2\)</span>.</p>
<p>You probably noticed that the game is just a version of the decision problem from a couple of paragraphs ago. The relevant states of the world are choices of Column. But that’s fine; the layout of that decision problem was neutral on what constituted the states <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>. Note that the game can be solved without explicitly saying anything about probabilities. What is added to the (unsolvable) decision-theoretic problem is not information about probabilities, but information about Column’s payouts, and the fact that Column is rational. Those facts imply something about Column’s play, namely that she would play <span class="math inline">\(S_2\)</span>. And that settles what Row should do.</p>
<p>There’s something quite general about this example. What’s distinctive about game theory isn’t that it involves any special kinds of decision making. Once we get the probabilities of each move by the other player, what’s left is (mostly) expected utility maximisation.<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a> The distinctive thing about game theory is that the probabilities aren’t specified in the setup of the game; rather, they are solved for. Apart from special cases, such as where one option strictly dominates another, not much can be said about a decision problem with unspecified probabilities. But a lot can be said about games where the setup of the game doesn’t specify the probabilities, because it is possible to solve for the probabilities given the information that is provided.</p>
<p>This way of thinking about games makes the description of game theory as ‘interactive epistemology’ <span class="citation">(Aumann <a href="references.html#ref-Aumann1999">1999</a>)</span> rather apt. The theorist’s work is to solve for what a rational agent should think other rational agents in the game should do. From this perspective, it isn’t surprising that game theory will make heavy use of equilibrium concepts. In solving a game, we must deploy a theory of rationality, and attribute that theory to rational actors in the game itself. In effect, we are treating rationality as something of an unknown, but one that occurs in every equation we have to work with. Not surprisingly, there are going to be multiple solutions to the puzzles we face.</p>
<p>This way of thinking lends itself to an epistemological interpretation of one of the most puzzling concepts in game theory, the mixed strategy. The most important solution concept in modern game theory is the Nash equilibrium. A set of moves is a Nash equilibrium if no player can improve their outcome by deviating from the equilibrium, conditional on no other player deviating. In many simple games, the only Nash equilibria involve mixed strategies. Here’s one simple example.</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(S_1\)</span></td>
<td align="center"><span class="math inline">\(S_2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(A_1\)</span></td>
<td align="center">0, 1</td>
<td align="center">10, 0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(A_2\)</span></td>
<td align="center">9, 0</td>
<td align="center">-1, 1</td>
</tr>
</tbody>
</table>
<p>This game is reminiscent of some puzzles that have been much discussed in the decision theory literature, namely asymmetric Death in Damascus puzzles [<span class="citation">Richter (<a href="references.html#ref-Richter1984">1984</a>)</span>;] . Here Column wants herself and Row to make the ‘same’ choice, i.e., <span class="math inline">\(A_1\)</span> and <span class="math inline">\(S_1\)</span> or <span class="math inline">\(A_2\)</span> and <span class="math inline">\(S_2\)</span>. She gets 1 if they do, 0 otherwise. And Row wants them to make different choices, and gets 10 if they do. Row also dislikes playing <span class="math inline">\(A_2\)</span>, and this costs her 1 whatever else happens. It isn’t too hard to prove that the only Nash equilibrium for this game is that Row plays a mixed strategy playing both <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> with probability <span class="math inline">\(\frac{1}{2}\)</span>, while Column plays the mixed strategy that gives <span class="math inline">\(S_1\)</span> probability <span class="math inline">\(\frac{11}{20}\)</span>, and <span class="math inline">\(S_2\)</span> with probability <span class="math inline">\(\frac{9}{20}\)</span>.</p>
<p>Now what is a mixed strategy? It is easy enough to take away form the standard game theory textbooks a <strong>metaphysical</strong> interpretation of what a mixed strategy is. Here, for instance, is the paragraph introducing mixed strategies in Dixit and Skeath’s <em>Games of Strategy</em>.</p>
<blockquote>
<p>When players choose to act unsystematically, they pick from among their pure strategies in some random way …We call a random mixture between these two pure strategies a mixed strategy. <span class="citation">(Dixit and Skeath <a href="references.html#ref-DixitSkeath2004">2004</a>, 186)</span></p>
</blockquote>
<p>Dixit and Skeath are saying that it is definitive of a mixed strategy that players use some kind of randomisation device to pick their plays on any particular run of a game. That is, the probabilities in a mixed strategy must be in the world; they must go into the players’ choice of play. That’s one way, the paradigm way really, that we can think of mixed strategies metaphysically.</p>
<p>But the understanding of game theory as interactive epistemology naturally suggests an <strong>epistemological</strong> interpretation of mixed strategies.</p>
<blockquote>
<p>One could easily … [model players] … turning the choice over to a randomizing device, but while it might be harmless to permit this, players satisfying the cognitive idealizations that game theory and decision theory make could have no motive for playing a mixed strategy. So how are we to understand Nash equilibrium in model theoretic terms as a solution concept? We should follow the suggestion of Bayesian game theorists, interpreting mixed strategy profiles as representations, not of players’ choices, but of their beliefs. <span class="citation">(Stalnaker <a href="references.html#ref-Stalnaker1994">1994</a>, 57–58)</span></p>
</blockquote>
<p>One nice advantage of the epistemological interpretation, as noted by Binmore <span class="citation">(<a href="references.html#ref-Binmore2007">2007</a>, 185)</span> is that we don’t require players to have <span class="math inline">\(n\)</span>-sided dice in their satchels, for every <span class="math inline">\(n\)</span>, every time they play a game.<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a> But another advantage is that it lets us make sense of the difference between playing a pure strategy and playing a mixed strategy where one of the ‘parts’ of the mixture is played with probability one.</p>
<p>With that in mind, consider the below game, which I’ll call Up-Down.<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a> Informally, in this game <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> must each play a card with an arrow pointing up, or a card with an arrow pointing down. I will capitalise <span class="math inline">\(A\)</span>’s moves, i.e., <span class="math inline">\(A\)</span> can play UP or DOWN, and italicise <span class="math inline">\(B\)</span>’s moves, i.e., <span class="math inline">\(B\)</span> can play <em>up</em> or <em>down</em>. If at least one player plays a card with an arrow facing up, each player gets $1. If two cards with arrows facing down are played, each gets nothing. Each cares just about their own wealth, so getting $1 is worth 1 util. All of this is common knowledge. More formally, here is the game table, with <span class="math inline">\(A\)</span> on the row and <span class="math inline">\(B\)</span> on the column.</p>
<table>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><em>up</em></td>
<td align="center"><em>down</em></td>
</tr>
<tr class="even">
<td>UP</td>
<td align="center">1, 1</td>
<td align="center">1, 1</td>
</tr>
<tr class="odd">
<td>DOWN</td>
<td align="center">1, 1</td>
<td align="center">0, 0</td>
</tr>
</tbody>
</table>
<p>When I write game tables like this, I mean that the players know that these are the payouts, that the players know the other players to be rational, and these pieces of knowledge are common knowledge to at least as many iterations as needed to solve the game. (I assume here that in solving the game, it is legitimate to assume that if a player knows that one option will do better than another, they have conclusive reason to reject the latter option. This is completely standard in game theory, though somewhat controversial in philosophy.) With that in mind, let’s think about how the agents should approach this game.</p>
<p>I’m going to make one big simplifying assumption at first. I’ll relax this later, but it will help the discussion to start with this assumption. This assumption is that the doctrine of Uniqueness applies here; there is precisely one rational credence to have in any salient proposition about how the game will play. Some philosophers think that Uniqueness always holds <span class="citation">(White <a href="references.html#ref-White2005-WHIEP">2005</a>)</span>. I join with those such as <span class="citation">North (<a href="references.html#ref-North2010">2010</a>)</span> and <span class="citation">Schoenfield (<a href="references.html#ref-Schoenfield2013">2013</a>)</span> who don’t. But it does seem like Uniqueness might often hold; there might often be a right answer to a particular problem. Anyway, I’m going to start by assuming that it does hold here.</p>
<p>The first thing to note about the game is that it is symmetric. So the probability of <span class="math inline">\(A\)</span> playing UP should be the same as the probability of <span class="math inline">\(B\)</span> playing <em>up</em>, since <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> face exactly the same problem. Call this common probability <span class="math inline">\(x\)</span>. If <span class="math inline">\(x &lt; 1\)</span>, we get a quick contradiction. The expected value, to Row, of UP, is 1. Indeed, the known value of UP is 1. If the probability of <em>up</em> is <span class="math inline">\(x\)</span>, then the expected value of UP is <span class="math inline">\(x\)</span>. So if <span class="math inline">\(x &lt; 1\)</span>, and Row is rational, she’ll definitely play UP. But that’s inconsistent with the claim that <span class="math inline">\(x &lt; 1\)</span>, since that means that it isn’t definite that Row will play UP.</p>
<p>So we can conclude that <span class="math inline">\(x = 1\)</span>. Does that mean we can know that Row will play UP? No. Assume we could conclude that. Whatever reason we would have for concluding that would be a reason for any rational person to conclude that Column will play <em>up</em>. Since any rational person can conclude this, Row can conclude it. So Row knows that she’ll get 1 whether she plays UP or DOWN. But then she should be indifferent between playing UP and DOWN. And if we know she’s indifferent between playing UP and DOWN, and our only evidence for what she’ll play is that she’s a rational player who’ll maximise her returns, then we can’t be in a position to know she’ll play UP.</p>
<p>For the rest of this ssection I want to reply to one objection, and weaken an assumption I made earlier. The objection is that I’m wrong to assume that agents will only maximise expected utility. They may have tie-breaker rules, and those rules might undermine the arguments I gave above. The assumption is that there’s a uniquely rational credence to have in any given situation.</p>
<p>I argued that if we knew that <span class="math inline">\(A\)</span> would play UP, we could show that <span class="math inline">\(A\)</span> had no reason to play UP. But actually what we showed was that the expected utility of playing UP would be the same as playing DOWN. Perhaps <span class="math inline">\(A\)</span> has a reason to play UP, namely that UP weakly dominates DOWN. After all, there’s one possibility on the table where UP does better than DOWN, and none where RED does better. And perhaps that’s a reason, even if it isn’t a reason that expected utility considerations are sensitive to.</p>
<p>Now I don’t want to insist on expected utility maximisation as the only rule for rational decision making. Sometimes, I think some kind of tie-breaker procedure is part of rationality. In the papers by Stalnaker I mentioned above, he often appeals to this kind of weak dominance reasoning to resolve various hard cases. But I don’t think weak dominance provides a reason to play UP in this particular case. When Stalnaker says that agents should use weak dominance reasoning, it is always in the context of games where the agents’ attitude towards the game matrix is different to their attitude towards each other. One case that Stalnaker discusses in detail is where the game table is common knowledge, but there is merely common (justified, true) belief in common rationality. Given such a difference in attitudes, it does seem there’s a good sense in which the most salient departure from equilibrium will be one in which the players end up somewhere else on the table. And given that, weak dominance reasoning seems appropriate.</p>
<p>But that’s not what we’ve got here. Assuming that rationality requires playing UP/<em>up</em>, the players know we’ll end up in the top left corner of the table. There’s no chance that we’ll end up elsewhere. Or, perhaps better, there is just as much chance we’ll end up ‘off the table’, as that we’ll end up in a non-equilibrium point on the table. To make this more vivid, consider the ‘possibility’ that <span class="math inline">\(B\)</span> will play <em>across</em>, and if <span class="math inline">\(B\)</span> plays <em>across</em>, <span class="math inline">\(A\)</span> will receive 2 if she plays DOWN, and -1 if she plays UP. Well hold on, you might think, didn’t I say that <em>up</em> and <em>down</em> were the only options, and this was common knowledge? Well, yes, I did, but if the exercise is to consider what would happen if something the agent knows to be true doesn’t obtain, then the possibility that one agent will play blue certainly seems like one worth considering. It is, after all, a metaphysical possibility. And if we take it seriously, then it isn’t true that under any possible play of the game, UP does better than DOWN.</p>
<p>We can put this as a dilemma. Assume, for reductio, that UP/<em>up</em> is the only rational play. Then if we restrict our attention to possibilities that are epistemically open to <span class="math inline">\(A\)</span>, then UP does just as well as DOWN; they both get 1 in every possibility. If we allow possibilities that are epistemically closed to <span class="math inline">\(A\)</span>, then the possibility where <span class="math inline">\(B\)</span> plays <em>blue</em> is just as relevant as the possibility that <span class="math inline">\(B\)</span> is irrational. After all, we stipulated that this is a case where rationality is common knowledge. In neither case does the weak dominance reasoning get any purchase.</p>
<p>With that in mind, we can see why we don’t need the assumption of Uniqueness. Let’s play through how a failure of Uniqueness could undermine the argument. Assume, again for reductio, that we have credence <span class="math inline">\(\varepsilon &gt; 0\)</span> that <span class="math inline">\(A\)</span> will play DOWN. Since <span class="math inline">\(A\)</span> maximises expected utility, that means <span class="math inline">\(A\)</span> must have credence 1 that <span class="math inline">\(B\)</span> will play <em>up</em>. But this is already odd. Even if you think people can have different reactions to the same evidence, it is odd to think that one rational agent could regard a possibility as infinitely less likely than another, given isomorphic evidence. And that’s not all of the problems. Even if <span class="math inline">\(A\)</span> has credence 1 that <span class="math inline">\(B\)</span> will play <em>up</em>, it isn’t obvious that playing UP is rational. After all, relative to the space of epistemic possibilities, UP weakly dominates DOWN. Remember that we’re no longer assuming that it can be known what <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> will play. So even without Uniqueness, there are two reasons to think that it is wrong to have credence <span class="math inline">\(\varepsilon &gt; 0\)</span> that <span class="math inline">\(A\)</span> will play DOWN. So we’ve still shown that credence 1 doesn’t imply knowledge, and since the proof is known to us, and full belief is incompatible with knowing that you can’t know, this is a case where credence 1 doesn’t imply full belief. So whether <span class="math inline">\(A\)</span> plays UP, like whether the coin will ever land tails, is a case where belief comes apart from high credence, even if by high credence we literally mean credence one. This is a problem for the Lockean, and, like Williamson’s coin, it is also a problem for the view that belief is credence one.</p>
</div>
<div id="lockepuzzles" class="section level2">
<h2><span class="header-section-number">6.4</span> Puzzles for Lockeans</h2>
<p>The phrase “the Lockean theory of belief” is sometimes used for a metaphysical claim, and sometimes for a normative claim. The two claims are closely related. The metaphysical claim is that what it is to believe <span class="math inline">\(p\)</span> is to have a credence in <span class="math inline">\(p\)</span> above some threshold <span class="math inline">\(t\)</span>. The normative claim is that one should believe <span class="math inline">\(p\)</span> if and only if one’s (rational) credence in <span class="math inline">\(p\)</span> is above some threshold <span class="math inline">\(t\)</span>. Crucially, this threshold is meant to be interest-invariant. If we restrict our attention to rational agents, then there won’t be a huge difference between the metaphysical and normative versions of the Lockean theory. At the very least, one of them will be extensionally adequate if and only if the other is. So that’s what I’ll do in this section. I’ll look at various rational agents, and show what puzzles arise if we assume that belief goes with high credence.</p>
<p>I’ve already mentioned two classes of puzzles, those to do with infinite sequences of coin tosses and those to do with weak dominance in games. But there are other puzzles that apply especially to the Lockean.</p>
<div id="lockearb" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Arbitrariness</h3>
<p>The first problem for the Lockeans, and in a way the deepest, is that it makes the boundary between belief and non-belief arbitrary. This is a point that was well made some years ago now by Robert <span class="citation">Stalnaker (<a href="references.html#ref-Stalnaker1984">1984</a>, 91)</span>. Unless these numbers are made salient by the environment, there is no special difference between believing <span class="math inline">\(p\)</span> to degree 0.9876 and believing it to degree 0.9875. But if <span class="math inline">\(t\)</span> is 0.98755, this will be <em>the difference</em> between believing <span class="math inline">\(p\)</span> and not believing it, which is an important difference.</p>
<p>The usual response to this, as found in <span class="citation">Foley (<a href="references.html#ref-Foley1993">1993</a> Ch. 4)</span> and <span class="citation">Hunter (<a href="references.html#ref-Hunter1996">1996</a>)</span> and <span class="citation">M. Lee (<a href="references.html#ref-Lee2017b">2017</a>)</span>, is to say that the boundary is vague. Now we might respond to this by noting that this only helps on an implausible theory of vagueness. On epistemicist theories, or supervaluationist theories, or on my preferred comparative truth theory (<span class="citation">Weatherson (<a href="references.html#ref-Weatherson2005b">2005</a><a href="references.html#ref-Weatherson2005b">c</a>)</span>), there will still be an arbitrary point which marks the difference between belief and non-belief. This won’t be the case on various kinds of degree of truth theories. But, as <span class="citation">Williamson (<a href="references.html#ref-Williamson1994">1994</a>)</span> pointed out, those are theories on which contradictions end up being half-true. And if saving the Lockean theory requires that we give up on the idea that contradictions are simply false, it is hard to see how it is worth the price.</p>
<p>But a better response is to think about what it means to say that the belief/non-belief boundary is a vague point on a scale. We know plenty of terms where the boundary is a vague point on a scale. Comparative adjectives are typically like that. Whether a day is hot depends on whether it is above some vague point on a temperature scale, for example. But here’s the thing about these vague terms - they don’t enter into lawlike generalisations. (At least in a non-trivial way. Hot days are 24 hours long, and that’s a law, but not one that hotness has a particular role in grounding.) The laws involve the scale; the most you can say using the vague term is some kind of generic. For instance, you can say that hot days are exhausting, or that electricity use is higher on hot days. But these are generics, and the interesting law-like claims will involve degrees of heat, not the hot/non-hot binary.</p>
<p>It’s a fairly central presupposition of this book that belief is not like that. Belief plays a key role in all sorts of non-trivial lawlike generalisations. Folk psychology is full of such lawlike generalisations. We’re doing social science here, so the laws in question are hardly exceptionless. But they are counterfactually resilient, and explanatorily deep, and not just generics that are best explained using the underlying scale.</p>
<p>Of course, the Lockean doesn’t believe that these generalisations of folk psychology are anything more than generics, so this is a somewhat question-begging argument. But if you’re not antecedently disposed to give up on folk psychology, or reduce it to the status of a bunch of helpful generics, it’s worth seeing how striking the Lockean view here is. So consider a generalisation like the following.</p>
<ul>
<li>If someone wants an outcome O, and they believe that doing X is the only way to get O, and they believe that doing X will neither incur any costs that are large in comparison to how good O is, nor prevent them being able to do something that brings about some other outcome that is comparatively good, then they will do X.</li>
</ul>
<p>This isn’t a universal - some people are just practically irrational. But it’s stronger than just a generic claim about high temperatures. Or so I say. But the Lockean does not say this; they say that this has widespread counterexamples, and when it is true, it is a relatively superficial truth whose explanatory force is entirely derived from deeper truths about credences.</p>
<p>The Lockean, for instance, thinks that someone in Blaise’s situation satisfies all the antecedents and qualifications in the principle. They want the child to have a moment of happiness. They believe (i.e., have a very high credence that) taking the bet will bring about this outcome, will have no costs at all, and will not prevent them doing anything else. Yet they will not think that people in Blaise’s situation will generally take the bet, or that it would be rational for them to take the bet, or that taking the bet is explained by these high credences.</p>
<p>That’s what’s bad about making the belief/non-belief distinction arbitrary. It means that generalisations about belief are going to be not particularly explanatory, and are going to have systematic (and highly rational) exceptions. We should expect more out of a theory of belief.</p>
</div>
<div id="lockecorrect" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Correctness</h3>
<p>I’ve talked about this one a bit in subsection <a href="belief.html#mecorrect">3.6.1</a>, so I’ll be brief here. Beliefs have correctness conditions. To believe <span class="math inline">\(p\)</span> when <span class="math inline">\(p\)</span> is false is to make a mistake. That might be an excusable mistake, or even a rational mistake, but it is a mistake. On the other hand, having an arbitrarily high credence in <span class="math inline">\(p\)</span> when <span class="math inline">\(p\)</span> turns out to be false is not a mistake. So having high credence in <span class="math inline">\(p\)</span> is not the same as believing <span class="math inline">\(p\)</span>.</p>
<p>Matthew <span class="citation">M. B. Lee (<a href="references.html#ref-Lee2017a">2017</a>)</span> argues that the versions of this argument by <span class="citation">Ross and Schroeder (<a href="references.html#ref-RossSchroeder2014">2014</a>)</span> and <span class="citation">Fantl and McGrath (<a href="references.html#ref-FantlMcGrath2009">2009</a>)</span> are incomplete because they don’t provide a conclusive case for the premise that having a high credence in a falsehood is not a mistake. But this gap can be plugged. Imagine a scientist, call her Marie, who knows the correct theory of chance for a given situation. She knows that the chance of <span class="math inline">\(p\)</span> obtaining is 0.999. (If you think <span class="math inline">\(t &gt; 0.999\)</span>, just increase this number, and change the resulting dialogue accordingly.) And her credence in <span class="math inline">\(p\)</span> is 0.999, because her credences track what she knows about chances. She has the following exchange with an assistant.</p>
<blockquote>
<p>ASSISTANT: Will <span class="math inline">\(p\)</span> happen?<br />
MARIE: Probably. It might not, but there is only a one in a thousand chance of that. So <span class="math inline">\(p\)</span> will probably happen.</p>
</blockquote>
<p>To their surprise, <span class="math inline">\(p\)</span> does not happen. But Marie did not make any kind of mistake here. Indeed, her answer to assistant’s question was exactly right. But if the Lockean theory of belief is right, and false beliefs are mistakes, then Marie did make a mistake. So the Lockean theory of belief is not right.</p>
</div>
<div id="lockemoore" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Moorean Paradoxes</h3>
<p>The Lockean says other strange things about Marie. By hypothesis, she believes that <span class="math inline">\(p\)</span> will obtain. Yet she certainly seems sincere when she says it might not happen. So she believes both <span class="math inline">\(p\)</span> and it might not be that <span class="math inline">\(p\)</span>. This looks like a Moore-paradoxical utterance, yet in context it seems completely banal.</p>
<p>The same thing goes for Chamira. Does she believe the Battle of Agincourt was in 1415? Yes, say the Lockeans. Does she also believe that it might not have been in 1415? Yes, say the Lockeans, that is why it was rational of her to play Red-True, and it would have been irrational to play Blue-True. So she believes both that something is the case, and that it might not be the case. This seems irrational, but Lockeans insist that it is perfectly consistent with her being a model of rationality.</p>
<p>Back in subsection <a href="interests.html#orthodoxmoore">2.3.1</a> I argued that this kind of thing would be a problem for any kind of orthodox theory. And in some sense all I’m doing here is noting that the Lockean really is a kind of orthodox theorist. But the argument that the Lockean is committed to the rationality of Moore-paradoxical claims doesn’t rely on those earlier arguments; it’s a direct consequence of their view applied to simple cases like Marie and Chamira.</p>
</div>
<div id="closure" class="section level3">
<h3><span class="header-section-number">6.4.4</span> Closure and the Lockean Theory</h3>
<p>The Lockean theory makes an implausible prediction about conjunction.<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a> It says that someone can believe two conjuncts, yet actively refuse to believe the conjunction. Here is how Stalnaker puts the point.</p>
<blockquote>
<p>Reasoning in this way from accepted premises to their deductive consequences (<span class="math inline">\(P\)</span>, also <span class="math inline">\(Q\)</span>, therefore <span class="math inline">\(R\)</span>) does seem perfectly straightforward. Someone may object to one of the premises, or to the validity of the argument, but one could not intelligibly agree that the premises are each acceptable and the argument valid, while objecting to the acceptability of the conclusion. <span class="citation">(Stalnaker <a href="references.html#ref-Stalnaker1984">1984</a>, 92)</span></p>
</blockquote>
<p>If believing that <span class="math inline">\(p\)</span> just means having a credence in <span class="math inline">\(p\)</span> above the threshold, then this will happen. Indeed, given some very weak assumptions about the world, it implies that there are plenty of quadruples <span class="math inline">\(\langle S, A, B, A \wedge B \rangle\)</span> such that</p>
<ul>
<li><span class="math inline">\(S\)</span> is a rational agent.</li>
<li><span class="math inline">\(A, B\)</span> and <span class="math inline">\(A \wedge B\)</span> are propositions.</li>
<li><span class="math inline">\(S\)</span> believes <span class="math inline">\(A\)</span> and believes <span class="math inline">\(B\)</span>.</li>
<li><span class="math inline">\(S\)</span> does not believe <span class="math inline">\(A \wedge B\)</span>.</li>
<li><span class="math inline">\(S\)</span> knows that she has all these states, and consciously reflectively
endorses them.</li>
</ul>
<p>Now one might think, indeed I do think, that such quadruples do not exist at all. But set that objection aside. If the Lockean is correct, these quadruples should be everywhere. That’s because for any <span class="math inline">\(t \in (0, 1)\)</span> you care to pick, quadruples of the form <span class="math inline">\(\langle S, C, D, C \wedge D \rangle\)</span> are very very common.</p>
<ul>
<li><span class="math inline">\(S\)</span> is a rational agent.</li>
<li><span class="math inline">\(C, D\)</span> and <span class="math inline">\(C \wedge D\)</span> are propositions.</li>
<li><span class="math inline">\(S\)</span>’s credence in <span class="math inline">\(C\)</span> is greater than <span class="math inline">\(t\)</span>, and her credence in <span class="math inline">\(D\)</span> is greater than <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(S\)</span>’s credence in <span class="math inline">\(C \wedge D\)</span> is less than <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(S\)</span> knows that she has all these states, and reflectively endorses them.</li>
</ul>
<p>The best arguments for the existence of quadruples <span class="math inline">\(\langle S, A, B, A \wedge B \rangle\)</span> are non-constructive existence proofs. David <span class="citation">Christensen (<a href="references.html#ref-Christensen2005">2005</a>)</span> for instance, argues from the existence of the preface paradox to the existence of these quadruples. I will come back to that argument in chapter <a href="preface.html#preface">8</a>. But what I want to stress here is that even if these existence proofs work, they don’t really prove what the Lockean needs. They don’t show that quadruples satisfying the constraints we associated with <span class="math inline">\(\langle S, A, B, A \wedge B \rangle\)</span> are just as common as quadruples satisfying the constraints we associated with <span class="math inline">\(\langle S, C, D, C \wedge D \rangle\)</span>, for any <span class="math inline">\(t\)</span>. But if the Lockean were correct, they should be exactly as common.</p>
</div>
</div>
<div id="solving" class="section level2">
<h2><span class="header-section-number">6.5</span> Solving the Challenges</h2>
<p>It’s not fair to criticise other theories for their inability to meet a challenge that one’s own theory cannot meet. So I’ll end this chapter by noting that the six problems I’ve raised so far for Lockeans are not problems for my interest-relative theory of (rational) belief. I’ve already discussed the points about correctness in subsection <a href="belief.html#mecorrect">3.6.1</a>, and about closure in chapter <a href="knowledge.html#knowledge">4</a>, and there isn’t much to be added. But it’s worth saying a few words about the other four problems.</p>
<div id="coins" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Coins</h3>
<p>I say that a necessary condition of believing that <span class="math inline">\(p\)</span> is a disposition to take <span class="math inline">\(p\)</span> for granted. The rational person will prefer betting on logically weaker rather than logically stronger propositions in the coin case, so they will not take the logically stronger ones for granted. If they did take them for granted, they would be indifferent between the bets. So they will not believe that one of the coin flips after the second will land heads, or even that one of the coin flips after the first will land heads. And that’s the right result. The rational person should assign those propositions probability one, but not believe them.</p>
</div>
<div id="games" class="section level3">
<h3><span class="header-section-number">6.5.2</span> Games</h3>
<p>In the up-down game, if the rational person believed that the other player would play up, they would be indifferent between up and down. But it’s irrational to be indifferent between those options, so they wouldn’t have the belief. They will think the probability that the other person will play up is one - what else could it be? But they will not believe it on pain of incoherence.</p>
</div>
<div id="arbitrariness" class="section level3">
<h3><span class="header-section-number">6.5.3</span> Arbitrariness</h3>
<p>According to IRT, the difference between belief and non-belief is the difference between willingness and unwillingness to take something as given in inquiry. This is far from an arbitrary difference. And it is a difference that supports law-like generalisations. If someone believes that <span class="math inline">\(p\)</span>, and believes that given <span class="math inline">\(p\)</span>, <span class="math inline">\(A\)</span> is better than <span class="math inline">\(B\)</span>, they will prefer <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span>. This isn’t a universal truth; people make mistakes. But nor is it merely a statistical generalisation. Counterexamples are things to be explained, while instances are explained by the underlying pattern.</p>
</div>
<div id="moore" class="section level3">
<h3><span class="header-section-number">6.5.4</span> Moore</h3>
<p>In many ways the guiding aim of this project was to avoid this kind of Moore paradoxicality. So it shouldn’t be a surprise that we avoid it here. If someone shouldn’t do something because <span class="math inline">\(p\)</span> might be false, that’s conclusive evidence that they don’t know that <span class="math inline">\(p\)</span>. And it’s conclusive evidence that either they don’t rationally believe <span class="math inline">\(p\)</span>, or they are making some very serious mistake in their reasoning. And in the latter case, the reason they are making a mistake is not that <span class="math inline">\(p\)</span> might be false, but that they have a seriously mistaken belief about the kind of choice they are facing. So we can never say that someone knows, or rationally believes, <span class="math inline">\(p\)</span>, but their choice is irrational because <span class="math inline">\(p\)</span> might be false.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="30">
<li id="fn30"><p>This section is based on material from my <span class="citation">(<a href="references.html#ref-Weatherson2012">2012</a> sect. 3.1)</span>.<a href="ratbel.html#fnref30" class="footnote-back">↩</a></p></li>
<li id="fn31"><p>Kenneth <span class="citation">Boyd (<a href="references.html#ref-Boyd2015">2016</a>)</span> suggests a somewhat similar role for vigilance in the course of defending an interest-invariant epistemic theory. Obviously I don’t agree with his conclusions, but my use of Sperber’s work does echo his.<a href="ratbel.html#fnref31" class="footnote-back">↩</a></p></li>
<li id="fn32"><p>This is based on material in my <span class="citation">(<a href="references.html#ref-Weatherson2014">2014</a> sect. 1)</span>.<a href="ratbel.html#fnref32" class="footnote-back">↩</a></p></li>
<li id="fn33"><p>I’m grateful to the participants in a game theory seminar at Arché in 2011, especially Josh Dever and Levi Spectre, for very helpful discussions that helped me see through my previous confusions.<a href="ratbel.html#fnref33" class="footnote-back">↩</a></p></li>
<li id="fn34"><p>The qualification is because weak dominance reasoning cannot be construed as orthodox expected utility maximisation. We saw that in the coins case, and it will become important again here. It is possible to model weak dominance reasoning using non-standard probabilities, as in <span class="citation">Keisler (<a href="references.html#ref-Brandenburger2008">2008</a>)</span>, but that introduces new complications.<a href="ratbel.html#fnref34" class="footnote-back">↩</a></p></li>
<li id="fn35"><p>It is worse than if some games have the only equilibria involving mixed strategies with irrational probabilities. And it might be noted that Binmore’s introduction of mixed strategies, on page 44 of his <span class="citation">(<a href="references.html#ref-Binmore2007">2007</a>)</span>, sounds much more like the metaphysical interpretation. But I think the later discussion is meant to indicate that this is just a heuristic introduction; the epistemological interpretation is the correct one.<a href="ratbel.html#fnref35" class="footnote-back">↩</a></p></li>
<li id="fn36"><p>In earlier work I’d called it Red-Green, but this is too easily confused with the Red-Blue game that plays such an important role in chapter <a href="interests.html#interests">2</a>.<a href="ratbel.html#fnref36" class="footnote-back">↩</a></p></li>
<li id="fn37"><p>This subsection draws on material from my <span class="citation">(<a href="references.html#ref-Weatherson2014">2014</a>)</span>.<a href="ratbel.html#fnref37" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="evidence.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ties.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Kahis.pdf", "Kahis.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
