## Atomism about Rational Belief {#atomism}

In chapter \@ref(belief) I argued for two individually necessary and jointly sufficient conditions for belief.^[This section is based on material from my -@Weatherson2012[sect. 3.1].] They are

1. In some possible decision problem, $p$ is taken for granted.
2. For every question the agent is interested in, the agent answers the question the same way (i.e., giving the same answer for the same reasons) whether the question is asked unconditionally or conditional on $p$.

At this point one might think that offering a theory of rational belief would be easy. It is rational to believe $p$ just in case it is rational to satisfy these conditions. Unfortunately, this nice thought can't be right. It can be irrational to satisfy these conditions while rationally believing $p$.

Coraline is like Anisa and Chamari, in that she has read a reliable book saying that the Battle of Agincourt was in 1415. And she now believes that the Battle of Agincourt was indeed in 1415, for the very good reason that she read it in a reliable book. 

In front of her is a sealed envelope, and inside the envelope a number is written on a slip of paper. Let $X$ denote that number, non-rigidly. (So when I say Coraline believes $X = x$, it means she believes that the number written on the slip of paper is $x$, where $x$ rigidly denotes some number.) Coraline is offered the following bet:

* If she declines the bet, nothing happens.
* If she accepts the bet, and the Battle of Agincourt was in 1415, she wins $1.
* If she accepts the bet, and the Battle of Agincourt was not in 1415, she loses $X$ dollars.

For some reason, Coraline is convinced that $X = 10$. This is very strange, since she was shown the slip of paper just a few minutes ago, and it clearly showed that $X = 1,000,000,000$. Coraline wouldn't bet on when the Battle of Agincourt was at odds of a billion to one. But she would take that bet at 10 to 1, which is what she thinks she is faced with. Indeed, she doesn't even conceptualise it as a bet; it's a free dollar she thinks. Right now, she is disposed to treat the date of the battle as a given. She is disposed to lose this disposition should a very long odds bet appear to depend on it. But she doesn't believe she is facing such a bet.

So Coraline accepts the bet; she thinks it is a free dollar. And that's when the battle took place, so she wins the dollar. All's well that end's well. But it was a really wildly irrational bet to take. You shouldn't bet at those odds on something you remember from a history book. Neither memory nor history books are that reliable. Coraline was not rational to treat the questions _Should I take this bet?_, and _Conditional on the Battle of Agincourt being in 1415, should I take this bet?_ the same way. Her treating them the same way was fortunate - she won a dollar - but irrational.

Yet it seems odd to say that Coraline's belief about the Battle of Agincourt was irrational. What was irrational was her belief about the envelope, not her belief about the battle. To say that a particular disposition was irrational is to make a holistic assessment of the person with the disposition. But whether a belief is rational or not is, relatively speaking, atomistic.

That suggests the following condition on rational belief.

> S's belief that $p$ is irrational if
> 
> 1. S irrationally has one of the dispositions that is characteristic of belief that $p$; and
> 2. What explains S having a disposition that is irrational in that way is her attitudes towards $p$, not (solely) her attitudes towards other propositions, or her skills in practical reasoning.

In "Knowledge, Bets and Interests" I gave a similar theory about these cases - I said that S's belief that $p$ was irrational if the irrational dispositions were caused by an irrationally high credence in $p$. I mean this account to be ever so slightly more general. I'll come back to that below, because first I want to spell out the second clause.

Intuitively, Coraline's irrational acceptance of the belief is explained by her (irrational) belief about $X$, not her (rational) belief about the Battle of Agincourt. We can take this notion of explanation as a primitive if we like; it's in no worse philosophical shape than other notions we take as a primitive. But it is possible to spell it out a little more.

Coraline has a pattern of irrational dispositions related to the envelope. If you offer her $50 or $X$ dollars, she'll take the $50. If you change the bet so it isn't about Agincourt, but is instead about any other thing she has excellent but not quite conclusive evidence for, she'll still take the bet.

On the other hand, she does not have a pattern of irrational dispositions related to the Battle of Agincourt. She has this one, but if you change the payouts so they are not related to this particular envelope, then for all we have said so far, she won't do anything irrational.

That difference in patterns matters. We know that it's the beliefs about the envelope, and not the beliefs about the battle, that are explanatory because of this pattern. We could try and create a reductive analysis of explanation in clause 2 using facts about patterns, like the way Lewis tries to create a reductive analysis of causation using similar facts about patterns in "Causation as Influence" [@Lewis2004a]. But doing so would invariably run up against edge cases that would be more trouble to resolve than they are worth.

That's because there are ever so many ways in which someone could have an irrational disposition about any particular case. We can imagine Coraline having a rational belief about the envelope, but still taking the bet because of any of the following reasons:

- It has been her life goal to lose a billion dollars in a day, so taking the bet strictly dominates not taking it.
- She believes (irrationally) that anyone who loses a billion dollars in a day goes to heaven, and she (rationally) values heaven above any monetary amount.
- She consistently makes reasoning errors about billions, so the prospect of losing a billion dollars rarely triggers an awareness that she should reconsider things she normally takes for granted.

The last one of these is especially interesting. The picture of rational agency I have in the background here owes a lot to the notion of epistemic vigilance, as developed by Dan Sperber and co-authors [@SperberEtAl2010]. The rational agent will have all these beliefs in their head that they will drop when the costs of being wrong about them are too high, or the costs of re-opening inquiry into them are too low. They can't reason, at least in any conscious way, about whether to drop these beliefs, because to do that is, in some sense, to call the belief into doubt. And what's at issue is whether they should call the belief into doubt. So what they need is some kind of disposition to replace a belief that $p$ with an attitude that $p$ is highly probable, and this disposition should correlate with the cases where taking $p$ for granted will not maximise expected utility. This disposition will be a kind of vigilance. As Sperber et al show, we need some notion of vigilance to explain a lot of different aspects of epistemic evaluation, and I think it can be usefully pressed into service here.^[Kenneth @Boyd2015 suggests a somewhat similar role for vigilance in the course of defending an interest-invariant epistemic theory. Obviously I don't agree with his conclusions, but my use of Sperber's work does echo his.]

But if you need something like vigilance, then you have to allow that vigilance might fail. And maybe some irrational dispositions can be traced to that failure, and not to any propositional attitude the decider has. For example, if Coraline systematically fails to be vigilant when exactly one billion dollars is at stake, then we might want to say that her belief in $p$ is still rational, and she is practically, rather than theoretically, irrational. (Why could this happen? Perhaps she thinks of Dr Evil every time she hears the phrase "One billion dollars", and this distractor prevents her normally reliable skill of being vigilant from kicking in.)

If one tries to turn the vague talk of patterns of bets involving one proposition or another into a reductive analysis of when one particular belief is irrational, one will inevitably run into hard cases where a decider has multiple failures. We can't say that what makes Coraline's belief about the envelope, and not her belief about the battle, irrational is that if you replaced the envelope, she would invariably have a rational disposition. After all, she might have some other irrational belief about whatever we replace the envelope with. Or she might have some failure of practical reasoning, like a vigilance failure. Any kind of universal claim, like that it is only bets about the envelope that she gets wrong, won't do the job we need.

In "Knowledge, Bets and Interests", I tried to use the machinery of credences to make something like this point. The idea was that Coraline's belief in $p$ was rational because her belief just was her high credence in $p$, and that credence was rational. I still think that's approximately right, but it can't be the full story. 

For one thing, beliefs and credences aren't as closely connected metaphysically as this suggests. To have a belief in $p$ isn't just to have a high credence, it's to be disposed to let $p$ play a certain role. (This will become important in the next two sections.)

For another thing, it is hard to identify precisely what a credence is in the case of an irrational agent. The usual ways we identify credences, via betting dispositions or representation theorems, assume away all irrationality. But an irrational person might still have some rational beliefs. 

Attempts to generalise accounts of credences so that they cover the irrational person will end up saying something like what I've said about patterns. What it is to have credence 0.6 in $p$ isn't to have a set of preferences that satisfies all the presuppositions of such and such a representation theorem, and that theorem to say that one can be represented by a probability function $\Pr$ and a utility function $U$ such that $\Pr(p) = 0.6$. That can't be right because some people will, intuitively, have credence about 0.6 in $p$ while not uniformly conforming to these constraints. But what makes them intuitive cases of credence roughly 0.6 in $p$ is that generally they behave like the perfectly rational person with credence 0.6 in $p$, and most of the exceptions are explained by other features of their cognitive system other than their attitude to $p$.

In other words, we don't have a full theory of credences for irrational beings right now, and when we get one, it won't be much simpler than the theory in terms of patterns and explanations I've offered here. So it's best for now to just understand belief in terms of a pattern of dispositions, and say that the belief is  rational just in case that pattern is rational. And that might mean that on some occasions $p$-related activity is irrational even though the pattern of $p$-related activity is a rational pattern. Any given action, like any thing whatsoever, can be classified in any number of ways. What matters here is what explains the irrationality of a particular irrational act, and that will be a matter of which patterns of irrational dispositions the actor has.

However we explain Coraline's belief, the upshot is that she has a rational, true belief that is not knowledge. This is a novel kind of Dharmottara case. (Or Gettier case for folks who prefer that nomenclature.) It's not the exact kind of case that Dharmottara originally described. Coraline doesn't infer anything about the Battle of Agincourt from a false belief. But it's a mistake to think that the class of rational, true beliefs that are not knowledge form a natural kind. In general, negatively defined classes are disjunctive; there are ever so many ways to not have a property. An upshot of this discussion of Coraline is that there is one more kind of Dharmottara case than was previously recognised. But as, for example, @WilliamsonLofoten and @Nagel2013-Williamson have shown, we have independent reason for thinking this is a very disjunctive class. So the fact that it doesn't look anything like Dharmottara's example shouldn't make us doubt it is a rational, true belief that is not knowledge.
