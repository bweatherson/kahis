## Objections and Replies {#evsolution}

*Objection*: The formal results of the previous section only go through if we assume that the agents do not know precisely what the payoffs are in the game. We shouldn't assume that what holds for arbitrarily small margins of error will hold in the limit, i.e., when they do know the payoffs.

*Reply*: If pushed, I would defend the use limit assumptions like this to resolve hard cases like Stag Hunt. But I don't strictly need that assumption here, What is needed is that Parveen doesn't know precisely the probability of Rahul being in the restaurant given the rest of her evidence. Given that evidence is not luminous, as @Williamson2000 shows, this is a reasonable assumption. So the margin of error assumption that Carlsson and van Damme make is not, in this case, an assumption that merely makes the math easier; it is built into the case.

*Objection*: Even if Parveen doesn't know the payoffs precisely, The Radical Interpreter does. The Radical Interpreter is an idealisation, so they can be taken
to be ideal.

*Reply*: It turns out that Carlsson and van Damme's result doesn't require that both parties are ignorant of the precise values of the payoffs. As long as one party doesn't know the exact value of the payoff, the argument goes through. That was the point of the proof in subsection \@ref(perfectri).

*Objection*: The formal argument requires that in the 'global game' there are values for $x$ that make $A$ the dominant choice. These cases serve as a base step for an inductive argument that follows. But in Parveen's case, there is no such setting for $x$, so the inductive argument can't get going.

*Reply*: What matters is that there are values of $x$ such that $A$ is the strictly dominant choice, and Human (or Parveen) doesn't know that they know that they know, etc., that those values are not actual. And that's true in our case. For all Human (or Parveen) knows that they know that they know that they know..., the proposition in question is not part of their evidence under a maximally expansive verdict on The Radical Interpreter's part. So the relevant cases are there in the model, even if for some high value of $n$ they are known$^n$ not to obtain.

*Objection*: This model is much more complex than the simple motivation for pragmatic encroachment.

*Reply*: Sadly, this is true. I would like to have a simpler model, but I don't know how to create one. I suspect any such simple model will just be incomplete; it won't say what Parveen's evidence is. In this respect, any simple model will look just like applying tools like Nash equilibria to coordination games. So more complexity will be needed, one way or another. I think paying this price in complexity is worth it overall, but I can see how some people might think otherwise.

*Objection*: Change the case involving Human so that the bet loses 15 utils if *p* is false, rather than 100. Now the risk-dominant equilibrium is that Human takes the bet, and The Radical Interpreter says that *p* is part of Human's evidence. But note that if it was clearly true that *p* was not part of Human's evidence, then this would still be too risky a situation for them to know *p*. So whether it is possible that *p* is part of Human's evidence matters.

*Reply*: This is all true, and it shows that the view I'm putting forward is incompatible with some programs in epistemology. In particular, it is incompatible with E=K, since the what it takes to be evidence on this story is slightly different from what it takes to be knowledge. I will come back to this point in section \@ref(cutelim).

*Objection*: Carlsson and van Damme discuss one kind of global game. But there are other global games that have different equilibria. For instance, changing the method by which the noisy signal is selected would change the equilibrium of the global game. So this kind of argument can't show that the risk-dominant equilibrium is the one true solution.

*Reply*: This is somewhat true. There are other ways of embedding the game involving Human and The Radical Interpreter in global games that lead to different outcomes. They are usually somewhat artificial; e.g., by having the signal be systematically biased in one way. But what really matters is the game where the error in Human's knowledge of the payoffs is determined by their actual epistemic limitations. I think that will lead to something like the model we have here. But it is possible that the final result will differ a bit from what I have here, or (more likely) have some indeterminacy about just how interests interact with evidence and knowledge. The precise details are ultimately less important to me than whether we can provide a motivated story of how interests affect knowledge and evidence that does not presuppose we know what the agent's evidence is. And the method I've outlined here shows that we can do that, even if we end up tinkering a bit with the details.
